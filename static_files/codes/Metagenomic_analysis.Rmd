---
title: "Metagenomic analysis"
author: "Endogenomiks Scientific Team"
date: "2023-05-05"
output: html_document
---

# Metagenomic 16S analysis 

First a tsv file was created with the metadata of the data in question. This metadata for the 16S data looks as follows. 

```{r}
#Reading metadata
metadata <- read.table("metadata.tsv", row.names = 1, header = TRUE, sep = "\t", stringsAsFactors = TRUE)
metadata #Print metadata 
```


### Quality analysis 

To analyze the quality of the samples, the reads are downloaded and processed using fastqc. For this purpose the following code must be run in the terminal. The results are located in the FASTQC_pre folder, with its corresponding name. The following code was used for this purpose. 

```{bash, eval=F}
mkdir FASTQC
for i in *.fastq.gz ; do echo " fastqc $i -o FASTQC_pre" >> fastqc.sh ; done

```

Finally, multiQC was used to gather the quality results in a single file. These can be consulted in the html file of the MULTIQC folder. The following code was used for this purpose.

```{bash, eval=F}
multiqc FASTQC_pre -o MULTIQC_pre
```


Now, you can open the html file inside the MULTIQC_pre folder. This will give you a resume of the data quality. 


### Taxonomic assignment of sequencing reads by FROGS.

To perform the taxonomic assignment of the sequencing reads, the frogs tool was used. 

In order to run the following code, it is necessary to have frogs installed on your system. If the instructions were followed, frogs will be installed in a conda environment, which must be activated using the following code. 

```{bash,eval=F}
# to use FROGS, first you need to activate your environment
conda activate frogs
```


#### Preprocessing tool 

Once frogs is installed on your system, you can process the samples with the following commands, first you will use the sample preprocessing program, we have reads with an average size of 470. 16S has an average size of 450, a maximum of 550 and a minimum of 350 (FROGS recommended parameters for 16S analysis). Flash will be used to join the reads due to its speed compared to pear tool. 

The following code will use the preprocessing tool from Frogs to process the reads. To process the results quickly, FROGS allows sample multiprocessing. This is achieved by creating a single file containing all reads. 

```{bash, eval=F}
#First the .fastq.gz files were compressed for easier processing.
tar zcvf data.tar.gz *.fastq.gz

#Running frogs preprocessing tool
preprocess.py illumina --input-archive data1.tar.gz --min-amplicon-size 350 --max-amplicon-size 550 --merge-software flash --without-primers --R1-size 300 --R2-size 300 --output-dereplicated preprocess.fasta --output-count preprocess.tsv --summary preprocess.html --log-file preprocess.log --expected-amplicon-size 450    
```

The outputs of the present pipeline are: 

+ *preprocess.fasta *: File with the fasta of the preprocessed and joined sequences. 
* *preprocess.tsv*: tsv file with the description of each sequence and to which group they belong.
* *preprocess.html*: html file with the results in graphical form. 
+ *preprocess.log*: Log file of the processing.

#### Clustering tool

In the next step of our workflow, we need to cluster the analysed data to recognize which reads belong to the same organism. Subsequently, a summary of the statistics of the clustering process is processed. 

The code needed to process it is the following:

```{bash, eval=F}
#Clusterizing the results 
clustering.py --nb-cpus 5 --input-fasta preprocess.fasta --input-count preprocess.tsv --output-biom clustering.biom --output-fasta clustering/clustering.fasta --output-compo clustering/clustering_compo.tsv --log-file clustering/clustering_log.txt --distance 3 --denoising

#To calculate the stats
clusters_stat.py --input-biom clustering/clustering.biom --output-file clustering/clusters_stats.html --log-file clustering/clusters_stats.log 

```

The outputs of this step are: 

+ *clustering.fasta *: File with the fasta of the sequences per cluster. 
* *clustering.biom*: Biological Observation Matrix (.biom) file with a contingency table of the samples against the clusters. 
* *clustering_compo.tsv*: tsv file with the composition of the clusters and the sample to which they belong.
* *clustering_log.txt*: Log file of the processing.
* *clusters_stats.html*: html file with the results of the clustering statistics graphically.
* *clusters_stats.log*: Log file of the clustering process.


#### Chimera filtering 

In the next step of our workflow the appropriate chimera removal tool will be used. 
The code needed to process it is the following:

```{bash, eval=F}
remove_chimera.py --input-fasta clustering/clustering.fasta --input-biom clustering/clustering.biom --non-chimera clustering/remove_chimera.fasta --nb-cpus 5 --log-file chimera/remove_chimera.log --out-abundance chimera/remove_chimera.biom --summary chimera/remove_chimera.html
```

The outputs of this step are: 

+ *remove_chimera.fasta *: File with the fasta of the sequences that do not have chimeras.  
+ *remove_chimera.biom*: Contingency matrix of the samples against the clusters after chimera removal. 
+ *remove_chimera.html*: Html file with the results after removing chimeras in a graphical way.
+ *remove_chimera.log*: Log file of the chimera removal process.


#### Filtering 

Next, it is required to remove the OTUS that have chimeras, contaminants or artifacts. The code needed to process it is the following, where a filter by presence (minimum in a sample) and by abudance (found in 0.00005% of the readings) was also used:

```{bash, eval=F}
otu_filters.py --input-fasta chimera/remove_chimera.fasta --input-biom chimera/remove_chimera.biom --output-fasta filter/filters.fasta --nb-cpus 5 --log-file filter/filters.log --output-biom filter/filters.biom --summary filter/filters.html --excluded filter/filters_excluded.tsv --min-sample-presence 1 --min-abundance 0.00005
```

The outputs of this step are: 

+ *filters.fasta *: File with the fasta of the filtered sequences.  
+ *filters.biom*: Contingency matrix of the samples against the filtered clusters. 
+ *filters.html*: Html file with the results of the filtering process in graphical form.
+ *filters.log*: Log file of the filtering process.
* *filters_excluded.tsv*: tsv file containing the clusters that were excluded by the filter. 


#### Taxonomic assignment or reads 

Next, the taxonomic assignment of the readings was carried out. 

The code needed to process it is the following, where it is important to note that the appropriate database must be downloaded first:


```{bash, eval=F}
affiliation_OTU.py --input-fasta filter/filters.fasta --input-biom filter/filters.biom --nb-cpus 5 --log-file filter/affiliation.log --output-biom tax_asg/affiliation.biom --summary tax_asg/affiliation.html --reference silva_16S_db/*/silva_138_16S.fasta
```

The output files of this step are: 

+  *affiliation.biom*: Contingency matrix of the samples against the taxonomic assignment results.
+ *affiliation.html*: html file with the assignment results in graphical form.
+ *affiliation.log*: Log file of the taxonomic assignment process.



Once we have obtained the results of the taxonomic assignment, we can further filter the results to retain those whose species are reported in the NCBI database and that possess sequence identity and coverage characteristics desirable for a correct assignment.  For this purpose, the affiliation_filters tool was used.


```{bash,eval=F}
affiliation_filters.py --input-biom tax_asg/affiliation.biom --input-fasta filters/filters.fasta --output-biom tax_asg/frogs_affi_filter.biom --output-fasta tax_asg/frogs_affi_filter.fasta --summary tax_asg/frogs_affi_filter.html --impacted-multihit tax_asg/frogs_affi_filter_impacted_OTU_multihit.tsv --log-file tax_asg/affi_filter.log --delete --min-blast-identity 0.8 --min-blast-coverage 0.8 --ignore-blast-taxa "unknown species" --impacted tax_asg/frogs_affi_filter_impacted_OTU.tsv
```


The output files are:

+ * *frogs_affi_filter.biom*: Contingency matrix of the samples against the filtered taxonomic assignment results. 
+ * *frogs_affi_filter.fasta*: Fasta file of the filtered sequences corresponding to the conserved clusters.  
* * *frogs_affi_filter.html*: Html file with the graphically filtered assignment results.
* *affi_filter.log*: Log file of the taxonomic assignment filtering process.
**frogs_affi_filter_impacted_OTU_multihit.tsv*: The multihit TSV file associated with the impacted OTU.


#### Affiliationn results 

Finally, a summary of the affiliation results can be generated. The output files are as follows: 

+  *affiliations_stats.html*: HTML file with the assignment summary graphically.
* *affiliations_stats.log*: Log file of the taxonomic assignment filtering process.

The code needed to process it is the following:

```{bash, eval=F}
affiliations_stat.py --input-biom tax_asg/frogs_affi_filter.biom --output-file final_aff/affiliations_stats.html --log-file final_aff/affiliations_stats.log --multiple-tag blast_affiliations --tax-consensus-tag blast_taxonomy --identity-tag perc_identity --coverage-tag perc_query_coverage
```

Finally, the corresponding tsv file of the taxonomic assignments will be generated. The output files are as follows: 

+ *affiliation_final.tsv*: This output file will contain the abundance and metadata.
+ *multi_aff.tsv*: This output file will contain information on multiple alignments.
+ *biom_to_tsv.log*: Log file of the process.

The code needed to process it is as follows:

````{bash,eval=F}
biom_to_tsv.py --input-biom tax_asg/frogs_affi_filter.biom --input-fasta tax_asg/frogs_affi_filter.fasta --output-tsv final_aff/affiliation_final.tsv --output-multi-affi final_aff/multi_aff.tsv --log-file final_aff/biom_to_tsv.log
```




### Results visualization

To visualize the results, we can use the R package phyloseq, and use the following code. With this we can see the amount of readings that were lost due to frogs filters. 

```{r message=FALSE, warning=FALSE}
library(phyloseq)
library(phyloseq.extended)
biomfile <- "tax_asg/filter.biom"
frogs <- import_frogs(biomfile, taxMethod = "blast")
metadata <- read.table("metadata.tsv", row.names = 1, header = TRUE, sep = "\t", stringsAsFactors = FALSE)
sample_data(frogs) <- metadata

samples <- rownames(sample_data(frogs))
final <- sample_sums(frogs)
initial <- metadata$Reads
final <- as.vector(t(final))
df <- data.frame(initial,final,samples)
library(reshape2)
library(ggplot2)
df <- melt(df, id.vars='samples')
ggplot(df, aes(x=samples, y=value, fill=variable)) + 
       geom_bar(stat='identity', position='dodge') +scale_fill_discrete(name = "Lecturas", labels = c("Lecturas iniciales", "Lecturas finales"))+theme(axis.text.x = element_text(angle=90))+  xlab("Muestras") + ylab("Lecturas")
```

Subsequently we can use the results of the affiliation to obtain the abundances by taxonomic classification. As an example, the graphs by genus are shown below. Those containing multi-affiliate or unknown results were eliminated. 

```{r message=FALSE, warning=FALSE}
phy_obj0 = subset_taxa(frogs, Genus != "Multi-affiliation")
phy_obj0 = subset_taxa(phy_obj0, Genus != "unknown genus")
plot_bar(phy_obj0, fill="Genus") + facet_wrap(~Batch, scales= "free_x", nrow=1)

```

A representation by proportion of reads 

```{r message=FALSE, warning=FALSE}

plot_composition(phy_obj0, taxaRank1 = NULL, taxaSet1 = NULL, taxaRank2 = "Genus",numberOfTaxa = 10,x="Name") + 
  scale_fill_manual(values =c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F", "#CCFCCC", "#CCCCCC", "#FFFFCC", "#CC79A7","#8FD744FF","#EF9A9A","#AED581", "#009E73","#CCFCCC")) +
  facet_grid(~Batch, scales = "free_x", space = "free_x")
```


