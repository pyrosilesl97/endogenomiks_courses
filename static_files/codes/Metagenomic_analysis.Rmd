---
title: "Metagenomic analysis"
author: "Endogenomiks Scientific Team"
date: "2023-05-05"
output: html_document
---

# Metagenomic 16S analysis 

First a tsv file was created with the metadata of the data in question. This metadata for the 16S data looks as follows. 

```{r}
#Reading metadata table1
metadata <- read.table("metadata.tsv", row.names = 1, header = TRUE, sep = "\t", stringsAsFactors = TRUE)
metadata #Print metadata 
```


Después de este proceso se uso bash, para agregar la cantidad original de reads al archivo metadata. 

```{bash, eval=F}
cd {Directorio del analysis de 16S}
head -n 1 metadata.tsv | tr -d "\n" > header.txt #Extraer headers
echo -e "\tReads" >> header.txt # Agregar reads
grep L001 metadata.tsv  | sort -k1,1  > file1 # Extraer y ordenar datos
awk -F $'\t' '{system("zcat " $1"_R1.fastq.gz | echo $((`wc -l`/4))"  )}' file1 >> reads #Contar reads
cat header.txt <(paste file1 reads) >> metadata2.txt # Agregar header, con el número de reads y lo demas.
rm file1 header.txt reads # remover lo archivos creados en el proceso
```

A continuación se muestra el resultado de la tabla nueva de metadata. 

```{r}
#Reading metadata table2
metadata <- read.table("results_16S/metadata2.txt", row.names = 1, header = TRUE, sep = "\t", stringsAsFactors = TRUE)
metadata #Print metadata2
```

### Análisis de calidad

Para analizar la calidad de las muestras, se procesaron los lecturas mediante el uso de fastqc. para ello, se creo un script (fastqc.sh) que posteriormente se corrió. Los resultados se encuentran en la carpeta FASTQC, con su correspondiente nombre. Para ello, se utilizó el siguiente código. 

```{bash, eval=F}
mkdir FASTQC
for i in *.fastq.gz ; do echo " fastqc $i -o FASTQC" >> fastqc.sh ; done
. fastqc.sh

```

Finalmente se utilizó multiQC, para reunir los resultados de calidad en un solo archivo. Los cuáles pueden consultarse en el archivo html de la carpeta MULTIQC. Para ello, se utilizó el siguiente código.

```{bash, eval=F}
multiqc FASTQC -o MULTIQC
```

### Asignación taxonómica de las lecturas de secuenciación mediante FROGS.

Para llevar a cabo la asignación taxonómica de las lecturas de secuenciación, se utilizó la herramienta frogs. 

Para poder correr el siguiente código, es necesario tener instalado frogs en su sistema. En caso de usar linux o MacOS, se puede usar conda para ello se requiere el archivo frogs-conda-requirements.yaml que incluyo en la carpeta principal del presente análisis. Posterior a ello la siguiente linea de código permitirá instalar las herramientas necesarias utilizando conda. 

```{bash,eval=F}
cd {MOVE TO THE DIRECTORY OF THE UNZIP FILE}
conda env create --name frogs@4.0.1 --file frogs-conda-requirements.yaml
# to use FROGS, first you need to activate your environment
conda activate frogs@4.0.1
```

Una vez instalado frogs en su sistema, se pueden procesar las muestras con los siguientes comandos, primero se usará el programa de preprocesamiento de muestras, tenemos lecturas con un tamaño promedio de 300. 16S tiene un tamaño promedio de 410, máximo de 450 y mínimo de 340 ( parámetros recomendados por FROGS para el análisis de 16S). Se utilizará flash para unir las lecturas debido a su rapidez, comparado con pear. Los outputs del presente pipeline se encuentran en una carpeta nombrada frogs e incluyen para esta parte: 

+ *preprocess.fasta *: Archivo con el fasta de las secuencias preprocesadas y unidas. 
+ *preprocess.tsv*: Archivo tsv con la descripción de cada secuencia y a qué grupo pertenecen.
+ *preprocess.html*: Archivo html con los resultados de manera gráfica. 
+ *preprocess.log*: Archivo log del procesamiento.

El código necesario para procesarlo es el siguiente:

```{bash, eval=F}
#Primero se comprimieron los archivos de .fastq.gz para que sea más sencillo el procesamiento
tar zcvf data.tar.gz *.fastq.gz

#Es importante ver los estadísticas de las lecturas y lo que esperamos de 16S. 
preprocess.py illumina --input-archive data.tar.gz --min-amplicon-size 340 --max-amplicon-size 450 --merge-software flash --without-primers --R1-size 300 --R2-size 300 --nb-cpus 5 --output-dereplicated frogs/preprocess.fasta --output-count frogs/preprocess.tsv --summary frogs/preprocess.html --log-file frogs/preprocess.log --expected-amplicon-size 410
```

En el siguiente paso de nuestro flujo de trabajo requerimos llevar a cabo la clusterización de los datos analizados, para con ello, reconocer cuáles lecturas pertenecen al mismo organismo. Los outputs de este paso son: 

+ *clustering.fasta *: Archivo con el fasta de las secuencias por cluster. 
+ *clustering.biom*: Archivo Biological Observation Matrix (.biom) posee una tabla de contingencia de las muestras contra los clusters. 
+ *clustering_compo.tsv*: Archivo tsv con la composición de los clusters y la muestra a la que pertenecen.
+ *clustering_log.txt*: Archivo log del procesamiento.

Posteriormente se proceso un resumen de los estadísticos del proceso de clusterización, donde obtenemos los siguientes archivos: 

+ *clusters_stats.html*: Archivo html con los resultados del estadístico de la clusterización de manera gráfica.
+ *clusters_stats.log*: Archivo log del proceso de clusterización.

El código necesario para procesarlo es el siguiente:

```{bash, eval=F}
#Posteriormente clusterizamos los resultados 
clustering.py --nb-cpus 5 --input-fasta frogs/preprocess.fasta --input-count frogs/preprocess.tsv --output-biom frogs/clustering.biom --output-fasta frogs/clustering.fasta --output-compo frogs/clustering_compo.tsv --log-file frogs/clustering_log.txt --distance 3 --denoising
#To check the stats
clusters_stat.py --input-biom frogs/clustering.biom --output-file frogs/clusters_stats.html --log-file frogs/clusters_stats.log 

```

En el siguiente paso de nuestro flujo de trabajo se utilizará la herramienta apropiada para la remoción de quimeras. Los outputs de este paso son: 

+ *remove_chimera.fasta *: Archivo con el fasta de las secuencias que no poseen quimeras.  
+ *remove_chimera.biom*: Matriz de contingencia de las muestras contra los clusters después de la remover quimeras. 
+ *remove_chimera.html*: Archivo html con los resultados después de remover las quimeras de manera gráfica.
+ *remove_chimera.log*: Archivo log del proceso para remover de quimeras.

El código necesario para procesarlo es el siguiente:

```{bash, eval=F}
remove_chimera.py --input-fasta frogs/clustering.fasta --input-biom frogs/clustering.biom --non-chimera frogs/remove_chimera.fasta --nb-cpus 5 --log-file frogs/remove_chimera.log --out-abundance frogs/remove_chimera.biom --summary frogs/remove_chimera.html
```

A continuación se requiere remover los OTUS que poseen quimeras,contaminantes o artefactos. Los outputs de este paso son: 

+ *filters.fasta *: Archivo con el fasta de las secuencias filtradas.  
+ *filters.biom*: Matriz de contingencia de las muestras contra los clusters filtrados. 
+ *filters.html*: Archivo html con los resultados del proceso de filtrado de manera gráfica.
+ *filters.log*: Archivo log del proceso de filtrado.
+ *filters_excluded.tsv*: Archivo tsv que tiene los clusters que fueron excluidos por el filtro. 

El código necesario para procesarlo es el siguiente, donde se utilizó además un filtro por presencia (mínimo en una muestra) y por abudancia  (que se encuentre en el 0.00005% de las lecturas):

```{bash, eval=F}
otu_filters.py --input-fasta frogs/remove_chimera.fasta --input-biom frogs/remove_chimera.biom --output-fasta frogs/filters.fasta --nb-cpus 5 --log-file frogs/filters.log --output-biom frogs/filters.biom --summary frogs/filters.html --excluded frogs/filters_excluded.tsv --contaminant contaminants_db/phi.fa --min-sample-presence 1 --min-abundance 0.00005
```

A continuación, se llevó a cabo la asignación taxonómica de las lecturas. Los archivos de salida de este paso son: 

+ *affiliation.biom*: Matriz de contingencia de las muestras contra  los resultados de la asignación taxonómica. 
+ *affiliation.html*: Archivo html con los resultados de asignación de manera gráfica.
+ *affiliation.log*: Archivo log del proceso de asignación taxonómica.

El código necesario para procesarlo es el siguiente, donde es importante notar que primero se tiene que descargar la base de datos apropiada para ello (se incluye dicho paso en el código):


```{bash, eval=F}
#Para descargar la base de datos 
mkdir silva_16S_db
cd silva_16S_db
wget http://genoweb.toulouse.inra.fr/frogs_databanks/assignation/SILVA/16S/silva_138.1_16S.tar.gz
tar -xvf silva_138.1_16S.tar.gz
cd ..
#Para procesar los datos 
affiliation_OTU.py --input-fasta frogs/filters.fasta --input-biom frogs/filters.biom --nb-cpus 5 --log-file frogs/affiliation.log --output-biom frogs/affiliation.biom --summary frogs/affiliation.html --reference silva_16S_db/*/silva_138_16S.fasta
```

Una vez que hemos obtenido los resultados de la asignación taxonómica, podemos filtrar aún más los resultados para conservar aquellos cuya especie se encuentre reportada en la base de datos de NCBI y que posean características de identidad de secuencía y covertura deseable para una correcta asignación.  Para ello se utilizó la herramienta affiliation_filters, cuyos archivos de salida son:

+ *frogs_affi_filter.biom*: Matriz de contingencia de las muestras contra los resultados de la asignación taxonómica filtrados. 
+*frogs_affi_filter.fasta*: Archivo fasta de las secuencias filtradas correspondientes a los clusters conservados.  
+ *frogs_affi_filter.html*: Archivo html con los resultados de asignación filtrados de manera gráfica.
+ *affi_filter.log*: Archivo log del proceso de filtrado de las asignaciones taxonómicas.
+*frogs_affi_filter_impacted_OTU_multihit.tsv*: El archivo TSV multihit asociado a la OTU impactada.

El código necesario para procesarlo es el siguiente:

```{bash,eval=F}
affiliation_filters.py --input-biom frogs/affiliation.biom --input-fasta frogs/filters.fasta --output-biom frogs/frogs_affi_filter.biom --output-fasta frogs/frogs_affi_filter.fasta --summary frogs/frogs_affi_filter.html --impacted-multihit frogs/frogs_affi_filter_impacted_OTU_multihit.tsv --log-file frogs/affi_filter.log --delete --min-blast-identity 0.8 --min-blast-coverage 0.8 --ignore-blast-taxa "unknown species" --impacted frogs/frogs_affi_filter_impacted_OTU.tsv
```

Finalmente, se puede generar un resumen de los resultados de la afiliación. Los archivos de salida son los siguientes: 

+ *affiliations_stats.html*: Archivo html con el resumen de asignación de manera gráfica.
+ *affiliations_stats.log*: Archivo log del proceso de filtrado de las asignaciones taxonómicas.

El código necesario para procesarlo es el siguiente:

```{bash, eval=F}
affiliations_stat.py --input-biom frogs/affiliation.biom --output-file frogs/affiliations_stats.html --log-file frogs/affiliations_stats.log --multiple-tag blast_affiliations --tax-consensus-tag blast_taxonomy --identity-tag perc_identity --coverage-tag perc_query_coverage
```

Finalmente, se generará el archivo correpondiente al tsv de las asignaciones taxonómicas. Los archivos de salida son los siguientes: 

+ *affiliation_final.tsv*: Este archivo de salida contendrá la abundancia y los metadatos
+ *multi_aff.tsv*: Este archivo de salida contendrá información sobre alineaciones múltiples.
+ *biom_to_tsv.log*: Archivo log del proceso.

El código necesario para procesarlo es el siguiente:

```{bash,eval=F}
biom_to_tsv.py --input-biom frogs/frogs_affi_filter.biom --input-fasta frogs/frogs_affi_filter.fasta --output-tsv frogs/affiliation_final.tsv --output-multi-affi frogs/multi_aff.tsv --log-file frogs/biom_to_tsv.log
```

A manera de resumen, los archivos html poseen los resultados gráficos de cada parte del proceso. Los archivos finales son **affiliation_final.tsv**, **frogs_affi_filter.fasta** y **frogs_affi_filter.biom.** Que contienen los resultados de la asignación taxonómica. 

```{r}
affiliation_table_16s <- read.delim("results_16S/frogs/affiliation_final.tsv")

```



### Visualización de los resultados 

Para visualizar los resultados, podemos utilizar el paquete phyloseq de R, y usar el siguiente código. Con esto podemos ver la cantidad de lecturas que se perdieron debido a los filtros de frogs. 

```{r message=FALSE, warning=FALSE}
library(phyloseq)
library(phyloseq.extended)
setwd("/Users/pablo/Endogenomiks/Projects/January/Analysis_results/")
biomfile <- "results_16S/frogs/frogs_affi_filter.biom"
frogs <- import_frogs(biomfile, taxMethod = "blast")
metadata <- read.table("results_16S/metadata2.txt", row.names = 1, header = TRUE, sep = "\t", stringsAsFactors = FALSE)
sample_data(frogs) <- metadata

samples <- rownames(sample_data(frogs))
final <- sample_sums(frogs)
initial <- metadata$Reads
final <- as.vector(t(final))
df <- data.frame(initial,final,samples)
library(reshape2)
library(ggplot2)
df <- melt(df, id.vars='samples')
ggplot(df, aes(x=samples, y=value, fill=variable)) + 
       geom_bar(stat='identity', position='dodge') +scale_fill_discrete(name = "Lecturas", labels = c("Lecturas iniciales", "Lecturas finales"))+theme(axis.text.x = element_text(angle=90))+  xlab("Muestras") + ylab("Lecturas")
pdf("results_16S/final_graphs/lecturas.pdf")
ggplot(df, aes(x=samples, y=value, fill=variable)) + 
       geom_bar(stat='identity', position='dodge') +scale_fill_discrete(name = "Lecturas", labels = c("Lecturas iniciales", "Lecturas finales"))+theme(axis.text.x = element_text(angle=90))+  xlab("Muestras") + ylab("Lecturas")
dev.off()
```

Posteriormente podemos utilizar los resultados de la afiliación para obtener las abundancias por clasificación taxonómica, estas se encuentran en el archivo affiliation_graphs_frogs.pdf en la carpeta final_graphs. A manera de ejemplo, se muestran a continuación las gráficas por Género. Para lo cual se eliminaro aquellas que contenían resultados multiafiliados o desconocidos. 

```{r message=FALSE, warning=FALSE}
phy_obj0 = subset_taxa(frogs, Genus != "Multi-affiliation")
phy_obj0 = subset_taxa(phy_obj0, Genus != "unknown genus")

plot_bar(phy_obj0, fill="Genus") + facet_wrap(~EnvType, scales= "free_x", nrow=1)
plot_composition(phy_obj0, taxaRank1 = NULL, taxaSet1 = NULL, taxaRank2 = "Genus",numberOfTaxa = 10,x="Treatment") + 
  scale_fill_manual(values =c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F", "#CCFCCC", "#CCCCCC", "#FFFFCC", "#CC79A7","#8FD744FF","#EF9A9A","#AED581", "#009E73","#CCFCCC")) +
  facet_grid(~EnvType, scales = "free_x", space = "free_x")

#Save in a pdf 
pdf("results_16S/final_graphs/affiliation_graphs_frogs.pdf")

## By genus
#Bar_plot
plot_bar(phy_obj0, fill="Genus",title="Abundance in reads by genus") + facet_wrap(~EnvType, scales= "free_x", nrow=1)
#Composition plot
plot_composition(phy_obj0, taxaRank1 = NULL, taxaSet1 = NULL, taxaRank2 = "Genus",numberOfTaxa = 10,x="Treatment") + 
  scale_fill_manual(values =c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F", "#CCFCCC", "#CCCCCC", "#FFFFCC", "#CC79A7","#8FD744FF","#EF9A9A","#AED581", "#009E73","#CCFCCC")) +
  facet_grid(~EnvType, scales = "free_x", space = "free_x")+
  ggtitle("Relative abundance by genus (Most abundant 10)")
#By phylum 
plot_bar(phy_obj0, fill="Phylum",title="Abundance in reads by Phylum") + facet_wrap(~EnvType, scales= "free_x", nrow=1)
plot_composition(phy_obj0, taxaRank1 = NULL, taxaSet1 = NULL, taxaRank2 = "Phylum",numberOfTaxa = 10,x="Treatment") + 
  scale_fill_manual(values =c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F", "#CCFCCC", "#CCCCCC", "#FFFFCC", "#CC79A7","#8FD744FF","#EF9A9A","#AED581", "#009E73","#CCFCCC")) +
  facet_grid(~EnvType, scales = "free_x", space = "free_x")+
  ggtitle("Relative abundance by Phylum (Most abundant 10)")

#By family 
plot_bar(phy_obj0, fill="Family",title="Abundance in reads by Family") + facet_wrap(~EnvType, scales= "free_x", nrow=1)
plot_composition(phy_obj0, taxaRank1 = NULL, taxaSet1 = NULL, taxaRank2 = "Family",numberOfTaxa = 10,x="Treatment") + 
  scale_fill_manual(values =c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F", "#CCFCCC", "#CCCCCC", "#FFFFCC", "#CC79A7","#8FD744FF","#EF9A9A","#AED581", "#009E73","#CCFCCC")) +
  facet_grid(~EnvType, scales = "free_x", space = "free_x")+
  ggtitle("Relative abundance by Family (Most abundant 10)")

#By kingdom 
plot_bar(phy_obj0, fill="Kingdom",title="Abundance in reads by Kingdom") + facet_wrap(~EnvType, scales= "free_x", nrow=1)
plot_composition(phy_obj0, taxaRank1 = NULL, taxaSet1 = NULL, taxaRank2 = "Kingdom",numberOfTaxa = 10,x="Treatment") + 
  scale_fill_manual(values =c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F", "#CCFCCC", "#CCCCCC", "#FFFFCC", "#CC79A7","#8FD744FF","#EF9A9A","#AED581", "#009E73","#CCFCCC")) +
  facet_grid(~EnvType, scales = "free_x", space = "free_x")+
  ggtitle("Relative abundance by Kingdom (Most abundant 10)")

dev.off()
```


### Rhea analysis 

Para llevar a cabo el análisis utilizando Rhea, se requieren un árbol filogenético en formato newick que no hemos generado aún. Para ello, se utilizó el siguiente código. Este código produce un arbol filogenético en formato nexus y newick. 

```{r phylogenetic_tree_16s, message=FALSE, warning=FALSE, cache=TRUE}
#Loading required packages for the phylogenetic analysis
library(seqinr)
library(ape)
library(msa) # this package is available through the Bioconductor packages. If you do not have the BiocManager package installed, you could install this using t


## Read sequences from FASTA file of the cases

sequence <- readAAStringSet("results_16S/frogs/tree_headers.fasta")

## Perform multiple sequence alignment

my_alignment <- msa(sequence)

## Compute distance matrix

my_alignment_sequence <- msaConvert(my_alignment, type="seqinr::alignment")

distance_alignment <- dist.alignment(my_alignment_sequence)

## compute phylogenetic tree using neighbor joining

Tree <- bionj(distance_alignment)


write.tree(Tree, file = "results_16S/tree_newick_16S", append = FALSE,
digits = 5, tree.names = TRUE)

write.nexus(Tree, file = "results_16S/tree_nexus_16S", translate = TRUE)

```

Ahora ya tenemos el árbol filogenético en formato newick podemos realizar los siguientes análisis siguiendo el código de Rhea. Esta sección produce las gráficas de rarefaction encontradas en las gráficas finales y las tablas de OTUS normalizadas y filtradas. 


```{r Rhea_normalization_16S, message=FALSE, warning=FALSE}
#Processing OTU_table
otu_table <- cbind(affiliation_table_16s[,12:18],affiliation_table_16s[,"blast_taxonomy"])
original_names <- colnames(otu_table) 
original_names[8] <- "taxonomy"
colnames(otu_table) <- original_names
rownames(otu_table) <- affiliation_table_16s[,"observation_name"]
#Rhea pipeline
           #<--- CHANGE ACCORDINGLY

#' Please select the normalisation method
#' 0 = No random subsampling, no rounding
#' 1 = Random subsampling with rounding
method <- 0                                   #<--- CHANGE ACCORDINGLY

#' Pease select the normalization level used
#' 0 = Minimum sampling size
#' 1 = Fixed value (e.g. 1000)
level <- 0                                    #<--- CHANGE ACCORDINGLY

#' Please choose the value at which all samples will be normalized. (Only used if level selected is 1)
normCutoff <- 1000

#' Please choose the number of samples with the steepest rarefaction curves to be selectively plotted
#' The default number of samples presented separately is 5
labelCutoff <- 5                              #<--- CHANGE ACCORDINGLY

######                  NO CHANGES ARE NEEDED BELOW THIS LINE               ######

##################################################################################
######                             Main Script                              ######
##################################################################################

###################       Load all required libraries     ########################

# Check if required packages are already installed, and install if missing
packages <- c("GUniFrac","vegan")

# Function to check whether the package is installed
InsPack <- function(pack)
{
  if ((pack %in% installed.packages()) == FALSE) {
    install.packages(pack,repos = "http://cloud.r-project.org/")
  }
}

# Applying the installation on the list of packages
lapply(packages, InsPack)

# Make the libraries
lib <- lapply(packages, require, character.only = TRUE)

# Check if it was possible to install all required libraries
flag <- all(as.logical(lib))

###################       Read all required input files      ####################


# Making sure tha taxonomy column gets lower-case
col_names <- colnames(otu_table)
tax_ind <- which(sapply(tolower(col_names),
                        function(x) "taxonomy" %in% x, USE.NAMES = FALSE))
if (length(tax_ind) != 0) {
  col_names[tax_ind] <- tolower(col_names[tax_ind])
  colnames(otu_table) <- col_names
}
rm(col_names)
rm(tax_ind)

# Clean table from empty lines
otu_table <- otu_table[!apply(is.na(otu_table) | otu_table=="",1,all),]

####################       Normalize OTU Table          ###################


# Save taxonomy information in vector
taxonomy <- as.vector(otu_table$taxonomy)

# Delete column with taxonomy information in dataframe
otu_table$taxonomy <- NULL



if (level == 0) {
  # Calculate the minimum sum of all columns/samples
  min_sum <- min(colSums(otu_table))
} else {
  # The minimum size is set to a fixed reference level
  min_sum <- normCutoff
}


if (method == 0) {
  # Divide each value by the sum of the sample and multiply by the minimal sample sum
  norm_otu_table <- t(min_sum * t(otu_table) / colSums(otu_table))
} else {
  # Rarefy the OTU table to an equal sequencing depth
  norm_otu_table <- Rarefy(t(otu_table),depth = min_sum)
  norm_otu_table <- t(as.data.frame(norm_otu_table$otu.tab.rff))
}

# Calculate relative abundances for all OTUs over all samples
# Divide each value by the sum of the sample and multiply by 100
rel_otu_table <- t(100 * t(otu_table) / colSums(otu_table))


# Re-insert the taxonomy information in normalized counts table
norm_otu_table_tax <- cbind(norm_otu_table,taxonomy)

# Re-insert the taxonomy information in relative abundance table
rel_otu_table_tax <- cbind(rel_otu_table,taxonomy)

################################################################################
# Generate a two-sided pdf with a rarefaction curve for all samples and a curve
# Plot the rarefaction curve for all samples
pdf(file="results_16S/final_graphs/rarefaction_curve.pdf")
rarefactionCurve <- rarecurve(data.frame(t(otu_table)),
                              step = 20,
                              col = "black",
                              lty = "solid",
                              label = F,
                              xlab = "Number of Reads",
                              ylab = "Number of Species",
                              main = "Rarefaction Curves of All Samples")

# Generate empty vectors for the analysis of the rarefaction curve
slope = vector()
SampleID = vector()
angle <- c()

# Iterate through all samples
for (i in seq_along(rarefactionCurve)) {
  # If the sequencing depth is greater than 100, the slope of the line that passes between the last and last-100 count is calculated
  richness <- ifelse(length(rarefactionCurve[[i]]) > 6, 
                     (rarefactionCurve[[i]][length(rarefactionCurve[[i]])] - rarefactionCurve[[i]][length(rarefactionCurve[[i]])-5])/(attr(rarefactionCurve[[i]], "Subsample")[length(rarefactionCurve[[i]])]-attr(rarefactionCurve[[i]], "Subsample")[length(rarefactionCurve[[i]])-5]) , 1000)
  angle[i] <- ifelse(richness!=1000, atan(richness)*180/pi, NA)
  slope <- c(slope,richness)
  SampleID <- c(SampleID,as.character(names(otu_table)[i]))
}

# Generate the output table for rarefaction curve
curvedf <- cbind(SampleID, slope, angle)
ordered_vector <- order(as.numeric(curvedf[,2]), decreasing = TRUE)
curvedf <- curvedf[order(as.numeric(curvedf[,2]), decreasing = TRUE),]

# Generates a graph with all samples
# Underestimated cases are shown in red
for (i in 1:labelCutoff) {
  N <- attr(rarefactionCurve[[ordered_vector[i]]], "Subsample")
  lines(N, rarefactionCurve[[ordered_vector[i]]],col="red")
}
dev.off()

# Determine the plotting width and height
Nmax <- sapply(rarefactionCurve[ordered_vector[1:labelCutoff]], function(x) max(attr(x, "Subsample")))
Smax <- sapply(rarefactionCurve[ordered_vector[1:labelCutoff]], max)

# Creates an empty plot for rarefaction curves of underestimated cases
pdf(file="results_16S/final_graphs/rarefaction_curve_2.pdf")

plot(c(1, max(Nmax)), c(1, max(Smax)), xlab = "Number of Reads",
     ylab = "Number of Species", type = "n", main=paste(labelCutoff,"- most undersampled cases"))

for (i in 1:labelCutoff) {
  N <- attr(rarefactionCurve[[ordered_vector[i]]], "Subsample")
  lines(N, rarefactionCurve[[ordered_vector[i]]],col="red")
  text(max(attr(rarefactionCurve[[ordered_vector[i]]],"Subsample")), max(rarefactionCurve[[ordered_vector[i]]]), curvedf[i,1],cex=0.6)
}
dev.off()
#################################################################################
######                        Write Output Files                           ######
#################################################################################

# Write the normalized table in a file and copy in directories alpha-diversity and beta-diversity if existing
write.table(norm_otu_table, "results_16S/OTUs_Table-norm.tab", sep = "\t",col.names = NA, quote = FALSE)
# Write the normalized table with taxonomy in a file
write.table(norm_otu_table_tax, "results_16S/OTUs_Table-norm-tax.tab", sep = "\t",col.names = NA, quote = FALSE)

# Write the normalized relative abundance table in a file and copy in directory Serial-Group-Comparisons if existing
write.table(rel_otu_table, "results_16S/OTUs_Table-norm-rel.tab", sep = "\t",col.names = NA, quote = FALSE)

# Write the normalized relative abundance with taxonomy table in a file and copy in directory Taxonomic-Binning if existing
write.table(rel_otu_table_tax, "results_16S/OTUs_Table-norm-rel-tax.tab", sep ="\t",col.names = NA, quote = FALSE)

# Write the rarefaction table
write.table(curvedf, "results_16S/RarefactionCurve.tab", sep ="\t", quote = FALSE, row.names = FALSE)

#################################################################################
######                           End of Script                             ######
#################################################################################
```

Posteriormente se calcula la diversidad alpha, para ello se utilizó el siguiente código de Rhea. La cuál procesa las muestras y produce la tabla alpha-diversity.tab que contiene los resultados. 


```{r Rhea_alpha_div_16s, message=FALSE, warning=FALSE}
file_name <- "results_16S/OTUs_Table-norm.tab"  
#' The abundance filtering cutoff 
eff.cutoff <- 0.0025 # this is the default value for Effective Richness (0.25%)

#' The normalized depth cutoff
norm.cutoff <- 1000 # this is the default value for Standard Richness (1000)

##################################################################################
######                        Diversity Functions                           ###### 
##################################################################################

# Calculate the species richness in a sample
Species.richness <- function(x)
{
  # Count only the OTUs that are present >0.5 normalized counts (normalization produces real values for counts)
  count=sum(x[x>0.5]^0)
  return(count)
}

# Calculate the Effective species richness in each individual sample
Eff.Species.richness <- function(x)
{
  # Count only the OTUs that are present more than the set proportion
  total=sum(x)
  count=sum(x[x/total>eff.cutoff]^0)
  return(count)
}

# Calculate the Normalized species richness in each individual sample
Norm.Species.richness <- function(x)
{
  # Count only the OTUs that are present >0.5 normalized counts (normalization produces real values for counts)
  # Given a fixed Normalization reads depth
  total=sum(x)
  count=sum(x[norm.cutoff*x/total>0.5]^0)
  return(count)
}


# Calculate the Shannon diversity index
Shannon.entropy <- function(x)
{
  total=sum(x)
  se=-sum(x[x>0]/total*log(x[x>0]/total))
  return(se)
}

# Calculate the effective number of species for Shannon
Shannon.effective <- function(x)
{
  total=sum(x)
  se=round(exp(-sum(x[x>0]/total*log(x[x>0]/total))),digits =2)
  return(se)
}

# Calculate the Simpson diversity index
Simpson.concentration <- function(x)
{
  total=sum(x)
  si=sum((x[x>0]/total)^2)
  return(si)
}

# Calculate the effective number of species for Simpson
Simpson.effective <- function(x)
{
  total=sum(x)
  si=round(1/sum((x[x>0]/total)^2),digits =2)
  return(si)
}

##################################################################################
######                             Main Script                              ###### 
##################################################################################

# Read a normalized OTU-table without taxonomy  
otu_table <- read.table (file_name, 
                       check.names = FALSE, 
                       header=TRUE, 
                       dec=".", 
                       sep = "\t",
                       row.names = 1)

# Clean table from empty lines
otu_table <- otu_table[!apply(is.na(otu_table) | otu_table=="",1,all),]

# Order and transpose OTU-table
my_otu_table <- otu_table[,order(names(otu_table))] 
my_otu_table <-data.frame(t(my_otu_table))

# Apply diversity functions to table
otus_div_stats<-data.frame(my_otu_table[,0])
otus_div_stats$Richness<-apply(my_otu_table,1,Species.richness)
otus_div_stats$Normalized.Richness<-apply(my_otu_table,1,Norm.Species.richness)
otus_div_stats$Effective.Richness<-apply(my_otu_table,1,Eff.Species.richness)
otus_div_stats$Shannon.Index<-apply(my_otu_table,1,Shannon.entropy)
otus_div_stats$Shannon.Effective<-apply(my_otu_table,1,Shannon.effective)
otus_div_stats$Simpson.Index<-apply(my_otu_table,1,Simpson.concentration)
otus_div_stats$Simpson.Effective<-apply(my_otu_table,1,Simpson.effective)
otus_div_stats$Evenness <- otus_div_stats$Shannon.Index/log(otus_div_stats$Richness,2)


# Write the results in a file and copy in directory "Serial-Group-Comparisons" if existing
write.table(otus_div_stats, "results_16S/alpha-diversity.tab", sep="\t", col.names=NA, quote=FALSE)

##################################################################################
######                          End of Script                               ###### 
##################################################################################

```



A continuación, se procesa la diversidad beta, con el siguiente código. La cuál produce un phylogram.pdf en la carpeta final. Ademas de las gráficas de NMDS, todas se encuentran en final_graphs.

```{r Rhea_beta_div_16S, message=FALSE, warning=FALSE}
##################################################################################
######             Set parameters in this section manually                  ######
##################################################################################

#' Please give the file name of the normalized OTU-table without taxonomic classification
input_otu = "results_16S/OTUs_Table-norm.tab"              #<--- CHANGE ACCORDINGLY !!!

#' Please give the name of the meta-file that contains individual sample information
input_meta = "results_16S/metadata2.txt"                #<--- CHANGE ACCORDINGLY !!!

#' Please give the name of the phylogenetic tree constructed from the OTU sequences
input_tree = "results_16S/tree_newick_16S"                   #<--- CHANGE ACCORDINGLY !!!

#' Please give the column name (in the mapping file) of the categorical variable to be used for comparison (e.g. Genotype)
#' Please make sure that your name do not contain hyphens "-" as they will cause problems in the parsing of the names.
group_name = "Treatment"                            #<--- CHANGE ACCORDINGLY !!!

##################################################################################
######                  Additional parameters                               ######
##################################################################################

#' Turn on sample labeling
#' 0 = Samples are not labeled in the MDS/NMDS plots
#' 1 = All Samples are labeled in the MDS/NMDS plots
label_samples = 0

#' Determine which sample label should appear
#' Write the name of samples (in quotation marks), which should appear in the MDS/NMDS plots, in the vector (c) below
#' If more than one sample should be plotted, please separate their IDs by comma (e.g. c("sample1","sample2"))
label_id =c("")

#' De-Novo Clustering will be performed for the number of samples or maximal for the set limit
#' Default Limit is 100
kmers_limit=20

######                  NO CHANGES ARE NEEDED BELOW THIS LINE               ######

##################################################################################
######                             Main Script                              ######
##################################################################################

###################       Load all required libraries     ########################

# Check if required packages are already installed, and install if missing
packages <-c("ade4","GUniFrac","phangorn","cluster","fpc","vegan","clusterSim") 

# Function to check whether the package is installed
InsPack <- function(pack)
{
  if ((pack %in% installed.packages()) == FALSE) {
    install.packages(pack,repos ="http://cloud.r-project.org/")
  } 
}

# Applying the installation on the list of packages
lapply(packages, InsPack)

# Make the libraries
lib <- lapply(packages, require, character.only = TRUE)

# Check if it was possible to install all required libraries
flag <- all(as.logical(lib))


###################       Read all required input files      ####################
# Load the mapping file containing individual sample information (sample names in the first column)
meta_file <- read.table (file = input_meta, check.names = FALSE, header = TRUE, dec = ".", sep = "\t", row.names = 1, comment.char = "")

# Clean table from empty lines
meta_file <- data.frame(meta_file[!apply(is.na(meta_file) | meta_file=="",1,all),,drop=FALSE])

# Order the mapping file by sample names (ascending)
meta_file <- data.frame(meta_file[order(row.names(meta_file)),,drop=FALSE])

# Save the position of the target group name in the mapping file
meta_file_pos <- which(colnames(meta_file) == group_name)

# Select metadata group based on the pre-set group name
all_groups <- as.factor(meta_file[,meta_file_pos])

#------------------------------------------------------------------------

# Load the tab-delimited file containing the values to be analyzed (samples names in the first column)
otu_file <- read.table (file = input_otu, check.names = FALSE, header = TRUE, dec = ".", sep = "\t", row.names = 1, comment.char = "")

# Clean table from empty lines
otu_file <- otu_file[!apply(is.na(otu_file) | otu_file =="",1,all),]

# keep only those rows that appear in the mapping file
otu_file <- otu_file[,rownames(meta_file)]

# OTU-table and mapping file should have the same order and number of sample names
# Order the OTU-table by sample names (ascending)
otu_file <- otu_file[,order(names(otu_file))]

# Transpose OTU-table and convert format to a data frame
otu_file <- data.frame(t(otu_file), check.names = FALSE)

#------------------------------------------------------------------------

# Load the phylogenetic tree calculated from the OTU sequences 
tree_file <- read.tree(input_tree)

# Remove single quotes from the tips of the tree
tree_file$tip.label <- gsub("'", "", tree_file$tip.label)

# Root the OTU tree at midpoint 
rooted_tree <- midpoint(tree_file)


####################       Calculate beta-diversity          ###################

# Create the directory where all output files are saved (is named after the target group name set above for comparisons)
dir.create(group_name)

# Calculate the UniFrac distance matrix for comparing microbial communities
unifracs <- GUniFrac(otu_file, rooted_tree, alpha = c(0.0,0.5,1.0))$unifracs

# Weight on abundant lineages so the distance is not dominated by highly abundant lineages with 0.5 having the best power
unifract_dist <- unifracs[, , "d_0.5"]

################ Generate tree #######################

# Save the UniFrac output as distance object
all_dist_matrix <- as.dist(unifract_dist)

# Apply a hierarchical cluster analysis on the distance matrix based on the Ward's method
all_fit <- hclust(all_dist_matrix, method = "ward.D2")

# Generates a tree from the hierarchically generated object
tree <- as.phylo(all_fit)
plot_color<-rainbow(length(levels(all_groups)))[all_groups]

# Save the generated phylogram in a pdf file
pdf(file="results_16S/final_graphs/phylogram_curve.pdf")

# The tree is visualized as a Phylogram color-coded by the selected group name
plot(tree, type = "phylogram",use.edge.length = TRUE, tip.color = (plot_color), label.offset = 0.01)
print.phylo(tree)
axisPhylo()
tiplabels(pch = 16, col = plot_color)
dev.off()

#################            Build NMDS plot           ########################

# Generated figures are saved in a pdf file 
file_name <- "results_16S/final_graphs/beta_diversity_curve.pdf"
pdf(file_name)

# Calculate the significance of variance to compare multivariate sample means (including two or more dependent variables)
# Omit cases where there isn't data for the sample (NA)
all_groups_comp <- all_groups[!is.na(all_groups)]
unifract_dist_comp <- unifract_dist[!is.na(all_groups), !is.na(all_groups)]
adonis<-adonis2(as.dist(unifract_dist_comp) ~ all_groups_comp)
permdisp <- permutest(betadisper(as.dist(unifract_dist_comp),as.factor(all_groups_comp),type="median"))
all_groups_comp<-factor(all_groups_comp,levels(all_groups_comp)[unique(all_groups_comp)])

# Calculate and display the MDS plot (Multidimensional Scaling plot)
s.class(
  cmdscale(unifract_dist_comp, k = 2), col = unique(plot_color), cpoint =
    2, fac = all_groups_comp, sub = paste("MDS plot of Microbial Profiles\nPERMDISP     p=",permdisp[["tab"]][["Pr(>F)"]][1],"\n",
                                          "PERMANOVA  p=",adonis[1,5],sep="")
)
if (label_samples==1) {
  lab_samples <- row.names(cmdscale(unifract_dist_comp, k = 2))
  ifelse (label_id != "",lab_samples <- replace(lab_samples, !(lab_samples %in% label_id), ""), lab_samples)
  text(cmdscale(unifract_dist_comp, k = 2),labels=lab_samples,cex=0.7,adj=c(-.1,-.8))
}

# Calculate and display the NMDS plot (Non-metric Multidimensional Scaling plot)
meta <- metaMDS(unifract_dist_comp,k = 2)
s.class(
  meta$points, col = unique(plot_color), cpoint = 2, fac = all_groups_comp,
  sub = paste("metaNMDS plot of Microbial Profiles\nPERMDISP     p=",permdisp[["tab"]][["Pr(>F)"]][1],"\n",
              "PERMANOVA  p=",adonis[1,5],sep="")
)
if (label_samples==1){
  lab_samples <- row.names(meta$points)
  ifelse (label_id != "",lab_samples <- replace(lab_samples, !(lab_samples %in% label_id), ""), lab_samples)
  text(meta$points,labels=lab_samples,cex=0.7,adj=c(-.1,-.8))
}

#close the pdf file
dev.off()

###############          NMDS for pairwise analysis        ###################

# This plot is only generated if there are more than two groups included in the comparison
# Calculate the pairwise significance of variance for group pairs
# Get all groups contained in the mapping file
unique_groups <- levels(all_groups_comp)
if (dim(table(unique_groups)) > 2) {
  
  # Initialise vector and lists
  pVal = NULL
  permdisppval=NULL
  pairedMatrixList <- list(NULL)
  pair_1_list <- NULL
  pair_2_list <- NULL
  
  for (i in 1:length(combn(unique_groups,2)[1,])) {
    
    # Combine all possible pairs of groups
    pair_1 <- combn(unique_groups,2)[1,i]
    pair_2 <- combn(unique_groups,2)[2,i]
    
    # Save pairs information in a vector
    pair_1_list[i] <- pair_1
    pair_2_list[i] <- pair_2
    
    # Generate a subset of all samples within the mapping file related to one of the two groups
    inc_groups <-
      rownames(subset(meta_file, meta_file[,meta_file_pos] == pair_1
                      |
                        meta_file[,meta_file_pos] == pair_2))
    
    # Convert UniFrac distance matrix to data frame
    paired_dist <- as.data.frame(unifract_dist_comp)
    
    # Save all row names of the mapping file
    row_names <- rownames(paired_dist)
    
    # Add row names to the distance matrix
    paired_dist <- cbind(row_names,paired_dist)
    
    # Generate distance matrix with samples of the compared groups (column-wise)
    paired_dist <- paired_dist[sapply(paired_dist[,1], function(x) all(x %in% inc_groups)),]
    
    # Remove first column with unnecessary group information
    paired_dist[,1] <- NULL
    paired_dist <- rbind(row_names,paired_dist)
    
    # Generate distance matrix with samples of the compared group (row-wise)
    paired_dist <- paired_dist[,sapply(paired_dist[1,], function(x) all(x %in% inc_groups))]
    
    # Remove first row with unnecessary group information 
    paired_dist <- paired_dist[-1,]
    
    # Convert generated distance matrix to data type matrix (needed by multivariate analysis)
    paired_matrix <- as.matrix(paired_dist)
    class(paired_matrix) <- "numeric"
    
    # Save paired matrix in list
    pairedMatrixList[[i]] <- paired_matrix
    
    # Applies multivariate analysis to a pair out of the selected groups
    adonis <- adonis2(paired_matrix ~ all_groups_comp[all_groups_comp == pair_1 |
                                                        all_groups_comp == pair_2])
    
    permdisp <- permutest(betadisper(as.dist(paired_matrix),as.factor(all_groups_comp[all_groups_comp == pair_1 |
                                                                                        all_groups_comp == pair_2]),type="median"),pairwise = T)
    
    # List p-values
    pVal[i] <- adonis[1,5]
    permdisppval[i] <- permdisp$pairwise[2]
    
  }
  
  # Adjust p-values for multiple testing according to Benjamini-Hochberg method
  pVal_BH <- round(p.adjust(pVal,method="BH", n=length(pVal)),4)
  permdisppval_BH <- round(p.adjust(permdisppval,method="BH", n=length(permdisppval)),4)
  
  
  # Generated NMDS plots are stored in one pdf file called "pairwise-beta-diversity-nMDS.pdf"
  file_name <- "results_16S/final_graphs/pairwise-beta-diversity-NMDS.pdf"
pdf(file_name)  
  for(i in 1:length(combn(unique_groups,2)[1,])){
    meta <- metaMDS(pairedMatrixList[[i]], k = 2)
    s.class(
      meta$points,
      col = rainbow(length(levels(all_groups_comp))), cpoint = 2,
      fac = as.factor(all_groups_comp[all_groups_comp == pair_1_list[i] |
                                        all_groups_comp == pair_2_list[i]]),
      sub = paste("NMDS plot of Microbial Profiles\n ",pair_1_list[i]," - ",pair_2_list[i], "\n PERMDISP     p=",permdisppval[[i]],",","  p.adj=", permdisppval_BH[i],"\n",
                  " PERMANOVA  p=",pVal[i],","," p.adj=",pVal_BH[i],sep="")
    )
  }
  dev.off()
  
  # Generated MDS plots are stored in one pdf file called "pairwise-beta-diversity-MDS.pdf"
    file_name <- "results_16S/final_graphs/pairwise-beta-diversity-MDS.pdf"
pdf(file_name) 
  
  for(i in 1:length(combn(unique_groups,2)[1,])){
    # Calculate and display the MDS plot (Multidimensional Scaling plot)
    s.class(
      cmdscale(pairedMatrixList[[i]], k = 2), col = rainbow(length(levels(all_groups_comp))), cpoint =
        2, fac = as.factor(all_groups_comp[all_groups_comp == pair_1_list[i] |
                                             all_groups_comp == pair_2_list[i]]), 
      sub = paste("MDS plot of Microbial Profiles\n ",pair_1_list[i]," - ",pair_2_list[i], "\n PERMDISP     p=",permdisppval[[i]],",","  p.adj=", permdisppval_BH[i],"\n",
                  " PERMANOVA  p=",pVal[i],","," p.adj=",pVal_BH[i],sep="")
    )
  }
  dev.off()                                     
  
}

######                        Determine number of clusters                           ######

ch_nclusters=NULL
sil_nclusters=NULL
dunn_nclusters=NULL
db_nclusters=NULL

if (dim(otu_file)[1]-1 <= kmers_limit) {
  kmers_limit=dim(otu_file)[1]-1
}
for (k in 1:kmers_limit) { 
  if (k==1) {
    ch_nclusters[k]=NA 
    sil_nclusters[k]=NA
    dunn_nclusters=NA
    db_nclusters=NA
  } else {
    # Partitioning the data into k clusters (max k is number of samples within the dataset)
    data_cluster=as.vector(pam(as.dist(unifract_dist_comp), k, diss=TRUE)$clustering)
    
    # Calculate Calinski-Harabasz and silhouette Index 
    index=cluster.stats(as.dist(unifract_dist_comp),data_cluster)
    index_db=index.DB(x=otu_file,cl=data_cluster,d=as.dist(unifract_dist_comp), centrotypes="medoids")
    ch_nclusters[k] <- index[["ch"]]
    sil_nclusters[k] <- index[["avg.silwidth"]]
    dunn_nclusters[k] <- index[["dunn2"]]
    db_nclusters[k] <-  index_db[["DB"]]
    print(k)
  }
}

# Generated plot showing the optimal number of clusters
for (i in 1:2){
  if (i==1) { pdf("results_16S/final_graphs/De-novo-clustering.png") }
  if (i==2) { pdf("results_16S/final_graphs/De-novo-clustering.png")}
  
  plot(ch_nclusters, type="h", xlab="k clusters", ylab="CH index",main="Optimal number of clusters (CH index)")
  title(sub="*The higher the value the better", adj=0, cex.sub=0.9)
  plot(sil_nclusters, type="h", xlab="k clusters", ylab="Average silhouette width",main="Optimal number of clusters (Silhouette index)")
  title(sub="*The higher the value the better", adj=0, cex.sub=0.9)
  plot(dunn_nclusters, type="h", xlab="k clusters", ylab="Dunn Index",main="Optimal number of clusters (Dunn Index)")
  title(sub="*The higher the value the better", adj=0, cex.sub=0.9)
  plot(db_nclusters, type="h", xlab="k clusters", ylab="Davies-Bouldin Index",main="Optimal number of clusters (Davies-Bouldin Index)")
  title(sub="*The lower the value the better", adj=0, cex.sub=0.9)
  
  dev.off()
}

#################################################################################
######                        Write Output Files                           ######
#################################################################################

# Write the distance matrix table in a file
file_name <- "results_16S/distance-matrix-gunif.tab"
write.table( unifract_dist_comp, file_name, sep = "\t", col.names = NA, quote = FALSE)
write.table( unifract_dist_comp, "results_16S/distance-matrix-gunif.tab", sep = "\t", col.names = NA, quote = FALSE)
write.tree(tree,"results_16S/samples-Tree.nwk",tree.names = FALSE)

# Graphical output files are generated in the main part of the script
if(!flag) { stop("
                 It was not possible to install all required R libraries properly.
                 Please check the installation of all required libraries manually.\n
                 Required libaries:ade4, GUniFrac, phangorn")
}

#################################################################################
######                           End of Script                             ######
#################################################################################

```


Posteriomente se realiza el taxonomic binning para procesar los datos de afiliación taxonómica en R. Para ello se utilizó el siguiente código. Los resultados gráficos se encuentran en la carpeta Taxonomic binning. 
```{r taxonomic_binning_rhea_16s}
##################################################################################
######             Set parameters in this section manually                  ######
##################################################################################

#' Please set the directory of the script as the working folder (e.g D:/studyname/NGS-Data/Rhea/numtax/)
#' Note: the path is denoted by forward slash "/"
setwd("results_16S/") #<--- CHANGE ACCORDINGLY

#' Please give the file name of the OTU-table containing relative abundances and taxonomic classification 
otu_file<-"OTUs_Table-norm-rel-tax.tab"  #<--- CHANGE ACCORDINGLY


######                  NO CHANGES ARE NEEDED BELOW THIS LINE               ######

##################################################################################
######                             Main Script                              ###### 
##################################################################################

###################            Read input table              ####################

# Load the tab-delimited file containing the abundances and taxonomic information to be checked (rownames in the first column)
otu_table <-  read.table (otu_file,
                          check.names = FALSE,
                          header = TRUE,
                          dec = ".",
                          sep = "\t",
                          row.names = 1,
                          comment.char = "")

# Clean table from empty lines
otu_table <- otu_table[!apply(is.na(otu_table) | otu_table=="",1,all),]

# Create a dataframe with a number of rows identical to the number of OTUs in the dataset
taxonomy <- otu_table[,dim(otu_table)[2]]

# Test if the taxonomy column is in the correct format (delimited by semicolon)
if(all(grepl("(?:[^;]*;){6}", taxonomy))==FALSE) {

#Send error message if taxonomy is not in the right format
  stop("Wrong number of taxonomic classes\n

Taxonomic levels have to be separated by semicolons (six in total). 
IMPORTANT: if taxonomic information at any level is missing, the semicolons are still needed:\n
       
      e.g.Bacteria;Bacteroidetes;Bacteroidia;Bacteroidales;Prevotellaceae;Prevotella;
      e.g.Bacteria;Bacteroidetes;Bacteroidia;Bacteroidales;Prevotellaceae;;")
} else { 

# Delete the taxonomy row from the OTU-table
otuFile <- otu_table[,c(1:dim(otu_table)[2] - 1)]

# Initialize empty dataframe
taxonomy_new <- NULL

for (i in 1:dim(otu_table)[1]) {
  # Split taxonomic information in its taxonomic classes
  # Kingdom - Phylum - Class - Family - Order - Genus
  splitTax <- strsplit(x = as.character(taxonomy[i]), ";")
  
  # Save the position where the first empty string (sequence of characters) occurs
  value <- which(splitTax[[1]] == "")[1]
  
  # Save the last known taxa information
  lastTaxa = splitTax[[1]][value - 1]
  
  # Replace all empty values by the last taxa information and the prefix "unkown_"
  splitTax <-replace(splitTax[[1]],splitTax[[1]] == "",paste("unknown_",lastTaxa))
 
  # Write new taxonomic information in the dataframe
  taxonomy_new[i] <- list(splitTax)
}

# Adjust dataframe with modified taxonomic information
taxonomy_new <- t(as.data.frame(taxonomy_new))
row.names(taxonomy_new) <- row.names(otuFile)

# Add level information to all taxonomies
# For taxonomies related to kingdom level
taxonomy_new[,1] <- sub("^","k__",taxonomy_new[,1])

# For taxonomies related to phylum level
taxonomy_new[,2] <- sub("^","p__",taxonomy_new[,2])

# For taxonomies related to class level
taxonomy_new[,3] <- sub("^","c__",taxonomy_new[,3])

# For taxonomies related to order level
taxonomy_new[,4] <- sub("^","o__",taxonomy_new[,4])

# For taxonomies related to family level
taxonomy_new[,5] <- sub("^","f__",taxonomy_new[,5])

# For taxonomies related to genus level
taxonomy_new[,6] <- sub("^","g__",taxonomy_new[,6])

#################################################################################

# Create list with taxonomic information for each taxonomy level
class_list <-
  list(
    unique(taxonomy_new[,1]),unique(taxonomy_new[,2]),
    unique(taxonomy_new[,3]), unique(taxonomy_new[,4]),
    unique(taxonomy_new[,5]),unique(taxonomy_new[,6])
  )

# Clone the created list for further processing
sample_list <- class_list
list_length <- NULL

# Iterate through all six taxonomy levels
for (a in 1:6) {
  
  lis <- lapply(class_list[a], lapply, length)
  names(lis)<-lapply(class_list[a],length)
  
  # Individual number of taxonomies for each taxonomic level
  num_taxa <- as.integer(names(lis))
  list_length[a] <- num_taxa
  
  # Iterate through taxonomic class specific taxonomies
  for (b  in 1:num_taxa) {
    
    # Initialize list with the value zero for all taxonomies
    sample_list[[a]][[b]] <- list(rep.int(0,dim(otuFile)[2]))
    
  }
}

#################################################################################
#################################################################################
# Save relative abundances of all samples for each taxonomy

# Iterate through all OTUs
for (i in 1:dim(otu_table)[1]) {
  
  # Iterate through all taxonomic levels
  for (m in 1:6) {
    
    # List of m-th taxonomies of i-th taxonomic levels (e.g. m = Kingdom, i = 4th OTU -> Clostridiales)
    taxa_in_list <- list(taxonomy_new[i,])[[1]][m]
    
    # Record the current position in a list
    position <- which(class_list[[m]] == taxa_in_list)
    
    # All rows with taxonomic information of n-th sample
    matrix <- data.matrix(otuFile)
    sub_sample_tax <-(subset(matrix,taxonomy_new[,m] == taxa_in_list))
    
    # Get the actual value out of the list (initialized with zero)
    temp <- unlist(sample_list[[m]][[position]])
    
    # Calculate the summed up relative abundances for the particular taxonomic class for n-th sample
    temp <- colSums(sub_sample_tax)
    
    # Replace values by new summed values
    sample_list[[m]][[position]] <- list(temp)
    
  }
}

#################################################################################
######                         Write output                                ######
#################################################################################

# Generate tables for each taxonomic class

##Kingdom table
# Create table with taxonomic information (kingdom level)
kingdom <-  matrix(unlist(sample_list[[1]]),nrow = dim(otuFile)[2],ncol = list_length[1],dimnames = list(names(otuFile),unlist(class_list[[1]])))
kingdom <- (t(kingdom))

##Phylum table
# Create table with taxonomic information (phylum level)
phyla <-matrix(unlist(sample_list[[2]]),nrow = dim(otuFile)[2],ncol = list_length[2],dimnames = list(names(otuFile),unlist(class_list[[2]])))
phyla <- (t(phyla))

# Order table according to taxonomic name (descending)
phyla <- phyla[order(row.names(phyla)),]

## Class table
# Create table with taxonomic information (class level)
classes <- matrix(unlist(sample_list[[3]]), nrow = dim(otuFile)[2], ncol = list_length[3], dimnames = list(names(otuFile),unlist(class_list[[3]])))
classes <- (t(classes))

# Order dataframe according to taxonomic name (descending)
classes <- classes[order(row.names(classes)),]

## Orders
# create table with taxonomic information (Order)
orders <-matrix(unlist(sample_list[[4]]),nrow = dim(otuFile)[2],ncol = list_length[4],dimnames = list(names(otuFile),unlist(class_list[[4]])))
orders <- (t(orders))

# Order dataframe according to taxonomic name (descending)
orders <- orders[order(row.names(orders)),]

## Family table
# Create table with taxonomic information (family level)
families <-matrix(unlist(sample_list[[5]]),nrow = dim(otuFile)[2],ncol = list_length[5],dimnames = list(names(otuFile),unlist(class_list[[5]])))
families <- (t(families))

# Order dataframe according to taxonomic name (descending)
families <- families[order(row.names(families)),]

## Genus level
# Create table with taxonomic information (generum level)
genera <- matrix(unlist(sample_list[[6]]),nrow = dim(otuFile)[2],ncol = list_length[6],dimnames = list(names(otuFile),unlist(class_list[[6]])))
genera <- (t(genera))

# Order dataframe according to taxonomic name (descending)
genera <- genera[order(row.names(genera)),]

# Merge all dataframes
tax_summary <-rbind.data.frame(kingdom,phyla,classes,orders,families,genera)

# Identify duplicates and remove them
tax_summary <- tax_summary[!duplicated(row.names(tax_summary)),]

################################################################################
######                        Write Output Files                           ######
#################################################################################

# Create a directory 
dir.create("Taxonomic-Binning")

# Set path for all outputs to the new directory
setwd("Taxonomic-Binning")

# Write output files for taxonomic composition of every sample
write.table(kingdom,"0.Kingdom.all.tab",sep = "\t",col.names = NA)
write.table(phyla,"1.Phyla.all.tab",sep = "\t",col.names = NA)
write.table(classes,"2.Classes.all.tab",sep = "\t",col.names = NA)
write.table(orders,"3.Orders.all.tab",sep = "\t",col.names = NA)
write.table(families,"4.Families.all.tab",sep = "\t",col.names = NA)
write.table(genera,"5.Genera.all.tab",sep = "\t",col.names = NA)
write.table(tax_summary,"tax.summary.all.tab",sep = "\t",col.names = NA)

#################################################################################
######                        Write Graphical Output                       ######
#################################################################################

pdf("taxonomic-overview.pdf")
par(xpd=T, mar=par()$mar+c(0,0,0,9))

#Kingdom
#k_col=distinctColorPalette(dim(kingdom)[1])
k_col=rainbow(dim(kingdom)[1])
k_col=sample(k_col)
barplot(kingdom,col=k_col, cex.names=0.5, ylab="cumulative relative abundance (%)", las=2, main="Taxonomic binning at Kingdom level")
legend(par('usr')[2], par('usr')[4], bty='n',rev(row.names(kingdom)),cex=0.7,col = rev(k_col),pch = 16,pt.cex = 1.2)

#Phyla
#p_col=distinctColorPalette(dim(phyla)[1])
p_col=rainbow(dim(phyla)[1])
p_col=sample(p_col)
barplot(phyla,col=p_col, cex.names=0.5,ylab="cumulative relative abundance (%)",las=2, main="Taxonomic binning at Phyla level")
legend(par('usr')[2], par('usr')[4], bty='n',rev(row.names(phyla)),cex=0.7,col = rev(p_col),pch = 16,pt.cex = 1.2)

#Classes
c_col=rainbow(dim(classes)[1])
c_col=sample(c_col)
barplot(classes,col=c_col, cex.names=0.5,ylab="cumulative relative abundance (%)",las=2, main="Taxonomic binning at Class level")
legend(par('usr')[2], par('usr')[4], bty='n',rev(row.names(classes)),cex=0.7,col = rev(c_col),pch = 16,pt.cex = 1.2)

#Orders
o_col=rainbow(dim(orders)[1])
o_col=sample(o_col)
barplot(orders,col=o_col, cex.names=0.5,ylab="cumulative relative abundance (%)",las=2, main="Taxonomic binning at Order level")
legend(par('usr')[2], par('usr')[4], bty='n',rev(row.names(orders)),cex=0.7,col = rev(o_col),pch = 16,pt.cex = 1.2)

#Families
f_col=rainbow(dim(families)[1])
f_col=sample(f_col)
barplot(families,col=f_col, cex.names=0.5,ylab="cumulative relative abundance (%)",las=2, main="Taxonomic binning at Family level")
legend(par('usr')[2], par('usr')[4], bty='n',rev(row.names(families)),cex=0.7,col = rev(f_col),pch = 16,pt.cex = 1.2)

#Genera
g_col=rainbow(dim(genera)[1])
g_col=sample(g_col)
barplot(genera,col=g_col, cex.names=0.5,ylab="cumulative relative abundance (%)",las=2, main="Taxonomic binning at Genus level")
legend(par('usr')[2], par('usr')[4], bty='n',rev(row.names(genera)),cex=0.7,col = rev(g_col),pch = 16,pt.cex = 1.2)

dev.off()
}

#################################################################################
######                           End of Script                             ######
#################################################################################

```


Finalmente se creo las correlaciones y los estadísticos correspondientes, para ello, se requiere procesar los datos con el siguiente código. 

```{r message=FALSE, warning=FALSE}
setwd("results_16S/")  #<--- CHANGE ACCORDINGLY

#' Please give the name of the file with alpha-diversity measures
alpha <- "alpha-diversity.tab";                         #<--- CHANGE ACCORDINGLY

#' Please give the name of the file with OTUs relative abundance
RelativeAbundanceOTUs <- "OTUs_Table-norm-rel.tab";     #<--- CHANGE ACCORDINGLY

#' Please give the name of the file with relative abundances of different taxonomic levels
TaxanomyAll <- "Taxonomic-Binning/tax.summary.all.tab";                   #<--- CHANGE ACCORDINGLY

#' Please give the name of the meta file with sample groups and additional metadata variables if available
MetaFile <- "metadata2.txt";                         #<--- CHANGE ACCORDINGLY

######                  NO CHANGES ARE NEEDED BELOW THIS LINE               ######

##################################################################################
######                             Main Script                              ######
##################################################################################

###################       Load all required libraries     ########################

# Check if required packages are already installed, and install if missing
packages <-c("compare")

# Function to check whether the package is installed
InsPack <- function(pack)
{
  if ((pack %in% installed.packages()) == FALSE) {
    install.packages(pack,repos ="http://cloud.r-project.org/")
  }
}

# Applying the installation on the list of packages
lapply(packages, InsPack)

# Make the libraries
lib <- lapply(packages, require, character.only = TRUE)

# Check if it was possible to install all required libraries
flag <- all(as.logical(lib))

###################            Read input table              ####################

# Reading Alpha diversity file
alpha <- read.table(file=alpha,header=TRUE,sep="\t",row.names=1,check.names = F)

# Clean table from empty lines
alpha <- alpha[!apply(is.na(alpha) | alpha=="",1,all),]

# Read OTU table
RelativeAbundanceOTUs <-as.data.frame(t(read.table(file=RelativeAbundanceOTUs,header=TRUE,sep="\t",row.names = 1,check.names = F)))

# Clean table from empty lines
RelativeAbundanceOTUs <- RelativeAbundanceOTUs[!apply(is.na(RelativeAbundanceOTUs) | RelativeAbundanceOTUs=="",1,all),]

# Read Mapping file
MetaFile <- read.table(file=MetaFile,header=TRUE,sep="\t",comment.char = "",row.names = 1,check.names = F)

# Clean table from empty lines
MetaFile <- MetaFile[!apply(is.na(MetaFile) | MetaFile=="",1,all),,drop=FALSE]

# Read taxonomy file
TaxanomyAll <- read.table(file=TaxanomyAll,header=TRUE,sep="\t",row.names=NULL,check.names = F)

# Clean table from empty lines
TaxanomyAll <- TaxanomyAll[!apply(is.na(TaxanomyAll) | TaxanomyAll=="",1,all),]

# Select the RelativeAbundanceOTUs and alpha rows based on the mapping file
RelativeAbundanceOTUs <- RelativeAbundanceOTUs[rownames(MetaFile),]
alpha <- alpha[rownames(MetaFile),]

# Prepare TaxanomyAll table
ColnameTo_assign<- TaxanomyAll[,1]
TaxanomyAll[,1] <- NULL
TaxanomyAll <- as.data.frame(t(TaxanomyAll))
TaxanomyAll <- TaxanomyAll[rownames(MetaFile),]
colnames(TaxanomyAll) <- ColnameTo_assign

######################          MAIN PROGRAM                #####################

# Preparing TAXA table:
combine_taxa<- cbind.data.frame(MetaFile[rownames(TaxanomyAll),],alpha[rownames(TaxanomyAll),],TaxanomyAll) # merging Meta+Alpha+Taxa based on same order row.
combine_taxa$SampleID <- row.names(combine_taxa)
combine_taxa<-combine_taxa[,c(ncol(combine_taxa),1:(ncol(combine_taxa)-1))]# replacing columnn at the beginging.

# Prepare OTU table:
combine_OTUs<- cbind.data.frame(MetaFile[rownames(RelativeAbundanceOTUs),],alpha[rownames(RelativeAbundanceOTUs),],RelativeAbundanceOTUs) # merging Meta+Alpha+OTUs based on same order row.
combine_OTUs$SampleID <- row.names(combine_OTUs)
colnames(combine_OTUs)
combine_OTUs<-combine_OTUs[,c(ncol(combine_OTUs),1:(ncol(combine_OTUs)-1))]# replacing columnn at the beginging.

#################################################################################
######                        Write Output Files                           ######
#################################################################################

# Writing tables
if(compareIgnoreOrder(row.names(TaxanomyAll),row.names((MetaFile)))$result &
   compareIgnoreOrder(row.names(alpha),row.names((MetaFile)))$result &
   compareIgnoreOrder(row.names(RelativeAbundanceOTUs),row.names((MetaFile)))$result){
  write.table(combine_taxa,file="TaxaCombined.tab",sep="\t",row.names=FALSE)
  write.table(combine_OTUs,file="OTUsCombined.tab",sep="\t",row.names=FALSE)
  message("Files are combined successfully")
}else{
  stop("ATTENTION !!!! Sample names differ across files. Script aborted.",
       "Please ensure that identical sample names are used.")
}

if(!flag) { stop("
    It was not possible to install all required R libraries properly.
                 Please check the installation of all required libraries manually.\n
                 Required libaries:compare")
}


#################################################################################
######                           End of Script                             ######
#################################################################################
```

Una vez que tenemos los resultados podemos proceder con el siguiente código. 

```{r OTUS_res_16S, message=FALSE, warning=FALSE}
##################################################################################
######             Set parameters in this section manually                  ######
##################################################################################

#' Please set the directory of the present script as the working folder (e.g. D:/studyname/NGS-Data/Rhea/correlation/)
#' Note: the path is denoted by forward slash "/"
setwd("results_16S/")                     #<--- CHANGE ACCORDINGLY !!!

#' Please give the file name of the table containing the variables for analysis
input_file <-"OTUsCombined.tab"              #<--- CHANGE ACCORDINGLY !!!

#' Please give the position where the taxonomic variables (OTUs or taxonomic groups) start!!
#' IMPORTANT: Since the first column in the input file will be used as row names, we do not count it!
otu_variables_start <- 10                                       #<--- CHANGE ACCORDINGLY !!!

#################################################################################
#########         Optional parameters in this section                       #####
#################################################################################
# Unless users follow specific purposes and know exactly what to test, default values are recommended.

# Set the cutoff for significance
# Possible values are any real number between 0 and 1 (default is 0.05)
signf_cutoff <- 0.05

# Calculate correlation among taxonomic variables
# If selected, this will result in many additional tests for correlation among the taxonomic data
# Possible parameters are 1 or 0 (default is 0):
# 1 = calculate correlations within OTUs or taxa
# 0 = NO test within taxonomic variables
includeTax <- 1

# Calculate correlation among meta-variables
# If selected, this will result in many additional tests for correlation among the meta-data
# Possible parameters are 1 or 0 (default is 0):
# 1 = calculate correlations within meta-variables
# 0 = NO test within meta-variables
includeMeta <- 1

# Handling of missing values for meta-variables
# Possible parameters are 1 or 0 (default is 0):
# 1 = missing values are filled with the mean for the corresponding variable
# 0 = NO imputation (replacing missing data with substituted values)
fill_NA <- 0

# Treat zeros in taxonomic variables as missing values
# Possible parameters are 1 or 0 (default is 1):
# 1 = Consider taxonomic zeros as missing values
# 0 = Keep zeros for the calculation of correlations
replace_zeros <- 1

# Set a cutoff for the minimum number of values (prevalence) for a given taxonomic variable to be considered for calculation
# OTUs or taxa with prevalences below the cutoff are excluded from the analysis because considered as non-relevant in the study (very incidental occurancies)
# An OTU is considered present if the value is NOT missing or zero
# This filter reduces the number of tests for poorly supported correlations
# Possible values are any real number between 0 and 1 (default is 0.3, i.e. 30 % of samples must have a value for the given variable)
prevalence_exclusion <- 0.3

# Set a cutoff for the minimal number of pairs observations required for calculation of correlations
# The decision is subjective and depends on the total number of samples
# This filter reduces the number of tests for poorly supported correlations
# Possible values: any positive integer between 0 and the total number of samples (default is 4)
min_pair_support <- 4

# Set a significance cutoff for graphical output
# Only correlations with an uncorrected p-value less than the cuttoff will be plotted
# Possible values: any real number between 0 and 1 (default is 0.05)
plot_pval_cutoff <- 0.05

# Set a correlation coefficient cutoff for graphical output
# Only correlations above the cuttof (absolute value) will be ploted
# Possible values: any real number between 0 and 1 (default is 0.5)
plot_corr_cutoff <- 0.5

######                  NO CHANGES ARE NEEDED BELOW THIS LINE               ######

##################################################################################
######                             Main Script                              ######
##################################################################################

###################       Load all required libraries     ########################

# Check if required packages are already installed, and install if missing
packages <-c("Hmisc","corrplot") 

# Function to check whether the package is installed
InsPack <- function(pack)
{
  if ((pack %in% installed.packages()) == FALSE) {
    install.packages(pack)
  } 
}

# Applying the installation on the list of packages
lapply(packages, InsPack)

# Make the libraries
lib <- lapply(packages, require, character.only = TRUE)

# Check if it was possible to install all required libraries
flag <- all(as.logical(lib))

###################            Read input table              ####################
# Load the tab-delimited file containing the values to be checked (rownames in the first column)
my_data <-
  read.table (
    file = input_file,
    check.names = FALSE,
    header = TRUE,
    dec = ".",
    sep = "\t",
    row.names = 1,
    comment.char = ""
  )
my_data <- my_data[,7:271]
# Clean table from empty lines
my_data <- my_data[!apply(is.na(my_data) | my_data=="",1,all),]
####################            Functions                  #####################

# Function for filling missing values with the mean of the column
fill_NA.mean <- function(vec)
{
  # Calculate mean value of each column (excluding missing values)
  m <- mean(vec, na.rm = TRUE)
  # Replace missing values with mean
  vec[is.na(vec)] <- m
  # Return the new input data frame
  return(vec)
}

# Function to logarithmically normalized OTU values
log_ratio <- function(data)
{
  # Compute the logarithmus
  log_data <- log(data)
  # Calculate exponential function of column-wise mean values for finite log transformed data
  gm <- exp(mean(log_data[is.finite(log_data)]))
  # Compute the logarithmus
  log_gm <- log(gm)
  # Take the difference of both log-transformed datasets
  data <- log_data - log_gm
  # Return the new OTU table
  return(data)
}

##################### END of FUNCTIONS ###########################

######################  MAIN PROGRAM #####################
my_data <- as.data.frame(apply(my_data,2,as.numeric))

first_OTU <- colnames(my_data)[otu_variables_start]

# Split the meta and taxonomic parts of the table
# Choose the continuous scaled variables
my_meta_data <- my_data[1:otu_variables_start - 1]

# Choose the taxonomic variables
my_otu_data <- my_data[otu_variables_start:dim(my_data)[2]]

# Process the meta measurements according to selection
if (fill_NA == 0) {
  # Do not do anything, just rename the file
  my_meta_fixed =  my_meta_data
}

if (fill_NA == 1) {
  # Fill non-zero missing meta-values with the mean of the column (optional)
  # Apply the previously implemented function 'fill_Na.mean' to the meta-data subset
  my_meta_fixed = apply(my_meta_data, 2, fill_NA.mean)
}

# The maximal number of absence
prevalence_cutoff <- dim(my_otu_data)[1] - (prevalence_exclusion*dim(my_otu_data)[1])

# Count how many missing values are found for each OTU 
na_count <-sapply(my_otu_data, function(y) sum(length(which(is.na(y)))))

# Count how many zeros are found for each OTU 
zero_count <-sapply(my_otu_data, function(y) sum(length(which(y==0))))
prevalence_count <- na_count + zero_count

# A new OTU-table is generated, where the number of missing values is below the set cutoff
my_otu_data <- my_otu_data[, prevalence_count <= prevalence_cutoff ]

# If the parameter is set, zeros are replaced with missing values
if (replace_zeros == 1) {
  my_otu_data[my_otu_data==0] <- NA
}

# Replace zeros with 0.0001 to avoid infinite number when calculating logarithmus (log(0)=-INF)
my_otu_data[my_otu_data==0] <- 0.0001

# Transform compositional data by log ratio transformation
my_otu_fixed = apply(my_otu_data, 2, log_ratio)

# Merge the meta- and OTU data in one table
transformed_data <- cbind(my_meta_fixed, my_otu_fixed)

# Centre and scale the values
my_scaled_data <- scale(transformed_data, center = TRUE, scale = TRUE)

# Calculate all pairwise correlations using Pearson correlation method
my_rcorr <- rcorr(as.matrix(my_scaled_data), type = "pearson")

# Generate vector with variable names
var_names <- row.names(my_rcorr$r)

# Depending on which parameters were set at the beginning, one query type is selected
# In each query type, three matrices are generated: p-value matrix, correlation matrix, support matrix
# All possibles pairs are saved in a vector (pairs)
if(includeTax==1 & includeMeta==0){

  # Correlation among OTUs and NO correlation among meta-variables
  row_names <- var_names[c(otu_variables_start:dim(my_rcorr$r)[1])]
  col_names <- var_names
  pairs <-expand.grid(row_names, col_names)
  my_cor_matrix <- my_rcorr$r[c(otu_variables_start:dim(my_rcorr$r)[1]),]
  my_pvl_matrix <-my_rcorr$P[c(otu_variables_start:dim(my_rcorr$P)[1]),]
  my_num_matrix <- my_rcorr$n[c(otu_variables_start:dim(my_rcorr$n)[1]),]
  
  # Set variable for plotting
  diagonale=0
  
} else if(includeTax==1 & includeMeta==1){
  
  # Correlation among OTUs and correlation among meta-variables
    row_names <-var_names
  col_names <- var_names
  pairs <-expand.grid(row_names, col_names)
  my_cor_matrix <- my_rcorr$r
  my_pvl_matrix <-my_rcorr$P
  my_num_matrix <- my_rcorr$n
  # Set variable for plotting
  diagonale=0
  
} else if (includeTax==0 & includeMeta==1) {
  # NO correlation among OTUs and correlation among meta-variables
  
  row_names <-var_names[c(1:(otu_variables_start - 1))]
  col_names <- var_names
  pairs <-expand.grid(row_names, col_names)
  my_cor_matrix <- my_rcorr$r[c(1:(otu_variables_start - 1)),]
  my_pvl_matrix <-my_rcorr$P[c(1:(otu_variables_start - 1)),]
  my_num_matrix <- my_rcorr$n[c(1:(otu_variables_start - 1)),]
  # Set variable for plotting
  diagonale=1
  
} else {
  # NO correlation among OTUs and NO correlation among meta-variables
  
  row_names <- var_names[c(1:(otu_variables_start - 1))]
  col_names <- var_names[otu_variables_start:dim(my_rcorr$r)[1]]
  pairs <-expand.grid(row_names, col_names)
  my_cor_matrix <- my_rcorr$r[c(1:(otu_variables_start - 1)),c(otu_variables_start:dim(my_rcorr$r)[1])]
  my_pvl_matrix <-my_rcorr$P[c(1:(otu_variables_start - 1)),c(otu_variables_start:dim(my_rcorr$P)[1])]
  my_num_matrix <- my_rcorr$n[c(1:(otu_variables_start - 1)),c(otu_variables_start:dim(my_rcorr$n)[1])]
  # Set variable for plotting
  diagonale=1
  
}

# Select the corresponding p-value for each pair 
p_vector <- as.vector(my_pvl_matrix)

# Select the corresponding correlation coefficient for each pair 
c_vector <- as.vector(my_cor_matrix)

# Select the corresponding number of observations for each pair 
n_vector <- as.vector(my_num_matrix)

# Generate matrix with the pairwise comparisons
my_pairs <-
  matrix(ncol = 5,
         c(
           as.character(pairs[, 2]),
           as.character(pairs[, 1]),
           c_vector,
           p_vector,
           n_vector
         ))

# Delete all pairs with insufficient number of pairs
my_pairs <- subset(my_pairs, as.numeric(my_pairs[,5]) > min_pair_support)

# Adjust p-value for multiple testing using the Benjamin-Hochberg method
pVal_BH <- round(p.adjust(my_pairs[,4], method = "BH"), 4)

# Add the corrected p-value in the table 
my_pairs <- cbind(my_pairs,as.numeric(pVal_BH))

# Remove similar pairs (values along the diagonal)
my_pairs <- my_pairs[!as.character(my_pairs[, 1]) == as.character(my_pairs[, 2]),]

# Remove duplicate pairs
my_pairs <- my_pairs[!duplicated(my_pairs[, 3]), ]

# Created matrix columns represent correlation coefficients, p-values, number of observations, and corrected p-values
# Rows represent the pairs
matrix_names <- list(c(rep("",times=dim(my_pairs)[1])),
                     c(
                       "variable1",
                       "variable2",
                       "correlation",
                       "pValue",
                       "support",
                       "Corrected"
                     ))

dimnames(my_pairs) <- matrix_names

# Create subset of pairs with significant p-values
my_pairs_cutoff <- my_pairs[as.numeric(my_pairs[, 4]) <= signf_cutoff, ]

# Convert to matrix
my_pairs_cutoff <- matrix(my_pairs_cutoff,ncol=6,dimnames = list(c(rep("",times=dim(my_pairs_cutoff)[1])),c("variable1","variable2","correlation","pvalue","support","corrected pvalue")))

# Create subset of significant pairs with strong correlation (above 0.5)
my_pairs_cutoff_corr <- my_pairs_cutoff[abs(as.numeric(my_pairs_cutoff[, 3])) >= 0.5, ]

# Remove columns containing no information
my_cor_matrix <- my_cor_matrix[, colSums(is.na(my_cor_matrix)) != nrow(my_cor_matrix)]

# Missing values in the correlation matrix are set to zero
my_cor_matrix[is.na(my_cor_matrix)] <- 0
#################################################################################
######                        Generate Graphs                              ######
#################################################################################
# Take current path in one variable to store results in seperate folders in further steps
OriginalPath <- getwd()

# Take the name of the inputfile to name the folder
prefix = paste(strsplit(input_file,"[.]")[[1]][1],sep="_")

# Make a directory name with inputfile name and date
newdir <- paste(prefix,Sys.Date(), sep = "_")

# Create a directory 
dir.create(newdir)

# Set path for all outputs to the new directory
setwd(newdir)
                    
# Save visualized correlation matrix in "corrplot.pdf"

# Check if significance value for graphical output was modified by the user
if (plot_pval_cutoff != signf_cutoff | plot_corr_cutoff != 0.5) {
  # Generate a new matrix with the signficance cutoff 
  my_pairs_cutoff <- my_pairs[as.numeric(my_pairs[, 4]) <= plot_pval_cutoff, ]
  
  # Extract all significant pairs with the set correlation cutoff
  corr_pval_cutoff <- my_pairs_cutoff[abs(as.numeric(my_pairs_cutoff[, 3])) >= plot_corr_cutoff, ]
  corr_pval_cutoff <- matrix(corr_pval_cutoff,ncol=6, dimnames=list(c(rep("",times=dim(corr_pval_cutoff)[1])),c("variable1","variable2","correlation","pvalue","support","corrected pvalue")))
} else  {
  # If the significance cutoff is 0.05 an the correlation cutoff is 0.5 (for plotting)
  # Take the previously generated matrix
  corr_pval_cutoff <- my_pairs_cutoff_corr
  corr_pval_cutoff <- matrix(corr_pval_cutoff,ncol=6, dimnames=list(c(rep("",times=dim(corr_pval_cutoff)[1])),c("variable1","variable2","correlation","pvalue","support","corrected pvalue")))
 }

# Save linearized transformed correlations of significant pairs in "linear_sign_pairs.pdf"
pdf(file = "linear_sign_pairs.pdf",family="sans",fillOddEven=TRUE)

# Iterate through all significant pairs
for (i in 1:dim(corr_pval_cutoff)[1]) {
  # Save log-scaled transformed values of the first variable of the pair
  x_df <- transformed_data[names(transformed_data) %in% corr_pval_cutoff[i, 2]]
  
  # Save as numerical vector
  x <- x_df[, 1]
  
  # Save log-scaled transformed values of the second variable of the pair
  y_df <- transformed_data[names(transformed_data) %in% corr_pval_cutoff[i, 1]]
  
  # Save as numerical vector
  y <- y_df[, 1]
  
  # Create a linear model for the pair (excluding missing values)
  clm <- lm(y ~ x, na.action = na.exclude)
  
  # Determine the number of steps for the generation of the confidence intervals
  steps <- (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))/1000
  
  # Sequence of more finely/evenly spaced data than original one for the calculation of the confidence interval
  newx <- seq(min(x, na.rm = TRUE), max(x, na.rm = TRUE), steps)
  
  # Predict confidence interval based on the generated linear model
  # Prediction intervals are calculated based on the residuals of the regression equation 
  # Prediction intervals account for the variability around the mean response inherent in any prediction
  # It represents the range where a single new observation is likely to fall
  a <- predict(clm, newdata = data.frame(x = newx), interval = "confidence")
  
  # Graphic display of all log-scaled transformed values of the ith-pair
  plot(
    x,
    y,
    ylab="",
    xlab="", 
    cex.axis = 0.75 ,
    xaxt = 'n',
    yaxt = 'n',
    xlim=c(min(x,na.rm = TRUE),max(x, na.rm = TRUE)),
   xaxs="i"
  )
  title(xlab = names(x_df),line=0.5,font.lab=2,cex.lab=1.4)
  title(ylab = names(y_df),line=0.5,font.lab=2,cex.lab=1.4)
  
  # Draw the confidence interval around the fitted line
  polygon(c(newx,rev(newx)),c(a[,2],rev(a[,3])),col="grey91",border=TRUE, lty="dashed")
  
  # Samples are shown as dots
  points(x,y)
  
  # Draw linear regression line
  abline(clm, lwd=2)
  
  # Take calculated pairwise p-value from rcorr
  pvalue_text <- paste("P-value:", round(as.numeric(corr_pval_cutoff[i, 4]), 4), sep = "")
  
  # Take corrected pairwise p-value
  pvalue_corr_text <- paste("Adj. p-value:", as.numeric(corr_pval_cutoff[i, 6]), sep = "")
  
  # Take calculated pairwise correlation coefficient from rcorr
  corr_text <- paste("Pearson's r:", round(as.numeric(corr_pval_cutoff[i, 3]), 4), sep = "")
  
  # Take calculated pairwise number of observations from rcorr
  support_text <- paste("supported by ", round(as.numeric(corr_pval_cutoff[i, 5]), 4)," observations", sep = "")
  
  # Show correlation coefficient and p-value in the plot
  mtext(corr_text, side = 3, line = 2)
  mtext(pvalue_text, side = 3, line = 1)
  mtext(pvalue_corr_text, side = 3, line = 0)
  mtext(support_text, side = 1, line = 2)
  
  abline(par("usr")[3],0)
  segments(par("usr")[1],a[1,2],par("usr")[1],a[1,3])
  abline(par("usr")[4],0)
  
}

dev.off()

#################################################################################
######                        Write Output Files                           ######
#################################################################################
# Take current path in one variable to store results in seperate folders in further steps
OriginalPath <- getwd()

# Take the name of the inputfile to name the folder
prefix = paste(strsplit(input_file,"[.]")[[1]][1],sep="_")

# Make a directory name with inputfile name and date
newdir <- paste(prefix,Sys.Date(), sep = "_")

# Create a directory 
dir.create(newdir)

# Set path for all outputs to the new directory
setwd(newdir)
                    
# Write the log-scale transformed table
write.table(my_scaled_data,"transformed.tab",sep = "\t",col.names = NA,quote = FALSE)

# Write the correlation table
write.table(my_cor_matrix,"correlation-table.tab",sep = "\t",col.names = NA,quote = FALSE)

# Write the pvalue table
write.table(my_pvl_matrix,"pval-table.tab",sep = "\t",col.names = NA,quote = FALSE)

# Write the number of effective samples table
write.table(my_num_matrix,"support-table.tab",sep = "\t",col.names = NA,quote = FALSE)

# Write the significant correlations
write.table(my_pairs_cutoff,"cutoff-pairs-corr-sign.tab",sep = "\t",col.names = NA,quote = FALSE)

# Write plotted pairs
write.table(corr_pval_cutoff,"plotted-pairs-stat.tab",sep = "\t",col.names = NA,quote = FALSE)


if(!flag) { stop("
    It was not possible to install all required R libraries properly.
                 Please check the installation of all required libraries manually.\n
                 Required libaries:ade4, GUniFrac, phangorn, randomcoloR, Rcpp")
}


#################################################################################
######                           End of Script                             ######
#################################################################################

```



# Metagenomic ITS analysis 

Primero se creo un archivo tsv con el metadata de los datos en cuestión. Este metadata para los datos de 16S se ve de la siguiente manera. 

```{r,echo=FALSE}
setwd("~/Endogenomiks/Projects/January/Analysis_results/")
```

```{r}
#Reading metadata table1
metadata <- read.table("results_ITS/metadata.tsv", row.names = 1, header = TRUE, sep = "\t", stringsAsFactors = TRUE)
metadata #Print metadata 
```


Después de este proceso se uso bash, para agregar la cantidad original de reads al archivo metadata. 

```{bash, eval=F}
cd {Directorio del analysis de ITS}
head -n 1 metadata.tsv | tr -d "\n" > header.txt #Extraer headers
echo -e "\tReads" >> header.txt # Agregar reads
grep L001 metadata.tsv  | sort -k1,1  > file1 # Extraer y ordenar datos
awk -F $'\t' '{system("zcat " $1"_R1.fastq.gz | echo $((`wc -l`/4))"  )}' file1 >> reads #Contar reads
cat header.txt <(paste file1 reads) >> metadata2.txt # Agregar header, con el número de reads y lo demas.
rm file1 header.txt reads # remover lo archivos creados en el proceso
```

A continuación se muestra el resultado de la tabla nueva de metadata. 

```{r}
#Reading metadata table2
metadata <- read.table("results_ITS/metadata2.txt", row.names = 1, header = TRUE, sep = "\t", stringsAsFactors = TRUE)
metadata #Print metadata2
```

### Análisis de calidad

Para analizar la calidad de las muestras, se procesaron los lecturas mediante el uso de fastqc. para ello, se creo un script (fastqc.sh) que posteriormente se corrió. Los resultados se encuentran en la carpeta FASTQC, con su correspondiente nombre. Para ello, se utilizó el siguiente código. 

```{bash, eval=F}
mkdir FASTQC
for i in *.fastq.gz ; do echo " fastqc $i -o FASTQC" >> fastqc.sh ; done
. fastqc.sh

```

Finalmente se utilizó multiQC, para reunir los resultados de calidad en un solo archivo. Los cuáles pueden consultarse en el archivo html de la carpeta MULTIQC. Para ello, se utilizó el siguiente código.

```{bash, eval=F}
multiqc FASTQC -o MULTIQC
```


### Asignación taxonómica de las lecturas de secuenciación mediante FROGS.

Para llevar a cabo la asignación taxonómica de las lecturas de secuenciación, se utilizó la herramienta frogs. 
Primero se usará el programa de preprocesamiento de muestras, tenemos lecturas con un tamaño promedio de 250. ITS tiene un máximo de amplicón de 20 y mínimo de 490 ( parámetros recomendados por FROGS para el análisis de ITS). Se utilizará vsearch para mantener las secuencias separadas debido a su rapidez, comparado con pear. Los outputs del presente pipeline se encuentran en una carpeta nombrada frogs e incluyen para esta parte: 

+ *preprocess.fasta *: Archivo con el fasta de las secuencias preprocesadas y unidas. 
+ *preprocess.tsv*: Archivo tsv con la descripción de cada secuencia y a qué grupo pertenecen.
+ *preprocess.html*: Archivo html con los resultados de manera gráfica. 
+ *preprocess.log*: Archivo log del procesamiento.

El código necesario para procesarlo es el siguiente:

```{bash, eval=F}
#Primero se comprimieron los archivos de .fastq.gz para que sea más sencillo el procesamiento
tar zcvf data.tar.gz *.fastq.gz

#Es importante ver los estadísticas de las lecturas y lo que esperamos de ITS 
preprocess.py illumina --input-archive data.tar.gz --keep-unmerged --merge-software vsearch --min-amplicon-size 20 --max-amplicon-size 490 --without-primers --R1-size 250 --R2-size 250 --nb-cpus 5 --output-dereplicated frogs/preprocess.fasta --output-count frogs/preprocess.tsv --summary frogs/preprocess.html --log-file frogs/preprocess.log


```

En el siguiente paso de nuestro flujo de trabajo requerimos llevar a cabo la clusterización de los datos analizados, para con ello, reconocer cuáles lecturas pertenecen al mismo organismo. Los outputs de este paso son: 

+ *clustering.fasta *: Archivo con el fasta de las secuencias por cluster. 
+ *clustering.biom*: Archivo Biological Observation Matrix (.biom) posee una tabla de contingencia de las muestras contra los clusters. 
+ *clustering_compo.tsv*: Archivo tsv con la composición de los clusters y la muestra a la que pertenecen.
+ *clustering_log.txt*: Archivo log del procesamiento.

Posteriormente se proceso un resumen de los estadísticos del proceso de clusterización, donde obtenemos los siguientes archivos: 

+ *clusters_stats.html*: Archivo html con los resultados del estadístico de la clusterización de manera gráfica.
+ *clusters_stats.log*: Archivo log del proceso de clusterización.

El código necesario para procesarlo es el siguiente:

```{bash, eval=F}
#Posteriormente clusterizamos los resultados 
clustering.py --nb-cpus 5 --input-fasta frogs/preprocess.fasta --input-count frogs/preprocess.tsv --output-biom frogs/clustering.biom --output-fasta frogs/clustering.fasta --output-compo frogs/clustering_compo.tsv --log-file frogs/clustering_log.txt --distance 1 --fastidious

#To check the stats
clusters_stat.py --input-biom frogs/clustering.biom --output-file frogs/clusters_stats.html --log-file frogs/clusters_stats.log 

```

En el siguiente paso de nuestro flujo de trabajo se utilizará la herramienta apropiada para la remoción de quimeras. Los outputs de este paso son: 

+ *remove_chimera.fasta *: Archivo con el fasta de las secuencias que no poseen quimeras.  
+ *remove_chimera.biom*: Matriz de contingencia de las muestras contra los clusters después de la remover quimeras. 
+ *remove_chimera.html*: Archivo html con los resultados después de remover las quimeras de manera gráfica.
+ *remove_chimera.log*: Archivo log del proceso para remover de quimeras.

El código necesario para procesarlo es el siguiente:

```{bash, eval=F}
remove_chimera.py --input-fasta frogs/clustering.fasta --input-biom frogs/clustering.biom --non-chimera frogs/remove_chimera.fasta --nb-cpus 5 --log-file frogs/remove_chimera.log --out-abundance frogs/remove_chimera.biom --summary frogs/remove_chimera.html
```

A continuación se requiere remover los OTUS que poseen quimeras,contaminantes o artefactos. Los outputs de este paso son: 

+ *filters.fasta *: Archivo con el fasta de las secuencias filtradas.  
+ *filters.biom*: Matriz de contingencia de las muestras contra los clusters filtrados. 
+ *filters.html*: Archivo html con los resultados del proceso de filtrado de manera gráfica.
+ *filters.log*: Archivo log del proceso de filtrado.
+ *filters_excluded.tsv*: Archivo tsv que tiene los clusters que fueron excluidos por el filtro. 

El código necesario para procesarlo es el siguiente, donde se utilizó además un filtro por presencia (mínimo en una muestra) y por abudancia  (que se encuentre en el 0.00005% de las lecturas):

```{bash, eval=F}
otu_filters.py --input-fasta frogs/remove_chimera.fasta --input-biom frogs/remove_chimera.biom --output-fasta frogs/filters.fasta --nb-cpus 5 --log-file frogs/filters.log --output-biom frogs/filters.biom --summary frogs/filters.html --excluded frogs/filters_excluded.tsv --min-sample-presence 1 --min-abundance 0.00005
```


Previo a la asignación taxónomica, se requiere el procesado de las lecturas mediante el uso de ITSX filter. Los outputs de este paso son: 

+ *its1.fasta *: Archivo con el fasta filtrado de ITS1.  
+ *its1.biom*: Matriz de contingencia de las muestras contra los clusters filtrados por ITSx. 
+ *its1.html*: Archivo html con los resultados del proceso de filtrado por ITSx de manera gráfica.
+ *its1.log*: Archivo log del proceso de filtrado por ITSx.

El código necesario para procesarlo:

```{bash,eval=F}
itsx.py --nb-cpus 5 --region ITS1 --check-its-only --input-fasta frogs/filters.fasta --input-biom frogs/filters.biom --out-fasta frogs/its1.fasta --out-abundance frogs/its1.biom --summary frogs/its1.html --log-file frogs/its1.log
```

A continuación, se llevó a cabo la asignación taxonómica de las lecturas, para ello se utilizaron dos bases de datos de referencia diferentes, una corresponde a fungi y otra a eucariontes. Los archivos de salida de este paso son: 

+ *euca_frogs.biom*: Matriz de contingencia de las muestras contra  los resultados de la asignación taxonómica para la base de datos de Eucariontes. 
+ *euca_affiliation.html*: Archivo html con los resultados de asignación de manera gráfica para la base de datos de Eucariontes.
+ *euca_affiliation.log*: Archivo log del proceso de asignación taxonómica para la base de datos de Eucariontes.

+ *fungi_frogs.biom*: Matriz de contingencia de las muestras contra  los resultados de la asignación taxonómica para la base de datos de fungi. 
+ *fungi_affiliation.html*: Archivo html con los resultados de asignación de manera gráficapara la base de datos de fungi.
+ *fungi_affiliation.log*: Archivo log del proceso de asignación taxonómica para la base de datos de fungi. 

El código necesario para procesarlo es el siguiente, donde es importante notar que primero se tiene que descargar la base de datos apropiada para ello (se incluye dicho paso en el código):


```{bash, eval=F}
#Para descargar la base de datos 
mkdir database_ITS
cd database_ITS
wget http://genoweb.toulouse.inra.fr/frogs_databanks/assignation/Unite/Unite_Euka_8.3_20210510.tar.gz
tar -xvf Unite_Euka_8.3_20210510.tar.gz
wget http://genoweb.toulouse.inra.fr/frogs_databanks/assignation/Unite/Unite_Fungi_8.3_20210510.tar.gz
tar -xvf Unite_Fungi_8.3_20210510.tar.gz
cd ..

#Para procesar los datos 
affiliation_OTU.py --nb-cpus 5 --reference database_ITS/Unite_Euka*_8.3_20210510*/Unite_Euka_8.3_20210510.fasta --input-biom frogs/itsx.biom --input-fasta frogs/itsx.fasta --output-biom frogs/euca_frogs.biom --summary frogs/euca_affiliation.html -l frogs/euca_affiliation.log

affiliation_OTU.py --nb-cpus 5 --reference database_ITS/Unite_Fungi*/Unite_Fungi_8.3_20210510.fasta --input-biom frogs/itsx.biom --input-fasta frogs/itsx.fasta --output-biom frogs/fungi_frogs.biom --summary frogs/fungi_affiliation.html -l frogs/fungi_affiliation.log
```

Una vez que hemos obtenido los resultados de la asignación taxonómica, podemos filtrar aún más los resultados para conservar aquellos cuya especie se encuentre reportada en la base de datos de NCBI y que posean características de identidad de secuencía y covertura deseable para una correcta asignación.  Para ello se utilizó la herramienta affiliation_filters, cuyos archivos de salida son:

Para la base de datos de fungi: 

+ *fungi_frogs__affi_filter.biom*: Matriz de contingencia de las muestras contra los resultados de la asignación taxonómica filtrados. 
+*fungi_frogs_affi_filter.fasta*: Archivo fasta de las secuencias filtradas correspondientes a los clusters conservados.  
+ *fungi_frogs_affi_filter.html*: Archivo html con los resultados de asignación filtrados de manera gráfica.
+ *fungi_affi_filter.log*: Archivo log del proceso de filtrado de las asignaciones taxonómicas.
+*fungi_frogs_affi_filter_impacted_OTU.tsv*: El archivo TSV multihit asociado a la OTU impactada.

Para la base de datos de eucariontes: 

+ *euca_frogs__affi_filter.biom*: Matriz de contingencia de las muestras contra los resultados de la asignación taxonómica filtrados. 
+*euca_frogs_affi_filter.fasta*: Archivo fasta de las secuencias filtradas correspondientes a los clusters conservados.  
+ *euca_frogs_affi_filter.html*: Archivo html con los resultados de asignación filtrados de manera gráfica.
+ *euca_affi_filter.log*: Archivo log del proceso de filtrado de las asignaciones taxonómicas.
+*euca_frogs_affi_filter_impacted_OTU_multihit.tsv*: El archivo TSV multihit asociado a la OTU impactada.

El código necesario para procesarlo es el siguiente:

```{bash,eval=F}
#Para fungi
affiliation_filters.py --input-biom frogs/fungi_frogs.biom --input-fasta frogs/itsx.fasta --output-biom frogs/fungi_frogs__affi_filter.biom --output-fasta frogs/fungi_frogs_affi_filter.fasta --summary frogs/fungi_frogs_affi_filter.html --impacted-multihit frogs/fungi_frogs_affi_filter_impacted_OTU_multihit.tsv --log-file frogs/fungi_affi_filter.log --delete --min-blast-identity 0.8 --min-blast-coverage 0.8 --ignore-blast-taxa "unknown species"

#Para eucariontes
affiliation_filters.py --input-biom frogs/euca_frogs.biom --input-fasta frogs/itsx.fasta --output-biom frogs/euca_frogs__affi_filter.biom --output-fasta frogs/euca_frogs_affi_filter.fasta --summary frogs/euca_frogs_affi_filter.html --impacted-multihit frogs/euca_frogs_affi_filter_impacted_OTU_multihit.tsv --log-file frogs/euca_affi_filter.log --delete --min-blast-identity 0.8 --min-blast-coverage 0.8 --ignore-blast-taxa "unknown species"
```

Finalmente, se puede generar un resumen de los resultados de la afiliación. Los archivos de salida son los siguientes: 

Para la base de datos de fungi: 

+ *fungi_affiliations_stats.html*: Archivo html con el resumen de asignación de manera gráfica.
+ *fungi_affiliations_stats.log*: Archivo log del proceso de filtrado de las asignaciones taxonómicas.

Para la base de datos de eucariontes: 

+ *euca_affiliations_stats.html*: Archivo html con el resumen de asignación de manera gráfica.
+ *euca_affiliations_stats.log*: Archivo log del proceso de filtrado de las asignaciones taxonómicas.

El código necesario para procesarlo es el siguiente:

```{bash, eval=F}
affiliations_stat.py --input-biom frogs/fungi_frogs__affi_filter.biom --output-file frogs/fungi_affiliations_stats.html --log-file frogs/fungi_affiliations_stats.log --multiple-tag blast_affiliations --tax-consensus-tag blast_taxonomy --identity-tag perc_identity --coverage-tag perc_query_coverage

affiliations_stat.py --input-biom frogs/euca_frogs__affi_filter.biom --output-file frogs/euca_affiliations_stats.html --log-file frogs/euca_affiliations_stats.log --multiple-tag blast_affiliations --tax-consensus-tag blast_taxonomy --identity-tag perc_identity --coverage-tag perc_query_coverage
```

Finalmente, se generará el archivo correpondiente al tsv de las asignaciones taxonómicas. Los archivos de salida son los siguientes: 

Para la base de datos de fungi: 

+ *fungi_affiliation_final_res.tsv*: Este archivo de salida contendrá la abundancia y los metadatos
+ *fungi_multi_aff_final_res.tsv*: Este archivo de salida contendrá información sobre alineaciones múltiples.
+ *fungi_biom_to_tsv_final_res.log*: Archivo log del proceso.

Para la base de datos de eucariontes: 

+ *euca_affiliation_final_res.tsv*: Este archivo de salida contendrá la abundancia y los metadatos
+ *euca_multi_aff_final_res.tsv*: Este archivo de salida contendrá información sobre alineaciones múltiples.
+ *euca_biom_to_tsv_final_res.log*: Archivo log del proceso.

El código necesario para procesarlo es el siguiente:

```{bash,eval=F}
biom_to_tsv.py --input-biom frogs/fungi_frogs__affi_filter.biom --input-fasta frogs/fungi_frogs_affi_filter.fasta --output-tsv frogs/fungi_affiliation_final_res.tsv --output-multi-affi frogs/fungi_multi_aff_final_res.tsv --log-file frogs/fungi_biom_to_tsv_final_res.log

biom_to_tsv.py --input-biom frogs/euca_frogs__affi_filter.biom --input-fasta frogs/euca_frogs_affi_filter.fasta --output-tsv frogs/euca_affiliation_final_res.tsv --output-multi-affi frogs/euca_multi_aff_final_res.tsv --log-file frogs/euca_biom_to_tsv_final_res.log
```

A manera de resumen, los archivos html poseen los resultados gráficos de cada parte del proceso. Los archivos finales para la base de datos de fungi son **fungi_affiliation_final_res.tsv**, **fungi_frogs_affi_filter.fasta** y **fungi_frogs__affi_filter.biom** que contienen los resultados de la asignación taxonómica. Para la base de datos de eucariontes, los archivos finales son  **euca_multi_aff_final_res.tsv**, **euca_frogs_affi_filter.fasta** y **euca_frogs__affi_filter.biom**.

Para visualizar los resultados, se puede utilizar los siguientes datos. 

#### Para la base de datos de hongos 

```{r}
library(phyloseq)
library(phyloseq.extended)
setwd("/Users/pablo/Endogenomiks/Projects/January/Analysis_results/")
biomfile <- "results_ITS/frogs/fungi_frogs__affi_filter.biom"
frogs <- import_frogs(biomfile, taxMethod = "blast")
metadata <- read.table("results_ITS/metadata2.txt", row.names = 1, header = TRUE, sep = "\t", stringsAsFactors = FALSE)
sample_data(frogs) <- metadata

samples <- rownames(sample_data(frogs))
final <- sample_sums(frogs)
initial <- metadata$Reads
final <- as.vector(t(final))
df <- data.frame(initial,final,samples)
library(reshape2)
library(ggplot2)
df <- melt(df, id.vars='samples')
ggplot(df, aes(x=samples, y=value, fill=variable)) + 
       geom_bar(stat='identity', position='dodge') +scale_fill_discrete(name = "Lecturas", labels = c("Lecturas iniciales", "Lecturas finales"))+theme(axis.text.x = element_text(angle=90))+  xlab("Muestras") + ylab("Lecturas")
```

```{r}
phy_obj0 = subset_taxa(frogs, Species != "Multi-affiliation")
phy_obj0 = subset_taxa(phy_obj0, Genus != "g__unidentified")

plot_bar(phy_obj0, fill="Genus") + facet_wrap(~EnvType, scales= "free_x", nrow=1)
plot_composition(phy_obj0, taxaRank1 = NULL, taxaSet1 = NULL, taxaRank2 = "Genus",numberOfTaxa = 10,x="Treatment") + 
  scale_fill_manual(values =c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F", "#CCFCCC", "#CCCCCC", "#FFFFCC", "#CC79A7","#8FD744FF","#EF9A9A","#AED581", "#009E73","#CCFCCC")) +
  facet_grid(~EnvType, scales = "free_x", space = "free_x")

#Save in a pdf 
pdf("/Users/pablo/Endogenomiks/Projects/January/Analysis_results/results_ITS/final_graph/fungi_affiliation_graphs_frogs.pdf")

## By genus
#Bar_plot
plot_bar(phy_obj0, fill="Genus",title="Abundance in reads by genus") + facet_wrap(~EnvType, scales= "free_x", nrow=1)
#Composition plot
plot_composition(phy_obj0, taxaRank1 = NULL, taxaSet1 = NULL, taxaRank2 = "Genus",numberOfTaxa = 10,x="Treatment") + 
  scale_fill_manual(values =c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F", "#CCFCCC", "#CCCCCC", "#FFFFCC", "#CC79A7","#8FD744FF","#EF9A9A","#AED581", "#009E73","#CCFCCC")) +
  facet_grid(~EnvType, scales = "free_x", space = "free_x")+
  ggtitle("Relative abundance by genus (Most abundant 10)")
#By phylum 
plot_bar(phy_obj0, fill="Phylum",title="Abundance in reads by Phylum") + facet_wrap(~EnvType, scales= "free_x", nrow=1)
plot_composition(phy_obj0, taxaRank1 = NULL, taxaSet1 = NULL, taxaRank2 = "Phylum",numberOfTaxa = 10,x="Treatment") + 
  scale_fill_manual(values =c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F", "#CCFCCC", "#CCCCCC", "#FFFFCC", "#CC79A7","#8FD744FF","#EF9A9A","#AED581", "#009E73","#CCFCCC")) +
  facet_grid(~EnvType, scales = "free_x", space = "free_x")+
  ggtitle("Relative abundance by Phylum (Most abundant 10)")

#By family 
plot_bar(phy_obj0, fill="Family",title="Abundance in reads by Family") + facet_wrap(~EnvType, scales= "free_x", nrow=1)
plot_composition(phy_obj0, taxaRank1 = NULL, taxaSet1 = NULL, taxaRank2 = "Family",numberOfTaxa = 10,x="Treatment") + 
  scale_fill_manual(values =c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F", "#CCFCCC", "#CCCCCC", "#FFFFCC", "#CC79A7","#8FD744FF","#EF9A9A","#AED581", "#009E73","#CCFCCC")) +
  facet_grid(~EnvType, scales = "free_x", space = "free_x")+
  ggtitle("Relative abundance by Family (Most abundant 10)")

#By kingdom 
plot_bar(phy_obj0, fill="Kingdom",title="Abundance in reads by Kingdom") + facet_wrap(~EnvType, scales= "free_x", nrow=1)
plot_composition(phy_obj0, taxaRank1 = NULL, taxaSet1 = NULL, taxaRank2 = "Kingdom",numberOfTaxa = 10,x="Treatment") + 
  scale_fill_manual(values =c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F", "#CCFCCC", "#CCCCCC", "#FFFFCC", "#CC79A7","#8FD744FF","#EF9A9A","#AED581", "#009E73","#CCFCCC")) +
  facet_grid(~EnvType, scales = "free_x", space = "free_x")+
  ggtitle("Relative abundance by Kingdom (Most abundant 10)")

dev.off()

```

#### Para la base de datos de eucariontes 

```{r}
library(phyloseq)
library(phyloseq.extended)
setwd("/Users/pablo/Endogenomiks/Projects/January/Analysis_results/")
biomfile <- "results_ITS/frogs/euca_frogs__affi_filter.biom"
frogs <- import_frogs(biomfile, taxMethod = "blast")
metadata <- read.table("results_ITS/metadata2.txt", row.names = 1, header = TRUE, sep = "\t", stringsAsFactors = FALSE)
sample_data(frogs) <- metadata

samples <- rownames(sample_data(frogs))
final <- sample_sums(frogs)
initial <- metadata$Reads
final <- as.vector(t(final))
df <- data.frame(initial,final,samples)
library(reshape2)
library(ggplot2)
df <- melt(df, id.vars='samples')
ggplot(df, aes(x=samples, y=value, fill=variable)) + 
       geom_bar(stat='identity', position='dodge') +scale_fill_discrete(name = "Lecturas", labels = c("Lecturas iniciales", "Lecturas finales"))+theme(axis.text.x = element_text(angle=90))+  xlab("Muestras") + ylab("Lecturas")
```

```{r}
phy_obj0 = subset_taxa(frogs, Genus != "Multi-affiliation")
phy_obj0 = subset_taxa(phy_obj0, Genus != "unknown genus")

plot_bar(phy_obj0, fill="Genus") + facet_wrap(~EnvType, scales= "free_x", nrow=1)
plot_composition(phy_obj0, taxaRank1 = NULL, taxaSet1 = NULL, taxaRank2 = "Genus",numberOfTaxa = 10,x="Treatment") + 
  scale_fill_manual(values =c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F", "#CCFCCC", "#CCCCCC", "#FFFFCC", "#CC79A7","#8FD744FF","#EF9A9A","#AED581", "#009E73","#CCFCCC")) +
  facet_grid(~EnvType, scales = "free_x", space = "free_x")

#Save in a pdf 
pdf("/Users/pablo/Endogenomiks/Projects/January/Analysis_results/results_ITS/final_graph/eucaria_graphs_frogs.pdf")

## By genus
#Bar_plot
plot_bar(phy_obj0, fill="Genus",title="Abundance in reads by genus") + facet_wrap(~EnvType, scales= "free_x", nrow=1)
#Composition plot
plot_composition(phy_obj0, taxaRank1 = NULL, taxaSet1 = NULL, taxaRank2 = "Genus",numberOfTaxa = 10,x="Treatment") + 
  scale_fill_manual(values =c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F", "#CCFCCC", "#CCCCCC", "#FFFFCC", "#CC79A7","#8FD744FF","#EF9A9A","#AED581", "#009E73","#CCFCCC")) +
  facet_grid(~EnvType, scales = "free_x", space = "free_x")+
  ggtitle("Relative abundance by genus (Most abundant 10)")
#By phylum 
plot_bar(phy_obj0, fill="Phylum",title="Abundance in reads by Phylum") + facet_wrap(~EnvType, scales= "free_x", nrow=1)
plot_composition(phy_obj0, taxaRank1 = NULL, taxaSet1 = NULL, taxaRank2 = "Phylum",numberOfTaxa = 10,x="Treatment") + 
  scale_fill_manual(values =c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F", "#CCFCCC", "#CCCCCC", "#FFFFCC", "#CC79A7","#8FD744FF","#EF9A9A","#AED581", "#009E73","#CCFCCC")) +
  facet_grid(~EnvType, scales = "free_x", space = "free_x")+
  ggtitle("Relative abundance by Phylum (Most abundant 10)")

#By family 
plot_bar(phy_obj0, fill="Family",title="Abundance in reads by Family") + facet_wrap(~EnvType, scales= "free_x", nrow=1)
plot_composition(phy_obj0, taxaRank1 = NULL, taxaSet1 = NULL, taxaRank2 = "Family",numberOfTaxa = 10,x="Treatment") + 
  scale_fill_manual(values =c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F", "#CCFCCC", "#CCCCCC", "#FFFFCC", "#CC79A7","#8FD744FF","#EF9A9A","#AED581", "#009E73","#CCFCCC")) +
  facet_grid(~EnvType, scales = "free_x", space = "free_x")+
  ggtitle("Relative abundance by Family (Most abundant 10)")

#By kingdom 
plot_bar(phy_obj0, fill="Kingdom",title="Abundance in reads by Kingdom") + facet_wrap(~EnvType, scales= "free_x", nrow=1)
plot_composition(phy_obj0, taxaRank1 = NULL, taxaSet1 = NULL, taxaRank2 = "Kingdom",numberOfTaxa = 10,x="Treatment") + 
  scale_fill_manual(values =c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F", "#CCFCCC", "#CCCCCC", "#FFFFCC", "#CC79A7","#8FD744FF","#EF9A9A","#AED581", "#009E73","#CCFCCC")) +
  facet_grid(~EnvType, scales = "free_x", space = "free_x")+
  ggtitle("Relative abundance by Kingdom (Most abundant 10)")

dev.off()

```


### Rhea analysis 

Para llevar a cabo el análisis utilizando Rhea, se requieren un árbol filogenético en formato newick que no hemos generado aún. Para ello, se utilizó el siguiente código. Solo se uso la base de datos de Fungi.   

```{r phylogenetic_tree_ITs}
#Loading required packages for the phylogenetic analysis
library(seqinr)
library(ape)
library(msa) # this package is available through the Bioconductor packages. If you do not have the BiocManager package installed, you could install this using t


## Read sequences from FASTA file of the cases

sequence <- readAAStringSet("results_ITS/frogs/tree_headers.fasta")

## Perform multiple sequence alignment

my_alignment <- msa(sequence)

## Compute distance matrix

my_alignment_sequence <- msaConvert(my_alignment, type="seqinr::alignment")

distance_alignment <- dist.alignment(my_alignment_sequence)

## compute phylogenetic tree using neighbor joining

Tree <- bionj(distance_alignment)


write.tree(Tree, file = "results_ITS/tree_newick_ITS", append = FALSE,
digits = 5, tree.names = TRUE)

write.nexus(Tree, file = "results_ITS/tree_nexus_ITS", translate = TRUE)

```

Ahora ya tenemos el árbol filogenético en formato newick podemos realizar los siguientes análisis siguiendo el código de Rhea. 

```{r Rhea_normalization_ITS}
#Processing OTU_table
affiliation_table_ITS <- read.delim("results_ITS/frogs/fungi_affiliation_final_res.tsv")

otu_table <- cbind(affiliation_table_ITS[,12:18],affiliation_table_ITS[,"blast_taxonomy"])
original_names <- colnames(otu_table) 
original_names[8] <- "taxonomy"
colnames(otu_table) <- original_names
rownames(otu_table) <- affiliation_table_ITS[,"observation_name"]
#Rhea pipeline
           #<--- CHANGE ACCORDINGLY

#' Please select the normalisation method
#' 0 = No random subsampling, no rounding
#' 1 = Random subsampling with rounding
method <- 0                                   #<--- CHANGE ACCORDINGLY

#' Pease select the normalization level used
#' 0 = Minimum sampling size
#' 1 = Fixed value (e.g. 1000)
level <- 0                                    #<--- CHANGE ACCORDINGLY

#' Please choose the value at which all samples will be normalized. (Only used if level selected is 1)
normCutoff <- 1000

#' Please choose the number of samples with the steepest rarefaction curves to be selectively plotted
#' The default number of samples presented separately is 5
labelCutoff <- 5                              #<--- CHANGE ACCORDINGLY

######                  NO CHANGES ARE NEEDED BELOW THIS LINE               ######

##################################################################################
######                             Main Script                              ######
##################################################################################

###################       Load all required libraries     ########################

# Check if required packages are already installed, and install if missing
packages <- c("GUniFrac","vegan")

# Function to check whether the package is installed
InsPack <- function(pack)
{
  if ((pack %in% installed.packages()) == FALSE) {
    install.packages(pack,repos = "http://cloud.r-project.org/")
  }
}

# Applying the installation on the list of packages
lapply(packages, InsPack)

# Make the libraries
lib <- lapply(packages, require, character.only = TRUE)

# Check if it was possible to install all required libraries
flag <- all(as.logical(lib))

###################       Read all required input files      ####################


# Making sure tha taxonomy column gets lower-case
col_names <- colnames(otu_table)
tax_ind <- which(sapply(tolower(col_names),
                        function(x) "taxonomy" %in% x, USE.NAMES = FALSE))
if (length(tax_ind) != 0) {
  col_names[tax_ind] <- tolower(col_names[tax_ind])
  colnames(otu_table) <- col_names
}
rm(col_names)
rm(tax_ind)

# Clean table from empty lines
otu_table <- otu_table[!apply(is.na(otu_table) | otu_table=="",1,all),]

####################       Normalize OTU Table          ###################


# Save taxonomy information in vector
taxonomy <- as.vector(otu_table$taxonomy)

# Delete column with taxonomy information in dataframe
otu_table$taxonomy <- NULL



if (level == 0) {
  # Calculate the minimum sum of all columns/samples
  min_sum <- min(colSums(otu_table))
} else {
  # The minimum size is set to a fixed reference level
  min_sum <- normCutoff
}


if (method == 0) {
  # Divide each value by the sum of the sample and multiply by the minimal sample sum
  norm_otu_table <- t(min_sum * t(otu_table) / colSums(otu_table))
} else {
  # Rarefy the OTU table to an equal sequencing depth
  norm_otu_table <- Rarefy(t(otu_table),depth = min_sum)
  norm_otu_table <- t(as.data.frame(norm_otu_table$otu.tab.rff))
}

# Calculate relative abundances for all OTUs over all samples
# Divide each value by the sum of the sample and multiply by 100
rel_otu_table <- t(100 * t(otu_table) / colSums(otu_table))


# Re-insert the taxonomy information in normalized counts table
norm_otu_table_tax <- cbind(norm_otu_table,taxonomy)

# Re-insert the taxonomy information in relative abundance table
rel_otu_table_tax <- cbind(rel_otu_table,taxonomy)

################################################################################
# Generate a two-sided pdf with a rarefaction curve for all samples and a curve
# Plot the rarefaction curve for all samples
pdf(file="results_ITS/final_graph/rarefaction_curve.pdf")
rarefactionCurve <- rarecurve(data.frame(t(otu_table)),
                              step = 20,
                              col = "black",
                              lty = "solid",
                              label = F,
                              xlab = "Number of Reads",
                              ylab = "Number of Species",
                              main = "Rarefaction Curves of All Samples")

# Generate empty vectors for the analysis of the rarefaction curve
slope = vector()
SampleID = vector()
angle <- c()

# Iterate through all samples
for (i in seq_along(rarefactionCurve)) {
  # If the sequencing depth is greater than 100, the slope of the line that passes between the last and last-100 count is calculated
  richness <- ifelse(length(rarefactionCurve[[i]]) > 6, 
                     (rarefactionCurve[[i]][length(rarefactionCurve[[i]])] - rarefactionCurve[[i]][length(rarefactionCurve[[i]])-5])/(attr(rarefactionCurve[[i]], "Subsample")[length(rarefactionCurve[[i]])]-attr(rarefactionCurve[[i]], "Subsample")[length(rarefactionCurve[[i]])-5]) , 1000)
  angle[i] <- ifelse(richness!=1000, atan(richness)*180/pi, NA)
  slope <- c(slope,richness)
  SampleID <- c(SampleID,as.character(names(otu_table)[i]))
}

# Generate the output table for rarefaction curve
curvedf <- cbind(SampleID, slope, angle)
ordered_vector <- order(as.numeric(curvedf[,2]), decreasing = TRUE)
curvedf <- curvedf[order(as.numeric(curvedf[,2]), decreasing = TRUE),]

# Generates a graph with all samples
# Underestimated cases are shown in red
for (i in 1:labelCutoff) {
  N <- attr(rarefactionCurve[[ordered_vector[i]]], "Subsample")
  lines(N, rarefactionCurve[[ordered_vector[i]]],col="red")
}
dev.off()

# Determine the plotting width and height
Nmax <- sapply(rarefactionCurve[ordered_vector[1:labelCutoff]], function(x) max(attr(x, "Subsample")))
Smax <- sapply(rarefactionCurve[ordered_vector[1:labelCutoff]], max)

# Creates an empty plot for rarefaction curves of underestimated cases
pdf(file="results_ITS/final_graph/rarefaction_curve_2.pdf")

plot(c(1, max(Nmax)), c(1, max(Smax)), xlab = "Number of Reads",
     ylab = "Number of Species", type = "n", main=paste(labelCutoff,"- most undersampled cases"))

for (i in 1:labelCutoff) {
  N <- attr(rarefactionCurve[[ordered_vector[i]]], "Subsample")
  lines(N, rarefactionCurve[[ordered_vector[i]]],col="red")
  text(max(attr(rarefactionCurve[[ordered_vector[i]]],"Subsample")), max(rarefactionCurve[[ordered_vector[i]]]), curvedf[i,1],cex=0.6)
}
dev.off()
#################################################################################
######                        Write Output Files                           ######
#################################################################################

# Write the normalized table in a file and copy in directories alpha-diversity and beta-diversity if existing
write.table(norm_otu_table, "results_ITS/OTUs_Table-norm.tab", sep = "\t",col.names = NA, quote = FALSE)
# Write the normalized table with taxonomy in a file
write.table(norm_otu_table_tax, "results_ITS/OTUs_Table-norm-tax.tab", sep = "\t",col.names = NA, quote = FALSE)

# Write the normalized relative abundance table in a file and copy in directory Serial-Group-Comparisons if existing
write.table(rel_otu_table, "results_ITS/OTUs_Table-norm-rel.tab", sep = "\t",col.names = NA, quote = FALSE)

# Write the normalized relative abundance with taxonomy table in a file and copy in directory Taxonomic-Binning if existing
write.table(rel_otu_table_tax, "results_ITS/OTUs_Table-norm-rel-tax.tab", sep ="\t",col.names = NA, quote = FALSE)

# Write the rarefaction table
write.table(curvedf, "results_ITS/RarefactionCurve.tab", sep ="\t", quote = FALSE, row.names = FALSE)

#################################################################################
######                           End of Script                             ######
#################################################################################
```

```{r Rhea_alpha_div_ITs, message=FALSE, warning=FALSE}
file_name <- "results_ITS/OTUs_Table-norm.tab"  
#' The abundance filtering cutoff 
eff.cutoff <- 0.0025 # this is the default value for Effective Richness (0.25%)

#' The normalized depth cutoff
norm.cutoff <- 1000 # this is the default value for Standard Richness (1000)

##################################################################################
######                        Diversity Functions                           ###### 
##################################################################################

# Calculate the species richness in a sample
Species.richness <- function(x)
{
  # Count only the OTUs that are present >0.5 normalized counts (normalization produces real values for counts)
  count=sum(x[x>0.5]^0)
  return(count)
}

# Calculate the Effective species richness in each individual sample
Eff.Species.richness <- function(x)
{
  # Count only the OTUs that are present more than the set proportion
  total=sum(x)
  count=sum(x[x/total>eff.cutoff]^0)
  return(count)
}

# Calculate the Normalized species richness in each individual sample
Norm.Species.richness <- function(x)
{
  # Count only the OTUs that are present >0.5 normalized counts (normalization produces real values for counts)
  # Given a fixed Normalization reads depth
  total=sum(x)
  count=sum(x[norm.cutoff*x/total>0.5]^0)
  return(count)
}


# Calculate the Shannon diversity index
Shannon.entropy <- function(x)
{
  total=sum(x)
  se=-sum(x[x>0]/total*log(x[x>0]/total))
  return(se)
}

# Calculate the effective number of species for Shannon
Shannon.effective <- function(x)
{
  total=sum(x)
  se=round(exp(-sum(x[x>0]/total*log(x[x>0]/total))),digits =2)
  return(se)
}

# Calculate the Simpson diversity index
Simpson.concentration <- function(x)
{
  total=sum(x)
  si=sum((x[x>0]/total)^2)
  return(si)
}

# Calculate the effective number of species for Simpson
Simpson.effective <- function(x)
{
  total=sum(x)
  si=round(1/sum((x[x>0]/total)^2),digits =2)
  return(si)
}

##################################################################################
######                             Main Script                              ###### 
##################################################################################

# Read a normalized OTU-table without taxonomy  
otu_table <- read.table (file_name, 
                       check.names = FALSE, 
                       header=TRUE, 
                       dec=".", 
                       sep = "\t",
                       row.names = 1)

# Clean table from empty lines
otu_table <- otu_table[!apply(is.na(otu_table) | otu_table=="",1,all),]

# Order and transpose OTU-table
my_otu_table <- otu_table[,order(names(otu_table))] 
my_otu_table <-data.frame(t(my_otu_table))

# Apply diversity functions to table
otus_div_stats<-data.frame(my_otu_table[,0])
otus_div_stats$Richness<-apply(my_otu_table,1,Species.richness)
otus_div_stats$Normalized.Richness<-apply(my_otu_table,1,Norm.Species.richness)
otus_div_stats$Effective.Richness<-apply(my_otu_table,1,Eff.Species.richness)
otus_div_stats$Shannon.Index<-apply(my_otu_table,1,Shannon.entropy)
otus_div_stats$Shannon.Effective<-apply(my_otu_table,1,Shannon.effective)
otus_div_stats$Simpson.Index<-apply(my_otu_table,1,Simpson.concentration)
otus_div_stats$Simpson.Effective<-apply(my_otu_table,1,Simpson.effective)
otus_div_stats$Evenness <- otus_div_stats$Shannon.Index/log(otus_div_stats$Richness,2)


# Write the results in a file and copy in directory "Serial-Group-Comparisons" if existing
write.table(otus_div_stats, "results_ITS/alpha-diversity.tab", sep="\t", col.names=NA, quote=FALSE)

##################################################################################
######                          End of Script                               ###### 
##################################################################################

```

```{r Rhea_beta_div_ITS, message=FALSE, warning=FALSE}
##################################################################################
######             Set parameters in this section manually                  ######
##################################################################################

#' Please give the file name of the normalized OTU-table without taxonomic classification
input_otu = "results_ITS/OTUs_Table-norm.tab"              #<--- CHANGE ACCORDINGLY !!!

#' Please give the name of the meta-file that contains individual sample information
input_meta = "results_ITS/metadata2.txt"                #<--- CHANGE ACCORDINGLY !!!

#' Please give the name of the phylogenetic tree constructed from the OTU sequences
input_tree = "results_ITS/tree_newick_ITS"                   #<--- CHANGE ACCORDINGLY !!!

#' Please give the column name (in the mapping file) of the categorical variable to be used for comparison (e.g. Genotype)
#' Please make sure that your name do not contain hyphens "-" as they will cause problems in the parsing of the names.
group_name = "Treatment"                            #<--- CHANGE ACCORDINGLY !!!

##################################################################################
######                  Additional parameters                               ######
##################################################################################

#' Turn on sample labeling
#' 0 = Samples are not labeled in the MDS/NMDS plots
#' 1 = All Samples are labeled in the MDS/NMDS plots
label_samples = 0

#' Determine which sample label should appear
#' Write the name of samples (in quotation marks), which should appear in the MDS/NMDS plots, in the vector (c) below
#' If more than one sample should be plotted, please separate their IDs by comma (e.g. c("sample1","sample2"))
label_id =c("")

#' De-Novo Clustering will be performed for the number of samples or maximal for the set limit
#' Default Limit is 100
kmers_limit=20

######                  NO CHANGES ARE NEEDED BELOW THIS LINE               ######

##################################################################################
######                             Main Script                              ######
##################################################################################

###################       Load all required libraries     ########################

# Check if required packages are already installed, and install if missing
packages <-c("ade4","GUniFrac","phangorn","cluster","fpc","vegan","clusterSim") 

# Function to check whether the package is installed
InsPack <- function(pack)
{
  if ((pack %in% installed.packages()) == FALSE) {
    install.packages(pack,repos ="http://cloud.r-project.org/")
  } 
}

# Applying the installation on the list of packages
lapply(packages, InsPack)

# Make the libraries
lib <- lapply(packages, require, character.only = TRUE)

# Check if it was possible to install all required libraries
flag <- all(as.logical(lib))


###################       Read all required input files      ####################
# Load the mapping file containing individual sample information (sample names in the first column)
meta_file <- read.table (file = input_meta, check.names = FALSE, header = TRUE, dec = ".", sep = "\t", row.names = 1, comment.char = "")

# Clean table from empty lines
meta_file <- data.frame(meta_file[!apply(is.na(meta_file) | meta_file=="",1,all),,drop=FALSE])

# Order the mapping file by sample names (ascending)
meta_file <- data.frame(meta_file[order(row.names(meta_file)),,drop=FALSE])

# Save the position of the target group name in the mapping file
meta_file_pos <- which(colnames(meta_file) == group_name)

# Select metadata group based on the pre-set group name
all_groups <- as.factor(meta_file[,meta_file_pos])

#------------------------------------------------------------------------

# Load the tab-delimited file containing the values to be analyzed (samples names in the first column)
otu_file <- read.table (file = input_otu, check.names = FALSE, header = TRUE, dec = ".", sep = "\t", row.names = 1, comment.char = "")

# Clean table from empty lines
otu_file <- otu_file[!apply(is.na(otu_file) | otu_file =="",1,all),]

# keep only those rows that appear in the mapping file
otu_file <- otu_file[,rownames(meta_file)]

# OTU-table and mapping file should have the same order and number of sample names
# Order the OTU-table by sample names (ascending)
otu_file <- otu_file[,order(names(otu_file))]

# Transpose OTU-table and convert format to a data frame
otu_file <- data.frame(t(otu_file), check.names = FALSE)

#------------------------------------------------------------------------

# Load the phylogenetic tree calculated from the OTU sequences 
tree_file <- read.tree(input_tree)

# Remove single quotes from the tips of the tree
tree_file$tip.label <- gsub("'", "", tree_file$tip.label)

# Root the OTU tree at midpoint 
rooted_tree <- midpoint(tree_file)


####################       Calculate beta-diversity          ###################

# Create the directory where all output files are saved (is named after the target group name set above for comparisons)
dir.create(group_name)

# Calculate the UniFrac distance matrix for comparing microbial communities
unifracs <- GUniFrac(otu_file, rooted_tree, alpha = c(0.0,0.5,1.0))$unifracs

# Weight on abundant lineages so the distance is not dominated by highly abundant lineages with 0.5 having the best power
unifract_dist <- unifracs[, , "d_0.5"]

################ Generate tree #######################

# Save the UniFrac output as distance object
all_dist_matrix <- as.dist(unifract_dist)

# Apply a hierarchical cluster analysis on the distance matrix based on the Ward's method
all_fit <- hclust(all_dist_matrix, method = "ward.D2")

# Generates a tree from the hierarchically generated object
tree <- as.phylo(all_fit)
plot_color<-rainbow(length(levels(all_groups)))[all_groups]

# Save the generated phylogram in a pdf file
pdf(file="results_ITS/final_graph/phylogram_curve.pdf")

# The tree is visualized as a Phylogram color-coded by the selected group name
plot(tree, type = "phylogram",use.edge.length = TRUE, tip.color = (plot_color), label.offset = 0.01)
print.phylo(tree)
axisPhylo()
tiplabels(pch = 16, col = plot_color)
dev.off()

#################            Build NMDS plot           ########################

# Generated figures are saved in a pdf file 
file_name <- "results_ITS/final_graph/beta_diversity_curve.pdf"
pdf(file_name)

# Calculate the significance of variance to compare multivariate sample means (including two or more dependent variables)
# Omit cases where there isn't data for the sample (NA)
all_groups_comp <- all_groups[!is.na(all_groups)]
unifract_dist_comp <- unifract_dist[!is.na(all_groups), !is.na(all_groups)]
adonis<-adonis2(as.dist(unifract_dist_comp) ~ all_groups_comp)
permdisp <- permutest(betadisper(as.dist(unifract_dist_comp),as.factor(all_groups_comp),type="median"))
all_groups_comp<-factor(all_groups_comp,levels(all_groups_comp)[unique(all_groups_comp)])

# Calculate and display the MDS plot (Multidimensional Scaling plot)
s.class(
  cmdscale(unifract_dist_comp, k = 2), col = unique(plot_color), cpoint =
    2, fac = all_groups_comp, sub = paste("MDS plot of Microbial Profiles\nPERMDISP     p=",permdisp[["tab"]][["Pr(>F)"]][1],"\n",
                                          "PERMANOVA  p=",adonis[1,5],sep="")
)
if (label_samples==1) {
  lab_samples <- row.names(cmdscale(unifract_dist_comp, k = 2))
  ifelse (label_id != "",lab_samples <- replace(lab_samples, !(lab_samples %in% label_id), ""), lab_samples)
  text(cmdscale(unifract_dist_comp, k = 2),labels=lab_samples,cex=0.7,adj=c(-.1,-.8))
}

# Calculate and display the NMDS plot (Non-metric Multidimensional Scaling plot)
meta <- metaMDS(unifract_dist_comp,k = 2)
s.class(
  meta$points, col = unique(plot_color), cpoint = 2, fac = all_groups_comp,
  sub = paste("metaNMDS plot of Microbial Profiles\nPERMDISP     p=",permdisp[["tab"]][["Pr(>F)"]][1],"\n",
              "PERMANOVA  p=",adonis[1,5],sep="")
)
if (label_samples==1){
  lab_samples <- row.names(meta$points)
  ifelse (label_id != "",lab_samples <- replace(lab_samples, !(lab_samples %in% label_id), ""), lab_samples)
  text(meta$points,labels=lab_samples,cex=0.7,adj=c(-.1,-.8))
}

#close the pdf file
dev.off()

###############          NMDS for pairwise analysis        ###################

# This plot is only generated if there are more than two groups included in the comparison
# Calculate the pairwise significance of variance for group pairs
# Get all groups contained in the mapping file
unique_groups <- levels(all_groups_comp)
if (dim(table(unique_groups)) > 2) {
  
  # Initialise vector and lists
  pVal = NULL
  permdisppval=NULL
  pairedMatrixList <- list(NULL)
  pair_1_list <- NULL
  pair_2_list <- NULL
  
  for (i in 1:length(combn(unique_groups,2)[1,])) {
    
    # Combine all possible pairs of groups
    pair_1 <- combn(unique_groups,2)[1,i]
    pair_2 <- combn(unique_groups,2)[2,i]
    
    # Save pairs information in a vector
    pair_1_list[i] <- pair_1
    pair_2_list[i] <- pair_2
    
    # Generate a subset of all samples within the mapping file related to one of the two groups
    inc_groups <-
      rownames(subset(meta_file, meta_file[,meta_file_pos] == pair_1
                      |
                        meta_file[,meta_file_pos] == pair_2))
    
    # Convert UniFrac distance matrix to data frame
    paired_dist <- as.data.frame(unifract_dist_comp)
    
    # Save all row names of the mapping file
    row_names <- rownames(paired_dist)
    
    # Add row names to the distance matrix
    paired_dist <- cbind(row_names,paired_dist)
    
    # Generate distance matrix with samples of the compared groups (column-wise)
    paired_dist <- paired_dist[sapply(paired_dist[,1], function(x) all(x %in% inc_groups)),]
    
    # Remove first column with unnecessary group information
    paired_dist[,1] <- NULL
    paired_dist <- rbind(row_names,paired_dist)
    
    # Generate distance matrix with samples of the compared group (row-wise)
    paired_dist <- paired_dist[,sapply(paired_dist[1,], function(x) all(x %in% inc_groups))]
    
    # Remove first row with unnecessary group information 
    paired_dist <- paired_dist[-1,]
    
    # Convert generated distance matrix to data type matrix (needed by multivariate analysis)
    paired_matrix <- as.matrix(paired_dist)
    class(paired_matrix) <- "numeric"
    
    # Save paired matrix in list
    pairedMatrixList[[i]] <- paired_matrix
    
    # Applies multivariate analysis to a pair out of the selected groups
    adonis <- adonis2(paired_matrix ~ all_groups_comp[all_groups_comp == pair_1 |
                                                        all_groups_comp == pair_2])
    
    permdisp <- permutest(betadisper(as.dist(paired_matrix),as.factor(all_groups_comp[all_groups_comp == pair_1 |
                                                                                        all_groups_comp == pair_2]),type="median"),pairwise = T)
    
    # List p-values
    pVal[i] <- adonis[1,5]
    permdisppval[i] <- permdisp$pairwise[2]
    
  }
  
  # Adjust p-values for multiple testing according to Benjamini-Hochberg method
  pVal_BH <- round(p.adjust(pVal,method="BH", n=length(pVal)),4)
  permdisppval_BH <- round(p.adjust(permdisppval,method="BH", n=length(permdisppval)),4)
  
  
  # Generated NMDS plots are stored in one pdf file called "pairwise-beta-diversity-nMDS.pdf"
  file_name <- "results_ITS/final_graph/pairwise-beta-diversity-NMDS.pdf"
pdf( file_name)  
  for(i in 1:length(combn(unique_groups,2)[1,])){
    meta <- metaMDS(pairedMatrixList[[i]], k = 2)
    s.class(
      meta$points,
      col = rainbow(length(levels(all_groups_comp))), cpoint = 2,
      fac = as.factor(all_groups_comp[all_groups_comp == pair_1_list[i] |
                                        all_groups_comp == pair_2_list[i]]),
      sub = paste("NMDS plot of Microbial Profiles\n ",pair_1_list[i]," - ",pair_2_list[i], "\n PERMDISP     p=",permdisppval[[i]],",","  p.adj=", permdisppval_BH[i],"\n",
                  " PERMANOVA  p=",pVal[i],","," p.adj=",pVal_BH[i],sep="")
    )
  }
  dev.off()
  
  # Generated MDS plots are stored in one pdf file called "pairwise-beta-diversity-MDS.pdf"
    file_name <- "results_ITS/final_graph/pairwise-beta-diversity-MDS.pdf"
pdf(file_name) 
  
  for(i in 1:length(combn(unique_groups,2)[1,])){
    # Calculate and display the MDS plot (Multidimensional Scaling plot)
    s.class(
      cmdscale(pairedMatrixList[[i]], k = 2), col = rainbow(length(levels(all_groups_comp))), cpoint =
        2, fac = as.factor(all_groups_comp[all_groups_comp == pair_1_list[i] |
                                             all_groups_comp == pair_2_list[i]]), 
      sub = paste("MDS plot of Microbial Profiles\n ",pair_1_list[i]," - ",pair_2_list[i], "\n PERMDISP     p=",permdisppval[[i]],",","  p.adj=", permdisppval_BH[i],"\n",
                  " PERMANOVA  p=",pVal[i],","," p.adj=",pVal_BH[i],sep="")
    )
  }
  dev.off()                                     
  
}

######                        Determine number of clusters                           ######

ch_nclusters=NULL
sil_nclusters=NULL
dunn_nclusters=NULL
db_nclusters=NULL

if (dim(otu_file)[1]-1 <= kmers_limit) {
  kmers_limit=dim(otu_file)[1]-1
}
for (k in 1:kmers_limit) { 
  if (k==1) {
    ch_nclusters[k]=NA 
    sil_nclusters[k]=NA
    dunn_nclusters=NA
    db_nclusters=NA
  } else {
    # Partitioning the data into k clusters (max k is number of samples within the dataset)
    data_cluster=as.vector(pam(as.dist(unifract_dist_comp), k, diss=TRUE)$clustering)
    
    # Calculate Calinski-Harabasz and silhouette Index 
    index=cluster.stats(as.dist(unifract_dist_comp),data_cluster)
    index_db=index.DB(x=otu_file,cl=data_cluster,d=as.dist(unifract_dist_comp), centrotypes="medoids")
    ch_nclusters[k] <- index[["ch"]]
    sil_nclusters[k] <- index[["avg.silwidth"]]
    dunn_nclusters[k] <- index[["dunn2"]]
    db_nclusters[k] <-  index_db[["DB"]]
    print(k)
  }
}

# Generated plot showing the optimal number of clusters
for (i in 1:2){
  if (i==1) { pdf("results_ITS/final_graph/De-novo-clustering.pdf") }
  if (i==2) { pdf("results_ITS/final_graph/De-novo-clustering.pdf")}
  
  plot(ch_nclusters, type="h", xlab="k clusters", ylab="CH index",main="Optimal number of clusters (CH index)")
  title(sub="*The higher the value the better", adj=0, cex.sub=0.9)
  plot(sil_nclusters, type="h", xlab="k clusters", ylab="Average silhouette width",main="Optimal number of clusters (Silhouette index)")
  title(sub="*The higher the value the better", adj=0, cex.sub=0.9)
  plot(dunn_nclusters, type="h", xlab="k clusters", ylab="Dunn Index",main="Optimal number of clusters (Dunn Index)")
  title(sub="*The higher the value the better", adj=0, cex.sub=0.9)
  plot(db_nclusters, type="h", xlab="k clusters", ylab="Davies-Bouldin Index",main="Optimal number of clusters (Davies-Bouldin Index)")
  title(sub="*The lower the value the better", adj=0, cex.sub=0.9)
  
  dev.off()
}

#################################################################################
######                        Write Output Files                           ######
#################################################################################

# Write the distance matrix table in a file
file_name <- "results_ITS/distance-matrix-gunif.tab"
write.table( unifract_dist_comp, file_name, sep = "\t", col.names = NA, quote = FALSE)
write.table( unifract_dist_comp, "results_ITS/distance-matrix-gunif.tab", sep = "\t", col.names = NA, quote = FALSE)
write.tree(tree,"results_ITS/samples-Tree.nwk",tree.names = FALSE)

# Graphical output files are generated in the main part of the script
if(!flag) { stop("
                 It was not possible to install all required R libraries properly.
                 Please check the installation of all required libraries manually.\n
                 Required libaries:ade4, GUniFrac, phangorn")
}

#################################################################################
######                           End of Script                             ######
#################################################################################

```


```{r taxonomic_binning_rhea_ITs}
##################################################################################
######             Set parameters in this section manually                  ######
##################################################################################

#' Please set the directory of the script as the working folder (e.g D:/studyname/NGS-Data/Rhea/numtax/)
#' Note: the path is denoted by forward slash "/"
setwd("results_ITS/") #<--- CHANGE ACCORDINGLY

#' Please give the file name of the OTU-table containing relative abundances and taxonomic classification 
otu_file<-"OTUs_Table-norm-rel-tax.tab"  #<--- CHANGE ACCORDINGLY


######                  NO CHANGES ARE NEEDED BELOW THIS LINE               ######

##################################################################################
######                             Main Script                              ###### 
##################################################################################

###################            Read input table              ####################

# Load the tab-delimited file containing the abundances and taxonomic information to be checked (rownames in the first column)
otu_table <-  read.table (otu_file,
                          check.names = FALSE,
                          header = TRUE,
                          dec = ".",
                          sep = "\t",
                          row.names = 1,
                          comment.char = "")

# Clean table from empty lines
otu_table <- otu_table[!apply(is.na(otu_table) | otu_table=="",1,all),]
otu_table <- otu_table[!otu_table$taxonomy=="no data",]
# Create a dataframe with a number of rows identical to the number of OTUs in the dataset
taxonomy <- otu_table[,dim(otu_table)[2]]
# Test if the taxonomy column is in the correct format (delimited by semicolon)
if(all(grepl("(?:[^;]*;){6}", taxonomy))==FALSE) {

#Send error message if taxonomy is not in the right format
  stop("Wrong number of taxonomic classes\n

Taxonomic levels have to be separated by semicolons (six in total). 
IMPORTANT: if taxonomic information at any level is missing, the semicolons are still needed:\n
       
      e.g.Bacteria;Bacteroidetes;Bacteroidia;Bacteroidales;Prevotellaceae;Prevotella;
      e.g.Bacteria;Bacteroidetes;Bacteroidia;Bacteroidales;Prevotellaceae;;")
} else { 

# Delete the taxonomy row from the OTU-table
otuFile <- otu_table[,c(1:dim(otu_table)[2] - 1)]

# Initialize empty dataframe
taxonomy_new <- NULL

for (i in 1:dim(otu_table)[1]) {
  # Split taxonomic information in its taxonomic classes
  # Kingdom - Phylum - Class - Family - Order - Genus
  splitTax <- strsplit(x = as.character(taxonomy[i]), ";")
  
  # Save the position where the first empty string (sequence of characters) occurs
  value <- which(splitTax[[1]] == "")[1]
  
  # Save the last known taxa information
  lastTaxa = splitTax[[1]][value - 1]
  
  # Replace all empty values by the last taxa information and the prefix "unkown_"
  splitTax <-replace(splitTax[[1]],splitTax[[1]] == "",paste("unknown_",lastTaxa))
 
  # Write new taxonomic information in the dataframe
  taxonomy_new[i] <- list(splitTax)
}

# Adjust dataframe with modified taxonomic information
taxonomy_new <- t(as.data.frame(taxonomy_new))
row.names(taxonomy_new) <- row.names(otuFile)

# Add level information to all taxonomies
# For taxonomies related to kingdom level
taxonomy_new[,1] <- sub("^","k__",taxonomy_new[,1])

# For taxonomies related to phylum level
taxonomy_new[,2] <- sub("^","p__",taxonomy_new[,2])

# For taxonomies related to class level
taxonomy_new[,3] <- sub("^","c__",taxonomy_new[,3])

# For taxonomies related to order level
taxonomy_new[,4] <- sub("^","o__",taxonomy_new[,4])

# For taxonomies related to family level
taxonomy_new[,5] <- sub("^","f__",taxonomy_new[,5])

# For taxonomies related to genus level
taxonomy_new[,6] <- sub("^","g__",taxonomy_new[,6])

#################################################################################

# Create list with taxonomic information for each taxonomy level
class_list <-
  list(
    unique(taxonomy_new[,1]),unique(taxonomy_new[,2]),
    unique(taxonomy_new[,3]), unique(taxonomy_new[,4]),
    unique(taxonomy_new[,5]),unique(taxonomy_new[,6])
  )

# Clone the created list for further processing
sample_list <- class_list
list_length <- NULL

# Iterate through all six taxonomy levels
for (a in 1:6) {
  
  lis <- lapply(class_list[a], lapply, length)
  names(lis)<-lapply(class_list[a],length)
  
  # Individual number of taxonomies for each taxonomic level
  num_taxa <- as.integer(names(lis))
  list_length[a] <- num_taxa
  
  # Iterate through taxonomic class specific taxonomies
  for (b  in 1:num_taxa) {
    
    # Initialize list with the value zero for all taxonomies
    sample_list[[a]][[b]] <- list(rep.int(0,dim(otuFile)[2]))
    
  }
}

#################################################################################
#################################################################################
# Save relative abundances of all samples for each taxonomy

# Iterate through all OTUs
for (i in 1:dim(otu_table)[1]) {
  
  # Iterate through all taxonomic levels
  for (m in 1:6) {
    
    # List of m-th taxonomies of i-th taxonomic levels (e.g. m = Kingdom, i = 4th OTU -> Clostridiales)
    taxa_in_list <- list(taxonomy_new[i,])[[1]][m]
    
    # Record the current position in a list
    position <- which(class_list[[m]] == taxa_in_list)
    
    # All rows with taxonomic information of n-th sample
    matrix <- data.matrix(otuFile)
    sub_sample_tax <-(subset(matrix,taxonomy_new[,m] == taxa_in_list))
    
    # Get the actual value out of the list (initialized with zero)
    temp <- unlist(sample_list[[m]][[position]])
    
    # Calculate the summed up relative abundances for the particular taxonomic class for n-th sample
    temp <- colSums(sub_sample_tax)
    
    # Replace values by new summed values
    sample_list[[m]][[position]] <- list(temp)
    
  }
}

#################################################################################
######                         Write output                                ######
#################################################################################

# Generate tables for each taxonomic class

##Kingdom table
# Create table with taxonomic information (kingdom level)
kingdom <-  matrix(unlist(sample_list[[1]]),nrow = dim(otuFile)[2],ncol = list_length[1],dimnames = list(names(otuFile),unlist(class_list[[1]])))
kingdom <- (t(kingdom))

##Phylum table
# Create table with taxonomic information (phylum level)
phyla <-matrix(unlist(sample_list[[2]]),nrow = dim(otuFile)[2],ncol = list_length[2],dimnames = list(names(otuFile),unlist(class_list[[2]])))
phyla <- (t(phyla))

# Order table according to taxonomic name (descending)
phyla <- phyla[order(row.names(phyla)),]

## Class table
# Create table with taxonomic information (class level)
classes <- matrix(unlist(sample_list[[3]]), nrow = dim(otuFile)[2], ncol = list_length[3], dimnames = list(names(otuFile),unlist(class_list[[3]])))
classes <- (t(classes))

# Order dataframe according to taxonomic name (descending)
classes <- classes[order(row.names(classes)),]

## Orders
# create table with taxonomic information (Order)
orders <-matrix(unlist(sample_list[[4]]),nrow = dim(otuFile)[2],ncol = list_length[4],dimnames = list(names(otuFile),unlist(class_list[[4]])))
orders <- (t(orders))

# Order dataframe according to taxonomic name (descending)
orders <- orders[order(row.names(orders)),]

## Family table
# Create table with taxonomic information (family level)
families <-matrix(unlist(sample_list[[5]]),nrow = dim(otuFile)[2],ncol = list_length[5],dimnames = list(names(otuFile),unlist(class_list[[5]])))
families <- (t(families))

# Order dataframe according to taxonomic name (descending)
families <- families[order(row.names(families)),]

## Genus level
# Create table with taxonomic information (generum level)
genera <- matrix(unlist(sample_list[[6]]),nrow = dim(otuFile)[2],ncol = list_length[6],dimnames = list(names(otuFile),unlist(class_list[[6]])))
genera <- (t(genera))

# Order dataframe according to taxonomic name (descending)
genera <- genera[order(row.names(genera)),]

# Merge all dataframes
tax_summary <-rbind.data.frame(kingdom,phyla,classes,orders,families,genera)

# Identify duplicates and remove them
tax_summary <- tax_summary[!duplicated(row.names(tax_summary)),]

################################################################################
######                        Write Output Files                           ######
#################################################################################

# Create a directory 
dir.create("Taxonomic-Binning")

# Set path for all outputs to the new directory
setwd("Taxonomic-Binning")

# Write output files for taxonomic composition of every sample
write.table(kingdom,"0.Kingdom.all.tab",sep = "\t",col.names = NA)
write.table(phyla,"1.Phyla.all.tab",sep = "\t",col.names = NA)
write.table(classes,"2.Classes.all.tab",sep = "\t",col.names = NA)
write.table(orders,"3.Orders.all.tab",sep = "\t",col.names = NA)
write.table(families,"4.Families.all.tab",sep = "\t",col.names = NA)
write.table(genera,"5.Genera.all.tab",sep = "\t",col.names = NA)
write.table(tax_summary,"tax.summary.all.tab",sep = "\t",col.names = NA)

#################################################################################
######                        Write Graphical Output                       ######
#################################################################################

pdf("taxonomic-overview.pdf")
par(xpd=T, mar=par()$mar+c(0,0,0,9))

#Kingdom
#k_col=distinctColorPalette(dim(kingdom)[1])
k_col=rainbow(dim(kingdom)[1])
k_col=sample(k_col)
barplot(kingdom,col=k_col, cex.names=0.5, ylab="cumulative relative abundance (%)", las=2, main="Taxonomic binning at Kingdom level")
legend(par('usr')[2], par('usr')[4], bty='n',rev(row.names(kingdom)),cex=0.7,col = rev(k_col),pch = 16,pt.cex = 1.2)

#Phyla
#p_col=distinctColorPalette(dim(phyla)[1])
p_col=rainbow(dim(phyla)[1])
p_col=sample(p_col)
barplot(phyla,col=p_col, cex.names=0.5,ylab="cumulative relative abundance (%)",las=2, main="Taxonomic binning at Phyla level")
legend(par('usr')[2], par('usr')[4], bty='n',rev(row.names(phyla)),cex=0.7,col = rev(p_col),pch = 16,pt.cex = 1.2)

#Classes
c_col=rainbow(dim(classes)[1])
c_col=sample(c_col)
barplot(classes,col=c_col, cex.names=0.5,ylab="cumulative relative abundance (%)",las=2, main="Taxonomic binning at Class level")
legend(par('usr')[2], par('usr')[4], bty='n',rev(row.names(classes)),cex=0.7,col = rev(c_col),pch = 16,pt.cex = 1.2)

#Orders
o_col=rainbow(dim(orders)[1])
o_col=sample(o_col)
barplot(orders,col=o_col, cex.names=0.5,ylab="cumulative relative abundance (%)",las=2, main="Taxonomic binning at Order level")
legend(par('usr')[2], par('usr')[4], bty='n',rev(row.names(orders)),cex=0.7,col = rev(o_col),pch = 16,pt.cex = 1.2)

#Families
f_col=rainbow(dim(families)[1])
f_col=sample(f_col)
barplot(families,col=f_col, cex.names=0.5,ylab="cumulative relative abundance (%)",las=2, main="Taxonomic binning at Family level")
legend(par('usr')[2], par('usr')[4], bty='n',rev(row.names(families)),cex=0.7,col = rev(f_col),pch = 16,pt.cex = 1.2)

#Genera
g_col=rainbow(dim(genera)[1])
g_col=sample(g_col)
barplot(genera,col=g_col, cex.names=0.5,ylab="cumulative relative abundance (%)",las=2, main="Taxonomic binning at Genus level")
legend(par('usr')[2], par('usr')[4], bty='n',rev(row.names(genera)),cex=0.7,col = rev(g_col),pch = 16,pt.cex = 1.2)

dev.off()
}

#################################################################################
######                           End of Script                             ######
#################################################################################

```

```{r}
#' Note: the path is denoted by forward slash "/"
setwd("results_ITS/")  #<--- CHANGE ACCORDINGLY

#' Please give the name of the file with alpha-diversity measures
alpha <- "alpha-diversity.tab";                         #<--- CHANGE ACCORDINGLY

#' Please give the name of the file with OTUs relative abundance
RelativeAbundanceOTUs <- "OTUs_Table-norm-rel.tab";     #<--- CHANGE ACCORDINGLY

#' Please give the name of the file with relative abundances of different taxonomic levels
TaxanomyAll <- "Taxonomic-Binning/tax.summary.all.tab";                   #<--- CHANGE ACCORDINGLY

#' Please give the name of the meta file with sample groups and additional metadata variables if available
MetaFile <- "metadata2.txt";                         #<--- CHANGE ACCORDINGLY

######                  NO CHANGES ARE NEEDED BELOW THIS LINE               ######

##################################################################################
######                             Main Script                              ######
##################################################################################

###################       Load all required libraries     ########################

# Check if required packages are already installed, and install if missing
packages <-c("compare")

# Function to check whether the package is installed
InsPack <- function(pack)
{
  if ((pack %in% installed.packages()) == FALSE) {
    install.packages(pack,repos ="http://cloud.r-project.org/")
  }
}

# Applying the installation on the list of packages
lapply(packages, InsPack)

# Make the libraries
lib <- lapply(packages, require, character.only = TRUE)

# Check if it was possible to install all required libraries
flag <- all(as.logical(lib))

###################            Read input table              ####################

# Reading Alpha diversity file
alpha <- read.table(file=alpha,header=TRUE,sep="\t",row.names=1,check.names = F)

# Clean table from empty lines
alpha <- alpha[!apply(is.na(alpha) | alpha=="",1,all),]

# Read OTU table
RelativeAbundanceOTUs <-as.data.frame(t(read.table(file=RelativeAbundanceOTUs,header=TRUE,sep="\t",row.names = 1,check.names = F)))

# Clean table from empty lines
RelativeAbundanceOTUs <- RelativeAbundanceOTUs[!apply(is.na(RelativeAbundanceOTUs) | RelativeAbundanceOTUs=="",1,all),]

# Read Mapping file
MetaFile <- read.table(file=MetaFile,header=TRUE,sep="\t",comment.char = "",row.names = 1,check.names = F)

# Clean table from empty lines
MetaFile <- MetaFile[!apply(is.na(MetaFile) | MetaFile=="",1,all),,drop=FALSE]

# Read taxonomy file
TaxanomyAll <- read.table(file=TaxanomyAll,header=TRUE,sep="\t",row.names=NULL,check.names = F)

# Clean table from empty lines
TaxanomyAll <- TaxanomyAll[!apply(is.na(TaxanomyAll) | TaxanomyAll=="",1,all),]

# Select the RelativeAbundanceOTUs and alpha rows based on the mapping file
RelativeAbundanceOTUs <- RelativeAbundanceOTUs[rownames(MetaFile),]
alpha <- alpha[rownames(MetaFile),]

# Prepare TaxanomyAll table
ColnameTo_assign<- TaxanomyAll[,1]
TaxanomyAll[,1] <- NULL
TaxanomyAll <- as.data.frame(t(TaxanomyAll))
TaxanomyAll <- TaxanomyAll[rownames(MetaFile),]
colnames(TaxanomyAll) <- ColnameTo_assign

######################          MAIN PROGRAM                #####################

# Preparing TAXA table:
combine_taxa<- cbind.data.frame(MetaFile[rownames(TaxanomyAll),],alpha[rownames(TaxanomyAll),],TaxanomyAll) # merging Meta+Alpha+Taxa based on same order row.
combine_taxa$SampleID <- row.names(combine_taxa)
combine_taxa<-combine_taxa[,c(ncol(combine_taxa),1:(ncol(combine_taxa)-1))]# replacing columnn at the beginging.

# Prepare OTU table:
combine_OTUs<- cbind.data.frame(MetaFile[rownames(RelativeAbundanceOTUs),],alpha[rownames(RelativeAbundanceOTUs),],RelativeAbundanceOTUs) # merging Meta+Alpha+OTUs based on same order row.
combine_OTUs$SampleID <- row.names(combine_OTUs)
colnames(combine_OTUs)
combine_OTUs<-combine_OTUs[,c(ncol(combine_OTUs),1:(ncol(combine_OTUs)-1))]# replacing columnn at the beginging.

#################################################################################
######                        Write Output Files                           ######
#################################################################################

# Writing tables
if(compareIgnoreOrder(row.names(TaxanomyAll),row.names((MetaFile)))$result &
   compareIgnoreOrder(row.names(alpha),row.names((MetaFile)))$result &
   compareIgnoreOrder(row.names(RelativeAbundanceOTUs),row.names((MetaFile)))$result){
  write.table(combine_taxa,file="TaxaCombined.tab",sep="\t",row.names=FALSE)
  write.table(combine_OTUs,file="OTUsCombined.tab",sep="\t",row.names=FALSE)
  message("Files are combined successfully")
}else{
  stop("ATTENTION !!!! Sample names differ across files. Script aborted.",
       "Please ensure that identical sample names are used.")
}

if(!flag) { stop("
    It was not possible to install all required R libraries properly.
                 Please check the installation of all required libraries manually.\n
                 Required libaries:compare")
}


#################################################################################
######                           End of Script                             ######
#################################################################################
```


```{r OTUS_res_ITS, message=FALSE, warning=FALSE}

##################################################################################
######             Set parameters in this section manually                  ######
##################################################################################

#' Please set the directory of the present script as the working folder (e.g. D:/studyname/NGS-Data/Rhea/correlation/)
#' Note: the path is denoted by forward slash "/"
setwd("results_ITS")                     #<--- CHANGE ACCORDINGLY !!!

#' Please give the file name of the table containing the variables for analysis
input_file <-"TaxaCombined.tab"              #<--- CHANGE ACCORDINGLY !!!

#' Please give the position where the taxonomic variables (OTUs or taxonomic groups) start!!
#' IMPORTANT: Since the first column in the input file will be used as row names, we do not count it!
otu_variables_start <- 10                                        #<--- CHANGE ACCORDINGLY !!!

#################################################################################
#########         Optional parameters in this section                       #####
#################################################################################
# Unless users follow specific purposes and know exactly what to test, default values are recommended.

# Set the cutoff for significance
# Possible values are any real number between 0 and 1 (default is 0.05)
signf_cutoff <- 0.05

# Calculate correlation among taxonomic variables
# If selected, this will result in many additional tests for correlation among the taxonomic data
# Possible parameters are 1 or 0 (default is 0):
# 1 = calculate correlations within OTUs or taxa
# 0 = NO test within taxonomic variables
includeTax <- 1

# Calculate correlation among meta-variables
# If selected, this will result in many additional tests for correlation among the meta-data
# Possible parameters are 1 or 0 (default is 0):
# 1 = calculate correlations within meta-variables
# 0 = NO test within meta-variables
includeMeta <- 1

# Handling of missing values for meta-variables
# Possible parameters are 1 or 0 (default is 0):
# 1 = missing values are filled with the mean for the corresponding variable
# 0 = NO imputation (replacing missing data with substituted values)
fill_NA <- 0

# Treat zeros in taxonomic variables as missing values
# Possible parameters are 1 or 0 (default is 1):
# 1 = Consider taxonomic zeros as missing values
# 0 = Keep zeros for the calculation of correlations
replace_zeros <- 1

# Set a cutoff for the minimum number of values (prevalence) for a given taxonomic variable to be considered for calculation
# OTUs or taxa with prevalences below the cutoff are excluded from the analysis because considered as non-relevant in the study (very incidental occurancies)
# An OTU is considered present if the value is NOT missing or zero
# This filter reduces the number of tests for poorly supported correlations
# Possible values are any real number between 0 and 1 (default is 0.3, i.e. 30 % of samples must have a value for the given variable)
prevalence_exclusion <- 0.3

# Set a cutoff for the minimal number of pairs observations required for calculation of correlations
# The decision is subjective and depends on the total number of samples
# This filter reduces the number of tests for poorly supported correlations
# Possible values: any positive integer between 0 and the total number of samples (default is 4)
min_pair_support <- 4

# Set a significance cutoff for graphical output
# Only correlations with an uncorrected p-value less than the cuttoff will be plotted
# Possible values: any real number between 0 and 1 (default is 0.05)
plot_pval_cutoff <- 0.05

# Set a correlation coefficient cutoff for graphical output
# Only correlations above the cuttof (absolute value) will be ploted
# Possible values: any real number between 0 and 1 (default is 0.5)
plot_corr_cutoff <- 0.5

######                  NO CHANGES ARE NEEDED BELOW THIS LINE               ######

##################################################################################
######                             Main Script                              ######
##################################################################################

###################       Load all required libraries     ########################

# Check if required packages are already installed, and install if missing
packages <-c("Hmisc","corrplot") 

# Function to check whether the package is installed
InsPack <- function(pack)
{
  if ((pack %in% installed.packages()) == FALSE) {
    install.packages(pack)
  } 
}

# Applying the installation on the list of packages
lapply(packages, InsPack)

# Make the libraries
lib <- lapply(packages, require, character.only = TRUE)

# Check if it was possible to install all required libraries
flag <- all(as.logical(lib))

###################            Read input table              ####################
# Load the tab-delimited file containing the values to be checked (rownames in the first column)
my_data <-
  read.table (
    file = input_file,
    check.names = FALSE,
    header = TRUE,
    dec = ".",
    sep = "\t",
    row.names = 1,
    comment.char = ""
  )

# Clean table from empty lines
my_data <- my_data[!apply(is.na(my_data) | my_data=="",1,all),]
####################            Functions                  #####################

# Function for filling missing values with the mean of the column
fill_NA.mean <- function(vec)
{
  # Calculate mean value of each column (excluding missing values)
  m <- mean(vec, na.rm = TRUE)
  # Replace missing values with mean
  vec[is.na(vec)] <- m
  # Return the new input data frame
  return(vec)
}

# Function to logarithmically normalized OTU values
log_ratio <- function(data)
{
  # Compute the logarithmus
  log_data <- log(data)
  # Calculate exponential function of column-wise mean values for finite log transformed data
  gm <- exp(mean(log_data[is.finite(log_data)]))
  # Compute the logarithmus
  log_gm <- log(gm)
  # Take the difference of both log-transformed datasets
  data <- log_data - log_gm
  # Return the new OTU table
  return(data)
}

##################### END of FUNCTIONS ###########################

######################  MAIN PROGRAM #####################
my_data <- as.data.frame(apply(my_data,2,as.numeric))

first_OTU <- colnames(my_data)[otu_variables_start]

# Split the meta and taxonomic parts of the table
# Choose the continuous scaled variables
my_meta_data <- my_data[1:otu_variables_start - 1]

# Choose the taxonomic variables
my_otu_data <- my_data[otu_variables_start:dim(my_data)[2]]

# Process the meta measurements according to selection
if (fill_NA == 0) {
  # Do not do anything, just rename the file
  my_meta_fixed =  my_meta_data
}

if (fill_NA == 1) {
  # Fill non-zero missing meta-values with the mean of the column (optional)
  # Apply the previously implemented function 'fill_Na.mean' to the meta-data subset
  my_meta_fixed = apply(my_meta_data, 2, fill_NA.mean)
}

# The maximal number of absence
prevalence_cutoff <- dim(my_otu_data)[1] - (prevalence_exclusion*dim(my_otu_data)[1])

# Count how many missing values are found for each OTU 
na_count <-sapply(my_otu_data, function(y) sum(length(which(is.na(y)))))

# Count how many zeros are found for each OTU 
zero_count <-sapply(my_otu_data, function(y) sum(length(which(y==0))))
prevalence_count <- na_count + zero_count

# A new OTU-table is generated, where the number of missing values is below the set cutoff
my_otu_data <- my_otu_data[, prevalence_count <= prevalence_cutoff ]

# If the parameter is set, zeros are replaced with missing values
if (replace_zeros == 1) {
  my_otu_data[my_otu_data==0] <- NA
}

# Replace zeros with 0.0001 to avoid infinite number when calculating logarithmus (log(0)=-INF)
my_otu_data[my_otu_data==0] <- 0.0001

# Transform compositional data by log ratio transformation
my_otu_fixed = apply(my_otu_data, 2, log_ratio)

# Merge the meta- and OTU data in one table
transformed_data <- cbind(my_meta_fixed, my_otu_fixed)

# Centre and scale the values
my_scaled_data <- scale(transformed_data, center = TRUE, scale = TRUE)

# Calculate all pairwise correlations using Pearson correlation method
my_rcorr <- rcorr(as.matrix(my_scaled_data), type = "pearson")

# Generate vector with variable names
var_names <- row.names(my_rcorr$r)

# Depending on which parameters were set at the beginning, one query type is selected
# In each query type, three matrices are generated: p-value matrix, correlation matrix, support matrix
# All possibles pairs are saved in a vector (pairs)
if(includeTax==1 & includeMeta==0){

  # Correlation among OTUs and NO correlation among meta-variables
  row_names <- var_names[c(otu_variables_start:dim(my_rcorr$r)[1])]
  col_names <- var_names
  pairs <-expand.grid(row_names, col_names)
  my_cor_matrix <- my_rcorr$r[c(otu_variables_start:dim(my_rcorr$r)[1]),]
  my_pvl_matrix <-my_rcorr$P[c(otu_variables_start:dim(my_rcorr$P)[1]),]
  my_num_matrix <- my_rcorr$n[c(otu_variables_start:dim(my_rcorr$n)[1]),]
  
  # Set variable for plotting
  diagonale=0
  
} else if(includeTax==1 & includeMeta==1){
  
  # Correlation among OTUs and correlation among meta-variables
    row_names <-var_names
  col_names <- var_names
  pairs <-expand.grid(row_names, col_names)
  my_cor_matrix <- my_rcorr$r
  my_pvl_matrix <-my_rcorr$P
  my_num_matrix <- my_rcorr$n
  # Set variable for plotting
  diagonale=0
  
} else if (includeTax==0 & includeMeta==1) {
  # NO correlation among OTUs and correlation among meta-variables
  
  row_names <-var_names[c(1:(otu_variables_start - 1))]
  col_names <- var_names
  pairs <-expand.grid(row_names, col_names)
  my_cor_matrix <- my_rcorr$r[c(1:(otu_variables_start - 1)),]
  my_pvl_matrix <-my_rcorr$P[c(1:(otu_variables_start - 1)),]
  my_num_matrix <- my_rcorr$n[c(1:(otu_variables_start - 1)),]
  # Set variable for plotting
  diagonale=1
  
} else {
  # NO correlation among OTUs and NO correlation among meta-variables
  
  row_names <- var_names[c(1:(otu_variables_start - 1))]
  col_names <- var_names[otu_variables_start:dim(my_rcorr$r)[1]]
  pairs <-expand.grid(row_names, col_names)
  my_cor_matrix <- my_rcorr$r[c(1:(otu_variables_start - 1)),c(otu_variables_start:dim(my_rcorr$r)[1])]
  my_pvl_matrix <-my_rcorr$P[c(1:(otu_variables_start - 1)),c(otu_variables_start:dim(my_rcorr$P)[1])]
  my_num_matrix <- my_rcorr$n[c(1:(otu_variables_start - 1)),c(otu_variables_start:dim(my_rcorr$n)[1])]
  # Set variable for plotting
  diagonale=1
  
}

# Select the corresponding p-value for each pair 
p_vector <- as.vector(my_pvl_matrix)

# Select the corresponding correlation coefficient for each pair 
c_vector <- as.vector(my_cor_matrix)

# Select the corresponding number of observations for each pair 
n_vector <- as.vector(my_num_matrix)

# Generate matrix with the pairwise comparisons
my_pairs <-
  matrix(ncol = 5,
         c(
           as.character(pairs[, 2]),
           as.character(pairs[, 1]),
           c_vector,
           p_vector,
           n_vector
         ))

# Delete all pairs with insufficient number of pairs
my_pairs <- subset(my_pairs, as.numeric(my_pairs[,5]) > min_pair_support)

# Adjust p-value for multiple testing using the Benjamin-Hochberg method
pVal_BH <- round(p.adjust(my_pairs[,4], method = "BH"), 4)

# Add the corrected p-value in the table 
my_pairs <- cbind(my_pairs,as.numeric(pVal_BH))

# Remove similar pairs (values along the diagonal)
my_pairs <- my_pairs[!as.character(my_pairs[, 1]) == as.character(my_pairs[, 2]),]

# Remove duplicate pairs
my_pairs <- my_pairs[!duplicated(my_pairs[, 3]), ]

# Created matrix columns represent correlation coefficients, p-values, number of observations, and corrected p-values
# Rows represent the pairs
matrix_names <- list(c(rep("",times=dim(my_pairs)[1])),
                     c(
                       "variable1",
                       "variable2",
                       "correlation",
                       "pValue",
                       "support",
                       "Corrected"
                     ))

dimnames(my_pairs) <- matrix_names

# Create subset of pairs with significant p-values
my_pairs_cutoff <- my_pairs[as.numeric(my_pairs[, 4]) <= signf_cutoff, ]

# Convert to matrix
my_pairs_cutoff <- matrix(my_pairs_cutoff,ncol=6,dimnames = list(c(rep("",times=dim(my_pairs_cutoff)[1])),c("variable1","variable2","correlation","pvalue","support","corrected pvalue")))

# Create subset of significant pairs with strong correlation (above 0.5)
my_pairs_cutoff_corr <- my_pairs_cutoff[abs(as.numeric(my_pairs_cutoff[, 3])) >= 0.5, ]

# Remove columns containing no information
my_cor_matrix <- my_cor_matrix[, colSums(is.na(my_cor_matrix)) != nrow(my_cor_matrix)]

# Missing values in the correlation matrix are set to zero
my_cor_matrix[is.na(my_cor_matrix)] <- 0
#################################################################################
######                        Generate Graphs                              ######
#################################################################################
# Take current path in one variable to store results in seperate folders in further steps
OriginalPath <- getwd()

# Take the name of the inputfile to name the folder
prefix = paste(strsplit(input_file,"[.]")[[1]][1],sep="_")

# Make a directory name with inputfile name and date
newdir <- paste(prefix,Sys.Date(), sep = "_")

# Create a directory 
dir.create(newdir)

# Set path for all outputs to the new directory
setwd(newdir)
                    


# Check if significance value for graphical output was modified by the user
if (plot_pval_cutoff != signf_cutoff | plot_corr_cutoff != 0.5) {
  # Generate a new matrix with the signficance cutoff 
  my_pairs_cutoff <- my_pairs[as.numeric(my_pairs[, 4]) <= plot_pval_cutoff, ]
  
  # Extract all significant pairs with the set correlation cutoff
  corr_pval_cutoff <- my_pairs_cutoff[abs(as.numeric(my_pairs_cutoff[, 3])) >= plot_corr_cutoff, ]
  corr_pval_cutoff <- matrix(corr_pval_cutoff,ncol=6, dimnames=list(c(rep("",times=dim(corr_pval_cutoff)[1])),c("variable1","variable2","correlation","pvalue","support","corrected pvalue")))
} else  {
  # If the significance cutoff is 0.05 an the correlation cutoff is 0.5 (for plotting)
  # Take the previously generated matrix
  corr_pval_cutoff <- my_pairs_cutoff_corr
  corr_pval_cutoff <- matrix(corr_pval_cutoff,ncol=6, dimnames=list(c(rep("",times=dim(corr_pval_cutoff)[1])),c("variable1","variable2","correlation","pvalue","support","corrected pvalue")))
 }

# Save linearized transformed correlations of significant pairs in "linear_sign_pairs.pdf"
pdf("/Users/pablo/Endogenomiks/Projects/January/Analysis_results/results_ITS/final_graph/linear_sign_pairs.pdf")

# Iterate through all significant pairs
for (i in 1:dim(corr_pval_cutoff)[1]) {
  # Save log-scaled transformed values of the first variable of the pair
  x_df <- transformed_data[names(transformed_data) %in% corr_pval_cutoff[i, 2]]
  
  # Save as numerical vector
  x <- x_df[, 1]
  
  # Save log-scaled transformed values of the second variable of the pair
  y_df <- transformed_data[names(transformed_data) %in% corr_pval_cutoff[i, 1]]
  
  # Save as numerical vector
  y <- y_df[, 1]
  
  # Create a linear model for the pair (excluding missing values)
  clm <- lm(y ~ x, na.action = na.exclude)
  
  # Determine the number of steps for the generation of the confidence intervals
  steps <- (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))/1000
  
  # Sequence of more finely/evenly spaced data than original one for the calculation of the confidence interval
  newx <- seq(min(x, na.rm = TRUE), max(x, na.rm = TRUE), steps)
  
  # Predict confidence interval based on the generated linear model
  # Prediction intervals are calculated based on the residuals of the regression equation 
  # Prediction intervals account for the variability around the mean response inherent in any prediction
  # It represents the range where a single new observation is likely to fall
  a <- predict(clm, newdata = data.frame(x = newx), interval = "confidence")
  
  # Graphic display of all log-scaled transformed values of the ith-pair
  plot(
    x,
    y,
    ylab="",
    xlab="", 
    cex.axis = 0.75 ,
    xaxt = 'n',
    yaxt = 'n',
    xlim=c(min(x,na.rm = TRUE),max(x, na.rm = TRUE)),
   xaxs="i"
  )
  title(xlab = names(x_df),line=0.5,font.lab=2,cex.lab=1.4)
  title(ylab = names(y_df),line=0.5,font.lab=2,cex.lab=1.4)
  
  # Draw the confidence interval around the fitted line
  polygon(c(newx,rev(newx)),c(a[,2],rev(a[,3])),col="grey91",border=TRUE, lty="dashed")
  
  # Samples are shown as dots
  points(x,y)
  
  # Draw linear regression line
  abline(clm, lwd=2)
  
  # Take calculated pairwise p-value from rcorr
  pvalue_text <- paste("P-value:", round(as.numeric(corr_pval_cutoff[i, 4]), 4), sep = "")
  
  # Take corrected pairwise p-value
  pvalue_corr_text <- paste("Adj. p-value:", as.numeric(corr_pval_cutoff[i, 6]), sep = "")
  
  # Take calculated pairwise correlation coefficient from rcorr
  corr_text <- paste("Pearson's r:", round(as.numeric(corr_pval_cutoff[i, 3]), 4), sep = "")
  
  # Take calculated pairwise number of observations from rcorr
  support_text <- paste("supported by ", round(as.numeric(corr_pval_cutoff[i, 5]), 4)," observations", sep = "")
  
  # Show correlation coefficient and p-value in the plot
  mtext(corr_text, side = 3, line = 2)
  mtext(pvalue_text, side = 3, line = 1)
  mtext(pvalue_corr_text, side = 3, line = 0)
  mtext(support_text, side = 1, line = 2)
  
  abline(par("usr")[3],0)
  segments(par("usr")[1],a[1,2],par("usr")[1],a[1,3])
  abline(par("usr")[4],0)
  
}

dev.off()

#################################################################################
######                        Write Output Files                           ######
#################################################################################
# Take current path in one variable to store results in seperate folders in further steps
OriginalPath <- getwd()

# Take the name of the inputfile to name the folder
prefix = paste(strsplit(input_file,"[.]")[[1]][1],sep="_")

# Make a directory name with inputfile name and date
newdir <- paste(prefix,Sys.Date(), sep = "_")

# Create a directory 
dir.create(newdir)

# Set path for all outputs to the new directory
setwd(newdir)
                    
# Write the log-scale transformed table
write.table(my_scaled_data,"transformed.tab",sep = "\t",col.names = NA,quote = FALSE)

# Write the correlation table
write.table(my_cor_matrix,"correlation-table.tab",sep = "\t",col.names = NA,quote = FALSE)

# Write the pvalue table
write.table(my_pvl_matrix,"pval-table.tab",sep = "\t",col.names = NA,quote = FALSE)

# Write the number of effective samples table
write.table(my_num_matrix,"support-table.tab",sep = "\t",col.names = NA,quote = FALSE)

# Write the significant correlations
write.table(my_pairs_cutoff,"cutoff-pairs-corr-sign.tab",sep = "\t",col.names = NA,quote = FALSE)

# Write plotted pairs
write.table(corr_pval_cutoff,"plotted-pairs-stat.tab",sep = "\t",col.names = NA,quote = FALSE)


if(!flag) { stop("
    It was not possible to install all required R libraries properly.
                 Please check the installation of all required libraries manually.\n
                 Required libaries:ade4, GUniFrac, phangorn, randomcoloR, Rcpp")
}


#################################################################################
######                           End of Script                             ######
#################################################################################

```



# Final graphs

## For 16S



#### Phylogenetic tree 

```{r}
library(tidyverse)
library(MASS)
library(ape)
library(phyloseq)
affiliation_table_16s <- read.delim("results_16S/frogs/affiliation_final.tsv")
otu_table_16S <- as.data.frame(cbind(affiliation_table_16s[,"observation_name"],affiliation_table_16s[,"blast_taxonomy"]))
otu_table_16S <- otu_table_16S[!otu_table_16S$V2=="no data",]

colnames(otu_table_16S) <- c("ID","taxa")

otu_table_16S <- otu_table_16S %>%
  mutate(taxa= str_remove_all(taxa, "D_\\d__")) %>%
  separate(taxa,
           into = c("Domain", "Phylum", "Class", "Order", "Family", "Genera", "Species"),
           sep = ";")



filename <- "results_16S/tree_nexus_16S"
tree_16S <- ape::read.nexus(filename)

new_tree <- tree_16S
orig_tiplabels <- tree_16S$tip.label
pdf("results_16S/final_graphs/phylogenetic_tress.pdf")
ID_list <- otu_table_16S$Species[match(orig_tiplabels, otu_table_16S$ID)] ## Change species for oter if neccesary
new_tree$tip.label <- ID_list
tree_without_NA <- drop.tip(new_tree, which(is.na(ID_list)))
plot(tree_without_NA,cex=0.3)

ID_list <- otu_table_16S$Genera[match(orig_tiplabels, otu_table_16S$ID)] ## Change species for oter if neccesary

new_tree$tip.label <- ID_list
tree_without_NA <- drop.tip(new_tree, which(is.na(ID_list)))
plot(tree_without_NA,cex=0.3)


ID_list <- otu_table_16S$Genera[match(orig_tiplabels, otu_table_16S$ID)] ## Change species for oter if neccesary
new_tree$tip.label <- ID_list
tree_without_NA <- drop.tip(new_tree, which(is.na(ID_list)))
plot(tree_without_NA,cex=0.3)

library(ggtree)
ggtree(tree_without_NA, layout="roundrect")+ geom_tiplab(size=1, color="forestgreen")
ggtree(tree_without_NA, layout="circular")+geom_tiplab(size=1, color="forestgreen")
ggtree(tree_without_NA, layout="equal_angle")+geom_tiplab(size=1, color="forestgreen")
ggtree(tree_without_NA, layout="daylight")+geom_tiplab(size=1, color="forestgreen")
ggtree(tree_without_NA, branch.length='none', layout='circular')+ geom_tiplab(size=1, color="forestgreen")
ggtree(tree_without_NA, layout="daylight", branch.length = 'none')+ geom_tiplab(size=1, color="forestgreen")
dev.off()

#Read tables 


```

#### Heatmap 

Como hay muchos datos con pocas cuentas, se eliminaron aquellos con pocas lecturas 

```{r}
library("RColorBrewer")
library("ComplexHeatmap")


my_data_16S <-
  read.table (
    file = "results_16S/OTUsCombined.tab",
    check.names = FALSE,
    header = TRUE,
    dec = ".",
    sep = "\t",
    row.names = 1,
    comment.char = ""
  )


names_vector <- colnames(my_data_16S)[16:271] # Change
ID_list <- otu_table_16S$Genera[match(names_vector, otu_table_16S$ID)]
colnames(my_data_16S)[16:271] <- ID_list
data_heat <- my_data_16S[16:271]

library(dplyr)

data_heat <- as.data.frame(t(data_heat))
# Get the base name of each column
colnames_base <- gsub("\\..*", "", row.names(data_heat))
data_heat$base_name <- colnames_base

# Group the data by base_name
grouped_df <- data_heat %>%
  group_by(base_name) %>%
  summarize_all(sum)

names <- grouped_df$base_name
#colnames(grouped_df) <- c("base_name"    ,  "AC1MD1SS08_16S" ,"AC1MD1SS09_16S", "AC1MD1SS10_16S","AC1MD1SS11_16S","AC1MD1SS12_16S" ,"AC1MD1SS13_16S", "AC1MD1SS14_16S")
grouped_df$base_name <- NULL
rownames(grouped_df) <- names

# prepare a data frame with the annotation
ann_df <- data.frame(row.names = colnames(grouped_df),
                     Treatment = c("Inoculo", "TI","TNO3","CN", "TI","TNO3","CN"),
                     Biosample = c("D.edule", "P.vulgaris","P.vulgaris","P.vulgaris", "T.repens","T.repens","T.repens"))
# calculate the sum of values in each column

pdf("results_16S/final_graphs/heatmap_abundancia_rel_per_genus_grouped_percen_scale.pdf")

# plot the heatmap itself
fontsize_row = 10 - nrow(grouped_df) / 15

pheatmap::pheatmap(grouped_df,
                   scale = "none",
                   cluster_rows = FALSE,
                   cluster_cols = TRUE,
                   cutree_cols = 5,
                   annotation_col = ann_df,main = "Composition of the population by genus",fontsize_row=fontsize_row)
dev.off()




```
```{r}
affiliation_table_16s <- read.delim("results_16S/frogs/affiliation_final.tsv")
otu_table_16S <- as.data.frame(cbind(affiliation_table_16s[,"observation_name"],affiliation_table_16s[,"blast_taxonomy"]))
otu_table_16S <- otu_table_16S[!otu_table_16S$V2=="no data",]

colnames(otu_table_16S) <- c("ID","taxa")

otu_table_16S <- otu_table_16S %>%
  mutate(taxa= str_remove_all(taxa, "D_\\d__")) %>%
  separate(taxa,
           into = c("Domain", "Phylum", "Class", "Order", "Family", "Genera", "Species"),
           sep = ";")

my_data_16S <-
  read.table (
    file = "results_16S/OTUs_Table-norm-tax.tab",
    check.names = FALSE,
    header = TRUE,
    dec = ".",
    sep = "\t",
    row.names = 1,
    comment.char = ""
  )


names_vector <- rownames(my_data_16S)# Change
ID_list <- otu_table_16S$Genera[match(names_vector, otu_table_16S$ID)]
my_data_16S$taxonomy <- ID_list

library(dplyr)

# Group the data by base_name
grouped_df <- my_data_16S %>%
  group_by(taxonomy) %>%
  summarize_all(sum)

names <- grouped_df$taxonomy
#colnames(grouped_df) <- c("base_name"    ,  "AC1MD1SS08_16S" ,"AC1MD1SS09_16S", "AC1MD1SS10_16S","AC1MD1SS11_16S","AC1MD1SS12_16S" ,"AC1MD1SS13_16S", "AC1MD1SS14_16S")
grouped_df$taxonomy <- NULL
row.names(grouped_df) <- names

# prepare a data frame with the annotation
ann_df <- data.frame(row.names = colnames(grouped_df),
                     Treatment = c("NA", "TI","TNO3","CN", "TI","TNO3","CN"),
                     Biosample = c("D.edule", "P.vulgaris","P.vulgaris","P.vulgaris", "T.repens","T.repens","T.repens"))

pdf("results_16S/final_graphs/heatmap_abundancia_rel_per_genus_grouped.pdf")

# plot the heatmap itself
fontsize_row = 10 - nrow(grouped_df) / 15

pheatmap::pheatmap(grouped_df,
                   scale = "none",
                   cluster_rows = FALSE,
                   cluster_cols = TRUE,
                   cutree_cols = 5,
                   annotation_col = ann_df,main = "Composition of the population by genus",fontsize_row=fontsize_row)
dev.off()
```

```{r}
my_data_16S <-
  read.table (
    file = "results_16S/OTUsCombined.tab",
    check.names = FALSE,
    header = TRUE,
    dec = ".",
    sep = "\t",
    row.names = 1,
    comment.char = ""
  )
new_data <- t(my_data_16S[,8:15])

# Define some graphics to display the distribution of rows
ann_df <- data.frame(row.names = colnames(new_data),
                     Treatment = c("Inoculo", "TI","TNO3","CN", "TI","TNO3","CN"),
                     Biosample = c("D.edule", "P.vulgaris","P.vulgaris","P.vulgaris", "T.repens","T.repens","T.repens"))

pdf("results_16S/final_graphs/heatmap_abundancia_rel_per_genus.pdf")

# plot the heatmap itself
fontsize_row = 10 - nrow(new_data) / 15
pheatmap::pheatmap(new_data,
                   scale = "none",
                   cluster_rows = FALSE,
                   cluster_cols = TRUE,
                   cutree_cols = 5,
                   annotation_col = ann_df,main = "Composition of the population by genus",fontsize_row=fontsize_row)

dev.off()
pdf("results_16S/final_graphs/boxplots.pdf")

library(dplyr)


# Group by treatment and add numbers
library(reshape2)
# Convert the matrix to a three-column format
df <- melt(t(new_data),varnames = c("ID","Statistic"))
df$Treatment <- rep(c("Inoculo", "TI","TNO3","CN", "TI","TNO3","CN"),8) 

ggplot(df, aes(x = value, y = Statistic, fill=Treatment)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by Treatment")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 


Richness_data <- df[grepl("Richness",df$Statistic),]
ggplot(Richness_data, aes(x = value, y = Statistic, fill=Treatment)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by Treatment")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 

Index_data <- df[grepl("Shannon",df$Statistic),]
ggplot(Index_data, aes(x = value, y = Statistic, fill=Treatment)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by Treatment")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 

Simpson_data <- df[grepl("Simpson",df$Statistic),]
ggplot(Simpson_data, aes(x = value, y = Statistic, fill=Treatment)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by Treatment")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 

Evenness_data <- df[grepl("Evenness",df$Statistic),]
ggplot(Evenness_data, aes(x = value, y = Statistic, fill=Treatment)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by Treatment")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 


##Figures by species 
df$BioSample <- rep(c("D.edule", "P.vulgaris","P.vulgaris","P.vulgaris", "T.repens","T.repens","T.repens"),8) 

ggplot(df, aes(x = value, y = Statistic, fill=BioSample)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by BioSample")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 

Richness_data <- df[grepl("Richness",df$Statistic),]
ggplot(Richness_data, aes(x = value, y = Statistic, fill=BioSample)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by BioSample")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 

Index_data <- df[grepl("Shannon",df$Statistic),]
ggplot(Index_data, aes(x = value, y = Statistic, fill=BioSample)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by BioSample")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 

Simpson_data <- df[grepl("Simpson",df$Statistic),]
ggplot(Simpson_data, aes(x = value, y = Statistic, fill=Treatment)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by Treatment")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 

Evenness_data <- df[grepl("Evenness",df$Statistic),]
ggplot(Evenness_data, aes(x = value, y = Statistic, fill=BioSample)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by BioSample")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 

dev.off()
```

### Venn diagramm
```{r}
library(tidyverse)
library(dplyr)

my_data_16S <-
  read.table (
    file = "results_16S/OTUs_Table-norm-rel-tax.tab",
    check.names = FALSE,
    header = TRUE,
    dec = ".",
    sep = "\t",
    comment.char = ""
  )

my_data_16S <- my_data_16S[!grepl("unknown genus",my_data_16S$taxonomy),]

OTUS_taxonomy <- cbind(my_data_16S[,1],my_data_16S[,"taxonomy"])
OTUS_taxonomy <- as.data.frame(OTUS_taxonomy)
colnames(OTUS_taxonomy) <- c("ID","taxonomy")

OTUS_taxonomy <- OTUS_taxonomy %>% 
  separate(taxonomy,
           into = c("Domain", "Phylum", "Class", "Order", "Family", "Genera", "Species"),
           sep = ";")

my_data_16S$taxonomy<- NULL
rownames(my_data_16S) = seq(length=nrow(my_data_16S))
rownames(OTUS_taxonomy) = seq(length=nrow(OTUS_taxonomy))
my_data_16S[,1] <- NULL

my_data_16S <- as.matrix(my_data_16S)

my_data_16S <- ifelse(my_data_16S > 0, 1, 0)

my_data_16S <- as.data.frame(my_data_16S)
my_data_16S$genera <- OTUS_taxonomy$Genera


my_data_16S <- my_data_16S[!grepl("Multi-affiliation",my_data_16S$genera),]

# Group the data by base_name
grouped_df <- my_data_16S %>%
  group_by(genera) %>%
  summarize_all(sum)

grouped_df <- as.data.frame(t(grouped_df))
colnames(grouped_df) <- grouped_df["genera",]
grouped_df <- grouped_df[-1,]
require(data.table)
df <- melt(t(grouped_df),varnames = c("genera","ID"))

#Dataframe with the metadata
metadata_df <- data.frame(ID = rownames(grouped_df),
                          Treatment = c("Inoculo", "TI","TNO3","CN", "TI","TNO3","CN"),
                          Biosample = c("D.edule", "P.vulgaris","P.vulgaris","P.vulgaris", "T.repens","T.repens","T.repens"))


df1 <- merge( df, metadata_df,by = "ID", all.x = TRUE)
df1$value <- as.numeric(df1$value)
df1 <- df1[!df1$value==0,]
df1$value <- NULL

library(ggvenn)
pdf("results_16S/final_graphs/Venn_diagrams-by treatment.pdf")

#Erase if the value is 0 

#drop the value 
colors <- c("#AED581", "#009E73","#80CBC4","#81D4FA")


P.vulgaris_venn <- rbind(df1[df1$Biosample==c("P.vulgaris"),],df1[df1$Biosample==c("D.edule"),])
T.repens_venn <- rbind(df1[df1$Biosample==c("T.repens"),],df1[df1$Biosample==c("D.edule"),])


## Do P.vulgaris by taxonomy 
CN_vulgaris <- P.vulgaris_venn[P.vulgaris_venn$Treatment=="CN",]
TI_vulgaris <- P.vulgaris_venn[P.vulgaris_venn$Treatment=="TI",]
TNO3_vulgaris <- P.vulgaris_venn[P.vulgaris_venn$Treatment=="TNO3",]
Inoculo_vulgaris <- P.vulgaris_venn[P.vulgaris_venn$Treatment=="Inoculo",]

list_taxa_vulgaris <- list(CN=as.character(CN_vulgaris$genera),TI=as.character(TI_vulgaris$genera),TNO3=as.character(TNO3_vulgaris$genera),Inoculo=as.character(Inoculo_vulgaris$genera))

library("ggVennDiagram")
ggVennDiagram(list_taxa_vulgaris)+scale_color_brewer(palette = "Paired")+
  labs(title = "Taxonomic genus share in P.vulgaris by treatment")
ggvenn(list_taxa_vulgaris, 
  fill_color = colors,
  stroke_size = 0.3, text_size = 3
  )



## Do Repens by taxonomy 
CN_repens <- T.repens_venn[T.repens_venn$Treatment=="CN",]
TI_repens <- T.repens_venn[T.repens_venn$Treatment=="TI",]
TNO3_repens <- T.repens_venn[T.repens_venn$Treatment=="TNO3",]
Inoculo_repens <- T.repens_venn[T.repens_venn$Treatment=="Inoculo",]

list_taxa_repens <- list(CN=as.character(CN_repens$genera),TI=as.character(TI_repens$genera),TNO3=as.character(TNO3_repens$genera),Inoculo=as.character(Inoculo_repens$genera))

ggVennDiagram(list_taxa_repens)+scale_color_brewer(palette = "Paired")+
  labs(title = "Taxonomic genus share in T.repens by treatment")

ggvenn(list_taxa_repens, 
  fill_color = colors,
  stroke_size = 0.3, text_size = 3
  )



dev.off()
```


```{r}


pdf("results_16S/final_graphs/Venn_diagrams-by_specie.pdf")

#drop the value 
colors <- c("#AED581", "#009E73","#80CBC4")

P.vulgaris_venn <- df1[df1$Biosample=="P.vulgaris",]
T.repens_venn <- df1[df1$Biosample=="T.repens",]
D.edule_venn <- df1[df1$Biosample=="D.edule",]

list_taxa_vulgaris <- list(P.vulgaris=as.character(P.vulgaris_venn$genera),T.repens=as.character(T.repens_venn$genera),D.edule=as.character(D.edule_venn$genera))

ggvenn(list_taxa_vulgaris, 
       fill_color = colors,
       stroke_size = 0.3, text_size = 3
)

ggVennDiagram(list_taxa_vulgaris)+scale_color_brewer(palette = "Paired")+
  labs(title = "Taxonomic genus share by species")


dev.off()
```


## For ITS

No cambie los nombres de las variables en todos los casos 
#### Phylogenetic tree 

```{r}
library(tidyverse)
library(MASS)
library(ape)
library(phyloseq)
affiliation_table_16s <- read.delim("results_ITS/frogs/fungi_affiliation_final_res.tsv")
otu_table_16S <- as.data.frame(cbind(affiliation_table_16s[,"observation_name"],affiliation_table_16s[,"blast_taxonomy"]))
otu_table_16S <- otu_table_16S[!otu_table_16S$V2=="no data",]

colnames(otu_table_16S) <- c("ID","taxa")

otu_table_16S <- otu_table_16S %>%
  mutate(taxa= str_remove_all(taxa, "D_\\d__")) %>%
  separate(taxa,
           into = c("Domain", "Phylum", "Class", "Order", "Family", "Genera", "Species"),
           sep = ";")



filename <- "results_ITS/tree_nexus_16S"
tree_16S <- ape::read.nexus(filename)

new_tree <- tree_16S
orig_tiplabels <- tree_16S$tip.label
pdf("results_ITS/final_graph/phylogenetic_tress.pdf")
ID_list <- otu_table_16S$Species[match(orig_tiplabels, otu_table_16S$ID)] ## Change species for oter if neccesary
new_tree$tip.label <- ID_list
tree_without_NA <- drop.tip(new_tree, which(is.na(ID_list)))
plot(tree_without_NA,cex=0.3)

ID_list <- otu_table_16S$Genera[match(orig_tiplabels, otu_table_16S$ID)] ## Change species for oter if neccesary

new_tree$tip.label <- ID_list
tree_without_NA <- drop.tip(new_tree, which(is.na(ID_list)))
plot(tree_without_NA,cex=0.3)


ID_list <- otu_table_16S$Genera[match(orig_tiplabels, otu_table_16S$ID)] ## Change species for oter if neccesary
new_tree$tip.label <- ID_list
tree_without_NA <- drop.tip(new_tree, which(is.na(ID_list)))
plot(tree_without_NA,cex=0.3)

library(ggtree)
ggtree(tree_without_NA, layout="roundrect")+ geom_tiplab(size=1, color="forestgreen")
ggtree(tree_without_NA, layout="circular")+geom_tiplab(size=1, color="forestgreen")
ggtree(tree_without_NA, layout="equal_angle")+geom_tiplab(size=1, color="forestgreen")
ggtree(tree_without_NA, layout="daylight")+geom_tiplab(size=1, color="forestgreen")
ggtree(tree_without_NA, branch.length='none', layout='circular')+ geom_tiplab(size=1, color="forestgreen")
ggtree(tree_without_NA, layout="daylight", branch.length = 'none')+ geom_tiplab(size=1, color="forestgreen")
dev.off()

#Read tables 


```

#### Heatmap 

Como hay muchos datos con pocas cuentas, se eliminaron aquellos con pocas lecturas 

```{r}
library("RColorBrewer")
library("ComplexHeatmap")
library(tidyverse)
affiliation_table_16s <- read.delim("results_ITS/frogs/fungi_affiliation_final_res.tsv")
otu_table_16S <- as.data.frame(cbind(affiliation_table_16s[,"observation_name"],affiliation_table_16s[,"blast_taxonomy"]))
otu_table_16S <- otu_table_16S[!otu_table_16S$V2=="no data",]

colnames(otu_table_16S) <- c("ID","taxa")

otu_table_16S <- otu_table_16S %>%
  mutate(taxa= str_remove_all(taxa, "D_\\d__")) %>%
  separate(taxa,
           into = c("Domain", "Phylum", "Class", "Order", "Family", "Genera", "Species"),
           sep = ";")

my_data_16S <-
  read.table (
    file = "results_ITS/OTUsCombined.tab",
    check.names = FALSE,
    header = TRUE,
    dec = ".",
    sep = "\t",
    row.names = 1,
    comment.char = ""
  )


names_vector <- colnames(my_data_16S)[16:98] # Change
ID_list <- otu_table_16S$Genera[match(names_vector, otu_table_16S$ID)]
colnames(my_data_16S)[16:98] <- ID_list
data_heat <- my_data_16S[16:98]

library(dplyr)

data_heat <- as.data.frame(t(data_heat))
# Get the base name of each column
colnames_base <- gsub("\\..*", "", row.names(data_heat))
data_heat$base_name <- colnames_base

# Group the data by base_name
grouped_df <- data_heat %>%
  group_by(base_name) %>%
  summarize_all(sum)
grouped_df <- grouped_df[-c(1), ] 

names <- grouped_df$base_name
#colnames(grouped_df) <- c("base_name"    ,  "AC1MD1SS08_16S" ,"AC1MD1SS09_16S", "AC1MD1SS10_16S","AC1MD1SS11_16S","AC1MD1SS12_16S" ,"AC1MD1SS13_16S", "AC1MD1SS14_16S")
grouped_df$base_name <- NULL
rownames(grouped_df) <- names

# prepare a data frame with the annotation
ann_df <- data.frame(row.names = colnames(grouped_df),
                     Treatment = c("Inoculo", "TI","TNO3","CN", "TI","TNO3","CN"),
                     Biosample = c("D.edule", "P.vulgaris","P.vulgaris","P.vulgaris", "T.repens","T.repens","T.repens"))
# calculate the sum of values in each column

pdf("results_ITS/final_graph/heatmap_abundancia_rel_per_genus_grouped_percen_scale.pdf")

# plot the heatmap itself
fontsize_row = 10 - nrow(grouped_df) / 15

pheatmap::pheatmap(grouped_df,
                   scale = "none",
                   cluster_rows = FALSE,
                   cluster_cols = TRUE,
                   cutree_cols = 5,
                   annotation_col = ann_df,main = "Composition of the population by genus",fontsize_row=fontsize_row)
dev.off()




```
```{r}

my_data_16S <-
  read.table (
    file = "results_ITS/OTUs_Table-norm-tax.tab",
    check.names = FALSE,
    header = TRUE,
    dec = ".",
    sep = "\t",
    row.names = 1,
    comment.char = ""
  )


names_vector <- rownames(my_data_16S)# Change
ID_list <- otu_table_16S$Genera[match(names_vector, otu_table_16S$ID)]
my_data_16S$taxonomy <- ID_list

library(dplyr)

# Group the data by base_name
grouped_df <- my_data_16S %>%
  group_by(taxonomy) %>%
  summarize_all(sum)
grouped_df <- grouped_df[!is.na(grouped_df$taxonomy), ] 
names <- grouped_df$taxonomy
#colnames(grouped_df) <- c("base_name"    ,  "AC1MD1SS08_16S" ,"AC1MD1SS09_16S", "AC1MD1SS10_16S","AC1MD1SS11_16S","AC1MD1SS12_16S" ,"AC1MD1SS13_16S", "AC1MD1SS14_16S")
grouped_df$taxonomy <- NULL
rownames(grouped_df) <- names

# prepare a data frame with the annotation
ann_df <- data.frame(row.names = colnames(grouped_df),
                     Treatment = c("NA", "TI","TNO3","CN", "TI","TNO3","CN"),
                     Biosample = c("D.edule", "P.vulgaris","P.vulgaris","P.vulgaris", "T.repens","T.repens","T.repens"))

pdf("results_ITS/final_graph/heatmap_abundancia_rel_per_genus_grouped.pdf")

# plot the heatmap itself
fontsize_row = 10 - nrow(grouped_df) / 15

pheatmap::pheatmap(grouped_df,
                   scale = "none",
                   cluster_rows = FALSE,
                   cluster_cols = TRUE,
                   cutree_cols = 5,
                   annotation_col = ann_df,main = "Composition of the population by genus",fontsize_row=fontsize_row)
dev.off()
```

```{r}
my_data_16S <-
  read.table (
    file = "results_ITS/OTUsCombined.tab",
    check.names = FALSE,
    header = TRUE,
    dec = ".",
    sep = "\t",
    row.names = 1,
    comment.char = ""
  )
new_data <- t(my_data_16S[,8:15])

# Define some graphics to display the distribution of rows
ann_df <- data.frame(row.names = colnames(new_data),
                     Treatment = c("Inoculo", "TI","TNO3","CN", "TI","TNO3","CN"),
                     Biosample = c("D.edule", "P.vulgaris","P.vulgaris","P.vulgaris", "T.repens","T.repens","T.repens"))

pdf("results_ITS/final_graph/heatmap_abundancia_rel_per_genus.pdf")

# plot the heatmap itself
fontsize_row = 10 - nrow(new_data) / 15
pheatmap::pheatmap(new_data,
                   scale = "none",
                   cluster_rows = FALSE,
                   cluster_cols = TRUE,
                   cutree_cols = 5,
                   annotation_col = ann_df,main = "Composition of the population by genus",fontsize_row=fontsize_row)

dev.off()
pdf("results_ITS/final_graph/boxplots.pdf")

library(dplyr)


# Group by treatment and add numbers
library(reshape2)
# Convert the matrix to a three-column format
df <- melt(t(new_data),varnames = c("ID","Statistic"))
df$Treatment <- rep(c("Inoculo", "TI","TNO3","CN", "TI","TNO3","CN"),8) 

ggplot(df, aes(x = value, y = Statistic, fill=Treatment)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by Treatment")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 


Richness_data <- df[grepl("Richness",df$Statistic),]
ggplot(Richness_data, aes(x = value, y = Statistic, fill=Treatment)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by Treatment")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 

Index_data <- df[grepl("Shannon",df$Statistic),]
ggplot(Index_data, aes(x = value, y = Statistic, fill=Treatment)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by Treatment")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 

Simpson_data <- df[grepl("Simpson",df$Statistic),]
ggplot(Simpson_data, aes(x = value, y = Statistic, fill=Treatment)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by Treatment")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 

Evenness_data <- df[grepl("Evenness",df$Statistic),]
ggplot(Evenness_data, aes(x = value, y = Statistic, fill=Treatment)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by Treatment")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 


##Figures by species 
df$BioSample <- rep(c("D.edule", "P.vulgaris","P.vulgaris","P.vulgaris", "T.repens","T.repens","T.repens"),8) 

ggplot(df, aes(x = value, y = Statistic, fill=BioSample)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by BioSample")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 

Richness_data <- df[grepl("Richness",df$Statistic),]
ggplot(Richness_data, aes(x = value, y = Statistic, fill=BioSample)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by BioSample")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 

Index_data <- df[grepl("Shannon",df$Statistic),]
ggplot(Index_data, aes(x = value, y = Statistic, fill=BioSample)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by BioSample")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 

Simpson_data <- df[grepl("Simpson",df$Statistic),]
ggplot(Simpson_data, aes(x = value, y = Statistic, fill=Treatment)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by Treatment")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 

Evenness_data <- df[grepl("Evenness",df$Statistic),]
ggplot(Evenness_data, aes(x = value, y = Statistic, fill=BioSample)) + 
  geom_boxplot() +
  labs(x = "", y = "Statistics") +
  ggtitle("Boxplot of statistics by BioSample")+
  scale_fill_manual(values=c("#AED581","#009E73","#80CBC4","#81D4FA","#9FA8DA","#CE93D8","#F4A582","#2C728F")) 

dev.off()
```

### Venn diagramm
```{r}
library(tidyverse)
library(dplyr)
my_data_ITS <-
  read.table (
    file = "results_ITS/OTUs_Table-norm-rel-tax.tab",
    check.names = FALSE,
    header = TRUE,
    dec = ".",
    sep = "\t",
    comment.char = ""
  )

my_data_ITS <- my_data_ITS[!grepl("g__unidentified",my_data_ITS$taxonomy),]
my_data_ITS <- my_data_ITS[!my_data_ITS$taxonomy=="no data",]
OTUS_taxonomy <- cbind(my_data_ITS[,1],my_data_ITS[,"taxonomy"])
OTUS_taxonomy <- as.data.frame(OTUS_taxonomy)
colnames(OTUS_taxonomy) <- c("ID","taxonomy")

OTUS_taxonomy <- OTUS_taxonomy %>% mutate(taxonomy= str_remove_all(taxonomy, "g__")) %>% mutate(taxonomy= str_remove_all(taxonomy, "s__")) %>% mutate(taxonomy= str_remove_all(taxonomy, "f__")) %>% mutate(taxonomy= str_remove_all(taxonomy, "o__")) %>% 
  separate(taxonomy,
           into = c("Domain", "Phylum", "Class", "Order", "Family", "Genera", "Species"),
           sep = ";")
my_data_ITS$taxonomy<- NULL
rownames(my_data_ITS) = seq(length=nrow(my_data_ITS))
rownames(OTUS_taxonomy) = seq(length=nrow(OTUS_taxonomy))
my_data_ITS[,1] <- NULL

my_data_ITS <- as.matrix(my_data_ITS)

my_data_ITS <- ifelse(my_data_ITS > 0, 1, 0)

my_data_ITS <- as.data.frame(my_data_ITS)
my_data_ITS$genera <- OTUS_taxonomy$Genera


# Group the data by base_name
grouped_df <- my_data_ITS %>%
  group_by(genera) %>%
  summarize_all(sum)

grouped_df <- as.data.frame(t(grouped_df))
colnames(grouped_df) <- grouped_df["genera",]
grouped_df <- grouped_df[-1,]
require(data.table)
df <- melt(t(grouped_df),varnames = c("genera","ID"))

#Dataframe with the metadata
metadata_df <- data.frame(ID = rownames(grouped_df),
                     Treatment = c("Inoculo", "TI","TNO3","CN", "TI","TNO3","CN"),
                     Biosample = c("D.edule", "P.vulgaris","P.vulgaris","P.vulgaris", "T.repens","T.repens","T.repens"))


df1 <- merge( df, metadata_df,by = "ID", all.x = TRUE)


library(ggvenn)

pdf("results_ITS/final_graph/Venn_diagrams-by treatment.pdf")
df1$value <- as.numeric(df1$value)

#Erase if the value is 0 
df1 <- df1[!df1$value==0,]

#drop the value 
df1$value <- NULL
colors <- c("#AED581", "#009E73","#80CBC4","#81D4FA")

P.vulgaris_venn <- rbind(df1[df1$Biosample==c("P.vulgaris"),],df1[df1$Biosample==c("D.edule"),])
T.repens_venn <- rbind(df1[df1$Biosample==c("T.repens"),],df1[df1$Biosample==c("D.edule"),])


## Do P.vulgaris by taxonomy 
CN_vulgaris <- P.vulgaris_venn[P.vulgaris_venn$Treatment=="CN",]
TI_vulgaris <- P.vulgaris_venn[P.vulgaris_venn$Treatment=="TI",]
TNO3_vulgaris <- P.vulgaris_venn[P.vulgaris_venn$Treatment=="TNO3",]
Inoculo_vulgaris <- P.vulgaris_venn[P.vulgaris_venn$Treatment=="Inoculo",]

list_taxa_vulgaris <- list(CN=as.character(CN_vulgaris$genera),TI=as.character(TI_vulgaris$genera),TNO3=as.character(TNO3_vulgaris$genera),Inoculo=as.character(Inoculo_vulgaris$genera))

library("ggVennDiagram")
library(ggvenn)
ggVennDiagram(list_taxa_vulgaris)+
  labs(title = "Taxonomic genus share in P.vulgaris by treatment")

print(list_taxa_vulgaris)

ggvenn(list_taxa_vulgaris, 
  fill_color = colors,
  stroke_size = 0.3, text_size = 3
  )



## Do Repens by taxonomy 
CN_repens <- T.repens_venn[T.repens_venn$Treatment=="CN",]
TI_repens <- T.repens_venn[T.repens_venn$Treatment=="TI",]
TNO3_repens <- T.repens_venn[T.repens_venn$Treatment=="TNO3",]
Inoculo_repens <- T.repens_venn[T.repens_venn$Treatment=="Inoculo",]

list_taxa_repens <- list(CN=as.character(CN_repens$genera),TI=as.character(TI_repens$genera),TNO3=as.character(TNO3_repens$genera),Inoculo=as.character(Inoculo_repens$genera))

ggVennDiagram(list_taxa_repens)+scale_color_brewer(palette = "Paired")+
  labs(title = "Taxonomic genus share in T.repens by treatment")
print(list_taxa_repens)

ggvenn(list_taxa_repens, 
  fill_color = colors,
  stroke_size = 0.3, text_size = 3
  )



dev.off()
```


```{r}


pdf("results_ITS/final_graph/Venn_diagrams-by_species.pdf")


#drop the value 
colors <- c("#AED581", "#009E73","#80CBC4")
#Split the data by Biosample
biosample_list <- split(df1, df1$Biosample,drop = T)

P.vulgaris_venn <- df1[df1$Biosample=="P.vulgaris",]
T.repens_venn <- df1[df1$Biosample=="T.repens",]
D.edule_venn <- df1[df1$Biosample=="D.edule",]

list_taxa_vulgaris <- list(P.vulgaris=as.character(P.vulgaris_venn$genera),T.repens=as.character(T.repens_venn$genera),D.edule=as.character(D.edule_venn$genera))

ggvenn(list_taxa_vulgaris, 
  fill_color = colors,
  stroke_size = 0.3, text_size = 3
  )

ggVennDiagram(list_taxa_vulgaris)+scale_color_brewer(palette = "Paired")+
  labs(title = "Taxonomic genus share in by species")


dev.off()
```

